\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MAT257 Notes}
\author{Jad Elkhaleq Ghalayini}
\date{April 2019}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{xcolor}

\usepackage[margin=1in]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem{exercise}{Exercise}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Vol}{vol}
\DeclareMathOperator{\D}{D}

\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\ints}[0]{\mathbb{Z}}
\newcommand{\rationals}[0]{\mathbb{Q}}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\sbrac}[1]{\left[#1\right]}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\eval}[3]{\left.#3\right|_{#1}^{#2}}
\newcommand{\ip}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\prt}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\hlfspc}[0]{\mathbb{H}}
\newcommand{\loint}[0]{\operatorname{L}\int}
\newcommand{\hiint}[0]{\operatorname{U}\int}
\newcommand{\indic}[1]{\chi_{#1}}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} #1}}


\begin{document}

\maketitle

This document is a collection of notes for the course MAT257: Analysis II, as taught by Professor Edward Bierstone in 2018 at the University of Toronto. The notes are a combination of notes I made in class (which can be found in their original form in the \verb|notes| folder in this repository) and scans of handwritten notes which Professor Bierstone has generously given me the permission to use.

\section{Introduction}

\TODO{this}

\section{Differentiation}

\TODO{this}

\section{Integration}

\subsection{The (Riemann) Integral Over a Rectangle}

\TODO{this}

\subsection{Integrals Over More General Bounded Sets}

\TODO{this}

\subsection{Fubini's Theorem}

\TODO{rewrite}

We are now going to talk about Fubini's theorem, which is about how to integrate over a high-dimensional rectangle by repeatedly performing individual integrals. Let's start with an example. Suppose we want to integrate over a rectangle
\begin{equation}
  A = [a, b] \times [c, d] \subseteq \reals^2
\end{equation}
and let's suppose, to make it easy, we have a continuous, non-negative function \(f\), defined on this rectangle, say
\begin{equation}
  z = f(x, y) \geq 0
\end{equation}
The idea is that if we fix a point on the \(x\)-axis, say \(x\), we can consider the ``slice'' in the \(y\)-direction determined by this point. We could find, for example, the area of that slice, and it's reasonable to expect that the integral of \(f\) on the rectangle, we could obtain by integrating the area of that slice along the length of the rectangle.

The idea then is to try to find the area of such a slice, which would be in this case, for some \(x\),
\begin{equation}
  h(x) = \int_c^dg_x(y)dy = \int_c^df(x, y)dy
\end{equation}
where \(g_x(y) = f(x, y)\) fixing \(x\). It's reasonable to expect that
\begin{equation}
  \int_Af = \int_a^bh = \int_a^b\left(\int_c^df(x, y)dy\right)dx
\end{equation}
So that's the idea of Fubini's theorem: that we should be able to integrate a function over a rectangle by repeated one-dimensional integrals. Of course, we're not interested in integrating only continuous functions, we want to look at more general, integrable functions, but if you think about that, supposing \(f\) is integrable, this could run into a problem: one of the functions \(g_x\) might not be integrable on \([c, d]\). After all, it's set of discontinuities could be \(x_0 \times [c, d]\) for some \(x_0\), so \(\int_c^dg_{x_0}\) makes no sense!

So we'll have to formulate something maybe a little bit more technical, but it's to capture that problem. Suppose we just have \(f: A \to \reals\) bounded. The function may or may not be integrable, meaning the supremum of lower sums is equal to the infimum of lower sums. Whether the function is integrable or not, we can still look at the supermum of lower sums and the infimum of lower sums, which is what we'll do.

We'll define the \underline{lower} and \underline{upper integrals} of \(f\) on \(A\), \(\loint_Af\) and \(\hiint_Af\) respectively, to be the supremum of all the lower sums \(L(f, \mc{P})\) and infimum of all the upper sums \(U(f, \mc{P})\) respectively. We can now write down our theorem as the previous formula, but taking into account that we don't know that the function \(g_x\) mentioned before, we just replace it by the lower or upper integral:
\begin{theorem}
  Suppose \(A \subset \reals^m\) and \(B \subset \reals^n\) are closed rectangles and \(f: A \times B \to \reals\) is integrable. For all \(x \in A\), define
  \begin{equation}
    g_x: B \to \reals, g_x(y) = f(x, y)
  \end{equation}
  Set
  \begin{equation}
    \mc{L}(x) = \loint_Bg_x = \loint_Bf(x, y)dy, \mc{U}(x) = \hiint_Bg_x = \hiint_Bf(x, y)dy
  \end{equation}
  Then \(\mc{L}(x), \mc{U}(x)\) are integrable on \(A\) and
  \begin{equation}
    \int_{A \times B}f = \int_A\mc{L} = \int_A\mc{U} = \int_A\left(\loint_Bf(x, y)dy\right)dx = \int_A\left(\hiint_Bf(x, y)dy\right)dx
  \end{equation}
\end{theorem}
Before we prove this, some remarks:
\begin{enumerate}

  \item We also have
  \begin{equation}
    \int_{A \times B}f = \int_B\left(\loint_A(f(x, y)dx)\right)dy = \int_B\left(\hiint_A(f(x, y)dx)\right)dy
  \end{equation}

  \item If \(g_x\) is integrable on \(B\) for every \(x \in A\), then
  \begin{equation}
    \int_{A \times B}f = \int_A\left(\int_Bf(x, y)dy\right)dx
  \end{equation}

  \item If
  \begin{equation}
    A = [a_1, b_1] \times [a_2, b_2] \times ... \times [a_n, b_n]
  \end{equation}
  we can apply Fubini's theorem repeatedly to obtain
  \begin{equation}
    \int_Af = \int_{a_n}^{b_n} \left(... \int_{a_2}^{b_2}\left(\int_{a_1}^{b_1}f(x_1,...,x_n)dx_1\right)dx_2...\right) dx_n
  \end{equation}
  if \(f\) is ``sufficiently nice'' (otherwise we'll have to sprinkle in some L's or U's).

\end{enumerate}

Now here's an application to \(\int_Cf\) when \(f\) is integrable on a Jordan-measurable set \(C \subset \reals^n\). What we're going to use is Fubini's theorem in a rectangle with the formula
\begin{equation}
  \int_Cf = \int_Af\indic{C}
\end{equation}
where \(A \supseteq C\) is a closed rectangle.
Let's do some quick examples:
\begin{enumerate}

\item Let's say we were integrating on the region
\begin{equation}
  C = [-1, 1] \times [-1, 1] \setminus \{x \in \reals^2, x_1^2 + x_2^2 < 1\}
\end{equation}
i.e. the unit square with the unit circle removed from it. We have
\begin{equation}
  \int_Cf = \int_{[-1, 1]^2}f\indic{C}
\end{equation}
We can write
\begin{equation}
  \indic{C} = \left\{\begin{array}{cc}
    1 & \text{if } -1 \leq y \leq -\sqrt{1 - x^2} \text{ or } \sqrt{1 - x^2} \leq y \leq 1 \\
    0 & \text{otherwise}
  \end{array}\right.
\end{equation}
We can write
\begin{equation}
  \int_{-1}^1f(x, y)\indic{C}(x, y)dy = \int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy
\end{equation}
i.e. the limits of integration correspond to the boundary of \(C\). So we get
\begin{equation}
  \int_Cf = \int_{-1}^1\left(\int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy\right)dx
\end{equation}

\item Say we want to integrate over a triangle \(C\), under the line from \((0, 0)\) to \((a, a)\). We have
\begin{equation}
  \int_Cf = \int_0^a\left(\int_y^af(x, y)dx\right)dy
\end{equation}
We could have done it the other way, integrating first with respect to \(y\) and then with respect to \(x\), and get
\begin{equation}
  \int_Cf = \int_0^a\left(\int_0^xf(x, y)dy\right)dx
\end{equation}
This shows the form might not be the same in different directions. And it could be important to exploit the difference. For example, what if our function \(f(x, y)\) only depends on one of the variables, say \(y\) (i.e. \(f\) is \textit{independent} of \(x\)). Then
\begin{equation}
  \int_0^a\left(\int_0^xf(y)dy\right)dx
\end{equation}
doesn't simplify very much, but
\begin{equation}
  \int_0^a\left(\int_y^af(y)dx\right)dy = \int_0^a(a - y)f(y)dy
\end{equation}
So this double integral reduces to a single integral with respect to \(y\).

\end{enumerate}

\TODO{proof}

\TODO{polish}
Before we move on, we will give some more examples of computing integrals, using Fubini's theorem and the change of variables theorems as tools.

Let us begin by recalling some facts about the regions on whuch we can integrate. What are the kinds of region on which we integrate? One of the kinds of regions would be the region between the graphs of two functions that are defined on a Jordan-measurable set. Specifically, if we have a subset \(C \subset \reals^n\) that is Jordan-measurable, and two integrable functions \(\varphi(x) \leq \psi(x)\) defined on \(C\), we're interested in integrating over the region bounded by the graphs of the two functions Let's call this region \(S\), that is, define
\begin{equation}
  S = \{(x, y) \in \reals^n \times \reals : x \in C, y \in [\varphi(x), \psi(x)]\}
\end{equation}

We have that \(S\) is a Jordan-measurable set. We want to be able to integrate a continuous function on \(S\). If \(f(x, y)\) is a bounded continuous on \(S\), then we can compute using Fubini's theorem that
\begin{equation}
  \int_Sf = \int_C\left(\int_{\varphi(x)}^{\psi(x)}f(x, y)dy\right)dx
\end{equation}
We went through the proof of this before, but I want to use it to compute some examples.
\begin{enumerate}

  \item Consider
  \begin{equation}
    \int_0^2\int_{y/2}^1ye^{-x^3}dxdy
  \end{equation}
  There's no hope in proceeding naively, since \(e^{-x^3}\) doesn't even have an elementary primitive, i.e. you cannot write down it's integral with only elementary functions. But let's consider what region we're integrating along: the area under the line between \((0, 0)\) and \((2, 1)\). So using the above observation, we can rewrite the integral to be
  \begin{equation}
    \int_0^1\left(\int_0^{2x}ye^{-x^3}dy\right)dx = \int_0^1\left.\frac{y^2}{2}e^{-x^3}\right|_0^{2x}dx = 2\int_0^1x^2e^{-x^3}dx = \left.-\frac{2}{3}e^{-x^3}\right|_{0}^{1} = \frac{2}{3}(1 - e^{-1})
  \end{equation}
  Here, the change of variable was a useful thing to do, because it enabled us to write down an original integral, which we couldn't do in the original form.

  \item Consider
  \begin{equation}
    \int_2^4\int_{4/x}^{\frac{20 - 4x}{8 - x}}(y - 4)dydx
  \end{equation}
  So here, again, should we just go ahead and do it as it's written? Well then we'll get \(\frac{y^2}{2} - 4y\) and we'll substitute those things in and we'll just get a terrible mess. But what's the region we're integrating over? \(\frac{4}{x}\) is like a hyperbola, which is 2 when \(x\) is 2 and 1 when \(x\) is 4. The other function on top, what does it look like? It's also a hyperbola: it's a constant plus something over \(8 - x\), so it'll open down. And this is the region we're integrating on: the area between a hyperbola open up and another opening down.

  If we change the order of integration, we'll be integrating on the outside with respect to \(y\), which goes from 1 to 2, and we'll be integrating on the inside with respect to \(x\) with \(x\) going from \(4/y\) to... let's solve:
  \begin{equation}
    y = \frac{20 - 4x}{8 - x} = 4 + \frac{12}{x - 8} \implies x - 8 = \frac{12}{y - 4} \iff x = 8 + \frac{12}{y - 4}
  \end{equation}
  Hence we can write the above as
  \begin{equation}
    \int_1^2\left(\int_{4/y}^{8 + 12/(y - 4)}(y - 4)dx\right)dy = \left.\int_1^2(y - 4)x\right|_{4/y}^{8 + 12/(y - 4)}dy = \int_1^2(y - 4)\left(8 + \frac{12}{y - 4} - \frac{4}{y}\right)dy
  \end{equation}
  which is all stuff that's very easy to integrate

  \item Let's see how to integrate a simple function \(z\) over a region of \(3\)-space
  \begin{equation}
    S = \{(x, y, z) \in \reals^3 : x^2 + y^2 \leq z^2 \land x^2 + y^2 + z^2 \leq 1\}
  \end{equation}
  So, what is this region? The second equation says we're inside the closed ball of radius 1 centered at the origin. The first equation is the region inside two  cones, opening up and down from the origin (kind of like an hour glass). So we want the intersection of these two regions. In terms of the theorem that we wrote down at the beginning, the set \(C\) is like the discs inside the circles formed by the intersections of the top and bottom of the cones and circles, and we're integrating on the region between the semicircles above and below the plane, and the cones above and below the plane. So what's this going to be? Well, by symmetry, it's going to be \textit{zero}.

  Let's make it a little harder. Let's consider only the top, i.e.
  \[S^+ = \{(x, y, z) \in S : z \geq 0\}\]
  So, rewriting our integral to be over the disc \(C\), we obtain
  \begin{equation}
  \begin{split}
    \int\int_{x^2 + y^2 \leq \frac{1}{2}}\left(\int_{\sqrt{x^2 + y^2}}^{\sqrt{1 - (x^2 + y^2)}}zdz\right)dxdy \\
    = \int\int_{x^2 + y^2 \leq \frac{1}{2}}\frac{1}{2}(1 - (x^2 + y^2) - (x^2 + y^2))dxdy \\
    = \frac{1}{2}\int\int_{x^2 + y^2 \leq \frac{1}{2}}dxdy - \int\int_{x^2 + y^2 - \frac{1}{2}}(x^2 + y^2)dxdy = \frac{1}{2}\pi\frac{1}{2} = \frac{\pi}{4}
  \end{split}
  \end{equation}
  How do we justify that last, ``magical'' step? Well, the prettiest way to do so is to change to polar coordinates, but we haven't justified this quite yet, which is the point of this example. You'll remember from first year caclulus that we can write
  \begin{equation}
    \int_a^bf(g(x))g'(x)dx = \int_{g(a)}^{g(b)}f(u)du
    \label{firstyearsub}
  \end{equation}
  This is what we're going to be doing in several variables. But first: polar coordinates? That means writing
  \begin{equation}
    x = r\cos\theta, y = r\cos\theta, \theta \in [0, 2\pi], r \in [0, \infty)
  \end{equation}
  This gives that
  \begin{equation}
    \prt{(x, y)}{(r, \theta)} = \begin{pmatrix}\cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta\end{pmatrix} \implies \det\prt{(x, y)}{(r, \theta)} = r \implies dxdy = drd\theta
  \end{equation}
  ``generalizing'' equation \ref{firstyearsub}. On the other hand, we have that
  \begin{equation}
    x^2 + y^2 = r^2
  \end{equation}
  Since we have that, for this disc, \(\theta\) ranges over the whole interval \([0, 2\pi]\) whereas \(r\) ranges over \([0, 1/\sqrt{2}]\), we can hence rewrite the above integral as
  \begin{equation}
    \int_0^{2\pi}\int_0^{\frac{1}{\sqrt{2}}}r^2 \cdot rdrd\theta =
    \int_0^{2\pi}d\theta\int_0^{\frac{1}{\sqrt{2}}}r^3dr =
    \left.2\pi\frac{r^4}{4}\right|_{0}^{\frac{1}{\sqrt{2}}} = \frac{\pi}{8}
  \end{equation}

\end{enumerate}


\subsection{Partitions of Unity}


\subsubsection{Bump Functions}

A bump function, in the one dimensional case, is a function which goes up to 1 from 0, stays there for a while, and then returns to zero. But we're interested in doing this in \(n\)-dimensions. How? Given an open \(U \subset \reals^n\), and compact \(C \subset U\), we want to construct a \(\mc{C}^\infty\) function \(f: U \to \reals\) with \(0 \leq f \leq 1\) such that \(\forall x \in C, f(x) = 1\) and \(f(x) = 0\) outside some compact subset of \(U\). So it looks like the 1-dimensional bump function, except over an \(n\)-dimensional area \(C\).

How can we construct such a function? We'll do it as a kind of series of exercises, based on one particular function, which you can imagine ought to be the kind of thing that you have to build this function, that is,
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    e^{1/x^2} & \text{if} & x \neq 0 \\
    0 & \text{if} & x = 0
  \end{array}\right.
  \label{bumpbuild}
\end{equation}
We proceed as follows, building up by steps:
\begin{enumerate}

  \item There is a \(\mc{C}^\infty\) function \(g: \reals \to \reals\) such that \(g > 0\) on \((-1, 1)\), 0 elsewhere. It's easy to write down a formula for such a function using the function given in \ref{bumpbuild}, as follows:
  \begin{equation}
    g(x) = \left\{\begin{array}{ccc}
      e^{-\frac{1}{(x - 1)^2}}e^{-\frac{1}{(x + 1)^2}} & \text{if} & x \in (-1, 1) \\
      0 & \text{if} & x \notin (-1, 1)
    \end{array}\right.
    \label{g}
  \end{equation}
  This works because \(e^{-\frac{1}{(x - 1)^2}}\) becomes completely flat at \(x = 1\), whereas \(e^{-\frac{1}{(x + 1)^2}}\) becomes completely flat at \(x = -1\).

  \item Given \(a \in \reals^n, \delta > 0\), there is a \(C^\infty\) function \(g_{a, \delta}: \reals^n \to \reals\) such that
  \begin{equation}
    g_{a, \delta} > 0 \text{ on } |x - a| < \delta, g_{a, \delta} = 0 \text{ outside}
  \end{equation}
  So how could achieve this? We could use the function defined in \ref{g} to write
  \begin{equation}
    g_{a, \delta} = g\left(\frac{|x - a|^2}{\delta^2}\right)
  \end{equation}

  \item There is a \(\mc{C}^\infty\) function \(h: U \to \reals\) such that \(h\) is greater than 0 on \(C\) and \(h = 0\) outside a compact subset of \(U\). So how do we do this?

  We could cover \(C\) with a finite set of open balls \(U_1,...,U_k\) with centers \(a_1,...,a_k\) and radii \(\delta_1,...,\delta_k\) whose closures lie in \(U\). We can do this since \(C\) is compact. Once we do this, we can define quite simply
  \begin{equation}
    h(x) = \sum_{i = 1}^kg_{a_k, \delta_k}(x)
    \label{compactsum}
  \end{equation}

  \item As a fourth thing, going back to the real line, let's show that if we're given any \(\epsilon \in \reals^+\), we can find a \(\mc{C}^\infty\) function \(\psi_\epsilon\) which is zero up to 0, becomes 1 at \(\epsilon\), and then remains 1 forever after. How can we do this from what we've already done? We're going to do it starting with a \(\mc{C}^\infty\) function like that in equation \ref{g}, but instead of on \((-1, 1)\), on \((0, \epsilon)\). More generally, actually, we can use any function \(g\) such that \(g > 0\) on \((0, \epsilon)\) and is zero outside. We can then define \(\psi_\epsilon\) by
  \begin{equation}
    \psi_\epsilon(x) = \frac{\int_0^xg}{\int_0^\epsilon g}
    \label{riseup}
  \end{equation}

  \item Now, putting everything together, how can we define the bump function that we wanted to begin with? We begin with the function \(h\) as defined in equation \ref{compactsum}, and we're going to modify it using equation \ref{riseup} as follows: letting \(\epsilon = \min_Ch > 0\), which exists since \(C\) is compact, let
  \begin{equation}
    f(x) = \psi_\epsilon \circ h
  \end{equation}

\end{enumerate}

Now there are other ways of doing it. One of them is to write down one integral formula from the beginning, but it would be harder to see just where this formula comes from in the first place.

\subsubsection{Definition}

We can now get to defining partitions of unity:
\begin{theorem}
  Given \(A \subset \reals^n\) and an open cover \(\mc{O}\) of \(A\), there is a countable collection \(\Phi\) of \(\mc{C}^\infty\) functions \(\varphi\) on \(\reals^n\) with the following properties:
  \begin{enumerate}

    \item \(0 \leq \varphi \leq 1\) \label{cond:partition_bounded}

    \item For all \(x \in A\), there is an open neighborhood \(V_x\) of \(x\) in \(\reals^n\) such that all but finitely many \(\varphi\) vanish on \(V_x\) \label{cond:finite_vanish}

    \item For all \(x \in A\), \(\sum_{\varphi \in \Phi}\varphi(x) = 1\) (sum is finite in a neighborhood of \(x\)) \label{cond:partition_sum_finite}

    \item For all \(\varphi \in \Phi\), there is some open \(U \in \mc{O}\) such that \(\varphi = 0\) outside a compact subset of \(U\). \label{cond:subordinate}

  \end{enumerate}

  \label{cheapway}
\end{theorem}
\begin{definition}
  A collection \(\varphi\) satisfying conditions
  \ref{cond:partition_bounded},
  \ref{cond:finite_vanish},
  \ref{cond:partition_sum_finite}
  above is called a \underline{\(\mc{C}^\infty\) partition for \(A\)}.
  If condition \ref{cond:subordinate} holds, this partition is called \underline{subordinate to \(\mc{O}\)}
\end{definition}
This is all \(\mc{C}^\infty\) stuff, but we could have made something weaker. We could have asked for a \(\mc{C}^r\) partition, or even a partition that was merely continous. So suppose we did that, and only cared about a \(\mc{C}^0\) partition. Then, we could also, though the proof will have to come next time, we could use functions that look like trapezoids, using \(|x|\) instead of \(e^{-\frac{1}{x^2}}\). If we wanted a \(\mc{C}^r\) partition of unity, then of course, we could do the same thing, but only caring about \(\mc{C}^r\) instead of \(\mc{C}^\infty\), and hence using the function given by
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    |x^{r + 1}| & \text{on} & [0, \infty) \\
    -|x^{r + 1}| & \text{on} & (-\infty, 0]
  \end{array}\right.
\end{equation}

Before we try to construct such a (\(\mc{C}^\infty\)) partition, let's discuss why this is an interesting idea. We're going to use this for several things in this course:
\begin{itemize}
  \item To extend the definition of the integral to more general regions
  \item To prove the change of variables theorem
  \item To extend the definition of the integral to manifolds
  \item To prove Stoke's theorem
\end{itemize}
So why? Suppose we wanted to extend the definition of the integral to manifolds. Take some shape in 3-space, with a boundary, which is a manifold. Right now, we can integrate over a rectangle. Once we prove the change of variables theorem, we'll be able to integrate over a space diffeomorphic to a rectangle, a sort of deformed rectangle. Now, if we have a coordinate chart, a diffeomorphism from \(\reals^n\) to some part of the manifold, we could use this to extend the definition of the integral from the rectangle to at least some kind of ``rectangular'' piece of the manifold. The question remains: how can we extend the definition to the \textit{entire} manifold? A reasonable way to try to do this is to try to cover the entire manifold by a grid of rectangles. If we add up the integrals on each of those pieces...

That was the way this was doen classically: you'd take a manifold and divide it up by some kind of rectangular grid, and just add up the contributions of every piece. This classical approach, however, ran into a big obstacle, which was: how do you know that you can really cover every manfiold with such a grid? How can you really divide up a really complex surface into rectangles? That is an extremely hard problem: it's true you can do it, but it's an extremely hard problem, and was solved way after the extension of integrals to manifolds.

So how can we avoid this issue? The way this was solved was, you do something a million times simpler. When you cover the manifold with rectangles, you don't care that they all fit together in a nice grid. Just, for every point, you take a rectangle containing that point, and we don't care whether they overlap or not. How do we exploit that? We use a partition of unity! Because what we'll do is, we'll pick a partition of unity such that every one of these bump functions sits in teh interior of one of these rectangles. Now, if we want to integrate our fucntion \(f\) over the manifold, and we have our partition of unity \(\Phi\), and we look at \(\varphi f\), it vanishes outside one of these rectangles, so we can just integrate over the rectangle and get the answer for the manifold. We can then write
\begin{equation}
  \int f = \sum\int\varphi f
\end{equation}
So the key is to take a function, and make it's support very small, and then work with those small pieces. And we're going to use this in the same way to prove Stoke's theorem. Stoke's theorem is like the FTC. What does the FTC say? It says that
\begin{equation}
  \int_a^bf' = f(a) - f(b)
\end{equation}
So the integral of \(f\) over the \underline{boundary} of \([a, b]\) is the same as the integral of the derivative of \(f\) over the whole interval. So Stoke's theorem is that the integral of a function over the boundary of a manifold is the same as the integral of the derivative over the whole manifold.

We can prove this for rectangles. Then, if we want to prove it for manifolds, again we need to try to cover the manfolds with a grid like this. If we can do that, we can prove Stoke's theorem for surfaces that we know can be covered by a grid. But we can do it for general manifolds in a really cheap way, again by multiplying with partitions of unity. So that's whay we're going to do in the rest of the course. But first, we have to prove Theorem \ref{cheapway}, because that's what we're going to use to glue together little pieces in a very nice but cheap way.

\begin{proof}

  The way we're going to do this is by considering a number of cases building up to the general case.

  \begin{enumerate}

    \item Assume \(A\) is compact. That means finitely many \(U \in \mc{O}\) cover \(A\). Call them \(U_1,...,U_q\). In this case we're going to construct a \textit{finite} (not countable) partition of unity subordinate to \(\{U_1,...,U_q\}\).

    For each \(i \in \{1,...,q\}\), the first thing that we're going to do is show that we can find a compact set \(D_i \subset U_i\) such that the interiors of the \(D_i\) cover \(A\). How?

    For all \(x \in A\), let \(D_x\) be a closed ball with centre \(x\) lying inside some \(U_i\). Since \(A\) is compact, finitely many of the interiors of the \(D_x\) cover \(A\). Number them \(D_{x_1},...,D_{x_p}\). For each \(i \in \{1,...,q\}\), let
    \begin{equation}
      D_i = \bigcup_{D_{x_j} \subset U_i}D_{x_j}
    \end{equation}

    Let \(\psi_i\) be a \(\mc{C}^\infty\) function \(0 \leq \psi_i \leq 1\) such that \(\psi_i > 0\) on \(D_i\), and is 0 outside a compact subset of \(U_i\). Notice if we do this, then, since the interior of the \(D_i\)'s cover's \(A\),
    \begin{equation}
      \forall x \in U \supseteq A, \sum_{i = 1}^q\psi_i(x) > 0
    \end{equation}
    where \(U\) is an open neighborhood of \(A\). SO we define our partition of unity as
    \begin{equation}
      \Phi = \{\varphi_1,...,\varphi_q\}, \varphi_i = \frac{\psi_i}{\sum_{i = 1}^q\psi_i}
    \end{equation}
    \label{compactcase}

    \item Now assume \(A = \bigcup_{i \in \nats}A_i\) where each \(A_i\) is compact and a subset of the interior of \(A_{i + 1}\). Let \(B_i = A_i \setminus \Int A_{i - 1}\). Let \(\mc{O}_i\) be covers of \(B_i\) by the open sets
    \begin{equation}
      U \cap (\Int A_{i + 1} \setminus A_{i - 2})
    \end{equation}
    for each \(U \in \mc{O}\). By Case \ref{compactcase} there is a finite partition of unity \(\Phi_i\) for \(B_i\) subordinate to \(\mc{O}_i\). Consider
    \begin{equation}
      \sigma = \sum_i\sum_{\psi_i \in \Phi_i}\psi_i
    \end{equation}
    This is a finite sum in some neighborhood of every \(x \in A\). Take for a partition
    \begin{equation}
      \varphi = \frac{\psi}{\sigma}
    \end{equation}
    for each \(\psi \in \Phi_i\), for every \(i\).

    \label{compacttowercase}

    \item Assume \(A\) is open. Then \(A = \bigcup_{i \in \nats}A_i\) as in Case \ref{compacttowercase}. How can we get such a representation? Try
    \begin{equation}
      A_i = \{x : |x| \leq i, d(x_, \reals^n \setminus A) > i^{-1}\}
    \end{equation}

    \label{opencase}

    \item Let \(A\) be arbitrary. Apply Case \ref{opencase} to \(\bigcup_{U \in \mc{O}}U\). A partition of unity subordinate to \(\mc{O}\) is also a partition of unity for \(A\) subordinate to \(\mc{O}\)

  \end{enumerate}

\end{proof}

\subsection{Extended Definition of the Integral (``Improper Integral'')}
Suppose that \(A\) is open and \(f\) is bounded and continuous. Then under the current definition, the expression
\begin{equation}
  \int_Af
\end{equation}
may not exist, since the boundary may not be Jordan-measurable. So we now want to try and rectify that, by attempting to extend the definition of the integral to open subsets \(A \subset \reals^n\) and \textit{locally} bounded functions \(f: A \to \reals\), i.e. for every point in \(A\), there is an open neighborhood around \(A\) such that \(f\) is bounded around \(A\). Furthermore, we assume the set of discontinuities is of measure zero.

Just like the improper integral from first year calculus, it won't always exist. But we'll want to know the value when it does. We'll begin with a useful note: if \(f\) vanishes outside of a compact subset \(C\) of \(A\), then in fact \(f\) \textit{is} integrable on \(C\) (since we can put that compact subset in a big rectangle and multiply by the characteristic function). In fact, we can say that \(f\) is integrable on any bounded open subset \(U\) of \(A\) containing \(C\), for the same reason, and
\begin{equation}
  \int_Uf = \int_Cf
\end{equation}
So it makes sense to say
\begin{equation}
  \int_Af = \int_Cf
  \label{conventioncompact}
\end{equation}
This is going to be our starting point. Now to go further, we're going to talk about a partition of unity. Let \(\mc{O}\) be an open cover of \(A\) such that \(U \subset A\) for all \(U \in \mc{O}\) and let \(\Phi\) be a partitionof unity for \(A\) subordinate to \(\mc{O}\). Note that this is the same as saying let \(\Phi\) be a partition of unity for \(A\) subordinate to \(\{A\}\), since that counts as an open covering of \(A\). As in equation \ref{conventioncompact},
we can define, \(\forall \varphi \in \Phi\),
\begin{equation}
  \int_A\varphi|f|
\end{equation}
We can now extend the definition of the integral as follows:
\begin{definition}
  \(f\) is \underline{integrable} on \(A\) if
  \begin{equation}
    \sum_{\varphi \in \Phi}\int_A\varphi|f|
    \label{absolute}
  \end{equation}
  converges. Then we define
  \begin{equation}
    \int_Af = \sum_{\varphi \in \Phi}\varphi f
    \label{relative}
  \end{equation}
\end{definition}
Note that equation \ref{relative} must converge, and in fact converges absolutely, if equation \ref{absolute} converges, because
\begin{equation}
  \sum_{\varphi \in \Phi}\left|\int_A\varphi f\right| \leq \sum_{\varphi \in \Phi}\int_A|\varphi||f| = \sum_{\varphi \in \Phi}\int_A\varphi f
\end{equation}
since \(\varphi\) is nonnegative.
\begin{theorem}
  Suppose \(A \subset \reals\) is open, \(f: A \to \reals\) is locally bounded and the set of discontinuities of \(f\) has measure 0.
  \begin{enumerate}

    \item Given partitions \(\Phi, \Psi\) as above, not necessarily subordinate to the same open covering, though this doesn't matter since both will be subordinate to \(\{A\}\), then if equation \ref{absolute} converges for \(\Phi\), then it converges for \(\Psi\) and the integral defined using \(\Phi\) and the integral defined using \(\Psi\) are equal, i.e.
    \begin{equation}
      \sum_{\psi \in \Psi}\int_A\psi f = \sum_{\varphi \in \Phi}\int_A\varphi f
      \label{integralequal}
    \end{equation}
    Note that equation \ref{integralequal} would not necessarily be true if equation \ref{absolute} does not converge.

    \item The new definition of the integral \textit{always} exists if both \(A\) and \(f\) are bounded, showing it truly generalizes the old definition. Specificially, if both \(A\) and \(f\) are bounded, whether or not the boundary of \(A\) has measure zero,
    \begin{equation}
      \sum_{\varphi \in \Phi}\int_A\varphi|f|
    \end{equation}
    (equation \ref{absolute}) converges

    \item If \(A\) is Jordan-measurable and \(f\) is bounded, then
    \begin{equation}
      \int_Af = \sum_{\varphi \in \Phi}\int_A\varphi f
    \end{equation}
    where the left is as defined before.

  \end{enumerate}
\end{theorem}
There are other ways of defining the integral, some of which look more like the improper integral from first year calculus. Another thing that you could do is the following: since you can always write an open set as a kind of ``expanding union'' of compact sets \(C_i \subseteq C_{i + 1}\), you can actually do that in such a way such that each \(C_i\) is Jordan-measurable, meaning the integral on each of these \(C_i\)'s exist. We could then define
\begin{equation}
  \int_Af = \lim_{i \to \infty}\int_{C_i}f
\end{equation}
After we finish proving this theorem, either I'll prove this or stick it on the next problem set.
\begin{proof}
  \begin{enumerate}

    \item \(\varphi \cdot f\) vanishes outside a compact set, and only finitely many \(\psi\) are nonzero on this set. So
    \begin{equation}
      \sum_{\psi \in \Psi}\psi = 1
      \implies \sum_{\varphi \in \Phi}\int_A\varphi f
      = \sum_{\varphi \in \Phi}\int_A\left(\sum_{\psi \in \Psi}\psi\right)\varphi f
      = \sum_{\varphi \in \Phi}\sum_{\psi \in \Psi}\int_A\varphi\psi f
    \end{equation}
    A priori, we don't know that we can interchange the order in the double sum of the last equal expression, but we could write down exactly the same stuff with the absolute value in it, which tells us we can. Interchanging them, we get the desired result.

    \item Let \(B\) be a big rectangle containing \(A\), and assume \(|f| \leq M\) on \(A\). For any finite subset \(F\) of \(\Phi\),
    \begin{equation}
      \sum_{\varphi \in F}\int_A\varphi|f| \leq M\int_A\sum_{\varphi \in F}\varphi \leq MV(B)
    \end{equation}
    implying the desired result. We still have to show that in the situation that \(\int_Af\) is defined according to our earlier definition, we get the same result. We'll do so next time.

  \end{enumerate}
\end{proof}

Let's recap. What we've shown so far is that
\begin{enumerate}

  \item The existence of the integral and it's value is independent of the parititon of unity chosen to compute it.

  \item The integral \textit{always} exists if \(A\) and \(f\) are both bounded.

  \item We \textit{want} to show that if we're in a situation where the integral exists according to our old definition, so that in particular \(A\) and \(f\) will be bounded, then our new definition agrees with the old one. That is, if moreover, \(A\) is Jordan measurable, then
  \begin{equation}
    \int_Af = \sum_{\varphi \in \Phi}\int_A\varphi f
  \end{equation}
  where the left hand side is computed using the old definition
\end{enumerate}
\begin{proof}
  For every \(\epsilon > 0\), there is a compact Jordan-measurable subset \(C \subset A\) such that \begin{equation}
    V(A \setminus C) = \int_{A \setminus C}1 = \int_A\chi_{A \setminus C} < \epsilon
  \end{equation}
  So how do we get this \(C\)? We're given that \(A\) is Jordan measurable, so as \(A\) is bounded we can cover the boundary of \(A\) by the interiors of finitely many closed balls \(\mc{B}\) (or rectangles) of total volume \(< \epsilon\). So we can take
  \begin{equation}
    C = A \setminus \Int\bigcup\mc{B}
  \end{equation}
  (we could also take the union of interiors). Why is \(C\) Jordan measurable? Since the boundary of \(C\) is a subset of the boundary of \(\Int\bigcup\mc{B}\), which is has measure zero, it follows that \(C\) has boundary of measure zero and is hence Jordan measurable.

  Since \(C\) is compact, only finitely many of our partition functions \(\varphi \in \Phi\) are nonzero on \(C\). So we'll take a finite sum, look at the difference, and show that that difference goes to zero: consider any finite subset \(F\) of \(\Phi\) including the ones which are nonzero on \(C\). Consider
  \begin{equation}
    \left|\int_Af - \sum_{\varphi \in F}\int_A\varphi f\right|
    \label{absint}
  \end{equation}
  As the absolute value of the integral is less than or equal to the integral of the absolute value, we have that equation \ref{absint} is less than or equal to
  \begin{equation}
    \int_A\left|f - \sum_{\varphi \in F}\varphi f\right|
    \label{intabs}
  \end{equation}
  Since \(f\) is bounded, we can choose \(M\) such that \(|f| \leq M\). So equation \ref{intabs} is less than or equal to
  \begin{equation}
    M\int_A\left(1 - \sum_{\varphi \in F}\varphi\right)
    = M \int_A\sum_{\varphi \in \Phi \setminus F}\varphi
    \label{intrem}
  \end{equation}
  Since
  \begin{equation}
    \forall \varphi \in \Phi \setminus F, \forall c \in C, \varphi(c) = 0
  \end{equation}
  we have that equation \ref{intrem} is less than or equal to
  \begin{equation}
    MV(A \setminus C) \leq M\epsilon
  \end{equation}

\end{proof}
As I mentioned last time, you can do this in another way which perhaps looks more like the improper integral from first year calculus, and that's integrating with bigger and bigger sets. Let me state that, but I won't prove it, and rather put it on the problem set:
\begin{theorem}
  Let \(A\) be open, \(f: A \to \reals\) be locally bounded and let the set of discontinuties of \(f\) have measure zero. Write
  \begin{equation}
    A = \bigcup_{n \in \nats}C_n
  \end{equation}
  where each \(C_n\) is compact and Jordan Measurble and satisfy \(C_n \subset \Int C_{n + 1}\). Part of the exercise is to show that we can do this. Then \(f\) is integrable on \(A\) if and only if
  \begin{equation}
    \left\{\int_{C_i}|f| : n \in \nats\right\}
  \end{equation}
  is bounded. Note that if this is the case, the sequence
  \begin{equation}
    \left\{\int_{C_i}f : n \in \nats\right\}
  \end{equation}
  converges absolutely, and we can write
  \begin{equation}
    \int_Af = \lim_{n \to \infty}\int_{C_n}f
  \end{equation}
\end{theorem}
\begin{proof}
  Exercise.
\end{proof}
It's good to appreciate that the improper integral which we defined using a partition of unity can be expressed in this way. It's also really good to do the above exercise as an exercise in using partitions of unity.

There's a little gap in the treatment of partitions of unity last time which I want to fill here. In the first case, where \(A\) was compact, we defined
\begin{equation}
  \psi_1,...,\psi_p
\end{equation}
such that \(\psi_1 + ... \psi_p > 0\) on an open set \(U \supseteq A\). We then let
\begin{equation}
  \varphi_i = \frac{\psi_i}{\psi_1 + ... \psi_p}
\end{equation}
This is all well and good, but individual \(\varphi_i\) might be defined on sets that go outside \(U\), and we have to worry about dividing by zero to guarantee that each bump function is \(\mc{C}^\infty\) everywhere. To deal with this, we can simply multiply everything by another bump function (we can find one which is 1 on \(A\) and zero outside a compact set \(U\)).

Let \(f(x)\) be a \(\mc{C}^\infty\) bump function such that \(f = 1\) on \(A\), \(f = 0\) outside a compact subset of \(U\). Then the set of functions
\begin{equation}
  \{f \cdot \varphi_i : i \in 1,...,p\}
\end{equation}
is a partition of unity as required.

Before we finish, I want to say something about integration by substitution, or change of variables.

\subsection{Change of Variables}

Recall the single-variable \underline{integration by substitution} formula
\begin{equation}
  \int_{g(a)}^{g(b)} f = \int_a^b(f \circ g)g'
  \label{onesubform}
\end{equation}
This holds whenever ``both sides make sense'', e.g. if \(f\) is continuous and \(g\) is continuously differentiable. The hypotheses, however, could be weaker. When we talk about integration by substitution, our \(g\) is going to be the change of variables. But change of variables means like ``invertible''. So not just any \(g\), but in particular 1-to-1. I wanted to point out how, in terms of our formula, we should think about this in the case where \(g\) is one-to-one. So suppose it is so. Then equation \ref{onesubform} can be rewritten as
\begin{equation}
  \int_{g([a, b])}f = \int_{[a, b]}(f \circ g)|g'|
\end{equation}
since otherwise the equaton would not be correct if \(g\) was decreasing. So this is what our change-of-variables formula is going to look like. Let's formalize this into a theorem:

\begin{theorem}[Change of Variables]
  Let \(A \subset \reals^n\) be open, \(g: A \to \reals^n\) be one to one, continuously differentiable and let, for all \(x \in A\), \(g'(x) \neq 0\). Then
  \begin{equation}
    f : g(A) \to \reals
  \end{equation}
  is integrable if and only if
  \begin{equation}
    f \circ g|\det g'|
  \end{equation}
  is integrable on \(A\). In this case,
  \begin{equation}
    \int_{g(A)}f = \int_Af \circ g|\det g'|
  \end{equation}
  \label{covtheorem}
\end{theorem}

\subsubsection{Examples}

Let's look at some examples, starting with polar coordinates: we use coordinates \(r \in \reals^+_0, \theta \in [0, 2\pi]\) and write
\begin{equation}
  x = r\cos\theta, y = r\sin\theta
\end{equation}
We have
\begin{equation}
  D = \prt{(x, y)}{(r, \theta)} = \begin{pmatrix}
    \cos\theta & -r\sin\theta \\
    \sin\theta & r\cos\theta
  \end{pmatrix} \implies \det D = r
\end{equation}
So we can write
\begin{equation}
  \iint_Af(x, y)dxdy = \iint_Af(r\cos\theta, r\sin\theta)rdrd\theta
\end{equation}
We now examine a corollary of Theorem \ref{covtheorem}.

\begin{corollary}
  Let \(A \subset C \subset \reals^n\) where \(A\) is open, \(C\) is compact and Jordan-measurable and \(C \setminus A\) has measure zero. If \(g\) is a continuously differentiable function from a neighborhood of \(C\) to \(\reals^n\) wich satisfies the conditions of theorem \ref{covtheorem} on \(A\), then
  \begin{equation}
    f: g(C) \to \reals
  \end{equation}
  is integrable if and only if
  \begin{equation}
    f \circ g|\det g'|
  \end{equation}
  is iintegrable on \(C\), and in this case
  \begin{equation}
    \int_{g(C)}f = \int_Cf \circ g|\det g'|
  \end{equation}
  \label{corcov}
\end{corollary}
\begin{lemma}
  Assume \(A \subset \reals^n\) is open and \(g: A \to \reals^n\) is continuously differentiable. If \(B \subset A\) has measure zero, then \(g(B)\) has measure zero.
\end{lemma}
\begin{proof}
  Enough to prove that \(g(B \cap C)\) has measure zero for any \(C \subset A\) compact, since \(A\) has an exhaustion by countably many compact sets \(C_1 \subset C_2 \subset ...\)

  To do so, remember that a countable intersection of measure 0 sets is measure 0. Using \(\mc{C}_1\), which is more than uniformly continuous, we have that
  \begin{equation}
    \forall x \in C, \forall y \in U, |g(x) - g(y)| \leq c|x - y|
  \end{equation}
  where \(U\) is some neighborhood of \(C\). So \(g\) maps a ball of radius \(\epsilon\) to a ball of radius \(c\epsilon\).
\end{proof}
We now proceed to prove Corollary \ref{corcov}
\begin{proof}
  \(g(C) \setminus g(A) \subseteq g(C \setminus A)\), and so it is of measure zero. We hence have that
  \begin{equation}
    \int_{g(A)}f = \int_{g(C)}f
  \end{equation}
  \begin{equation}
    \int_A(f \circ g)|\det g'| = \int_{C}(f \circ g)|\det g'|
  \end{equation}
  giving the desired equality by Theorem \ref{covtheorem}
\end{proof}
Let's move on to another example: what are called spherical coordinates. We use coordinates \(r \in \reals^+_0, \phi \in [0, 2\pi], \theta \in [0, \pi]\) where
\begin{equation}
  x = r\cos\phi\sin\theta, y = r\sin\phi\sin\theta, z = r\cos]theta
\end{equation}
We have... this is going to hurt...
\begin{equation}
  D = \prt{(x, y, z)}{(r, \theta, \phi)} = \begin{pmatrix}
    \cos\phi\sin\theta & r\cos\phi\cos\theta & -r\sin\phi\sin\theta \\
    \sin\phi\sin\theta & r\sin\phi\cos\theta & r\cos\phi\sin\theta \\
    \cos\theta & -r\sin\theta & 0
  \end{pmatrix} \implies \det D = r^2\sin\theta
\end{equation}
Now, for some problems:
\begin{enumerate}

  \item Find the volume of an ellipsoid
  \begin{equation}
    E = \left\{(x, y, z) \in \reals^3 : \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \leq 1\right\}
  \end{equation}
  We can make a change of variable to ``transform'' this into a sphere. Specifically, we can use the transformation
  \begin{equation}
    g : \reals \to \reals^3, (x, y, z) \mapsto (x/a, y/b z/c)
  \end{equation}
  to take the ellipsoid to the unit ball, where \((u, v, w) = g(x, y, z)\). So
  \begin{equation}
    \iiint_{B_1(\mb{0})}1dudvdw = \iiint_{E}\frac{1}{abc}dxdydz = \frac{1}{abc}\iiint_{E}1dxdydz = \frac{4}{3}\pi \implies V(E) = \frac{4}{3}\pi abc
  \end{equation}
  Now what if we didn't know the volume of a ball of radius 1? We could figure it out using change of variables, using spherical coordinates:
  \begin{equation}
    \int_{0}^{2\pi}\int_{0}^{\pi}\int_0^1r^2\sin\theta drd\theta d\phi = \int_0^1r^2dr\int_0^\pi\sin\theta d\theta\int_0^{2\pi}d\phi = \frac{1}{3} \cdot 2 \cdot 2\pi = \frac{4\pi}{3}
  \end{equation}
  Of course, the volume of the sphere of radius \(r\) can be treated as a special case of the ellipsoid where \(a = b = c = r\), giving volume \(\frac{4}{3}\pi r^3\).

  \item What's the simplest kind of change of variable, save the identity? Of course, a linear change of variable. Consider a linear transformation \(T: \reals^n \to \reals^n\) and suppose \(A \subset \reals^n\) is Jordan measurable and \(B = T(A)\). Show that \(B\) is Jordan measurable and \(V(B) = V(A)|\det T|\). We split this into two cases:
  \begin{enumerate}

    \item Assume \(T\) is not invertible, i.e. \(\det T = 0\). Then the image \(I\) of \(T\) is a linear subspace of \(\reals^n\) of dimension less than \(N\), and \(B \subset I\). So all of \(B\) has \(n\)-dimensional measure zero, and hence \(V(B) = 0 = V(A)|\det T|\) as desired.

    \item Assume \(T\) is invertible, i.e. \(\det T \neq 0\). Then \(B\) is Jordan measurable and now by the change of variable theorem
    \begin{equation}
      V(B) = \int_B1 = \int_A1 \cdot |\det T| = |\det T|\int_A1 = V(A)|\det T|
    \end{equation}
    as desired.

  \end{enumerate}

\end{enumerate}

\subsubsection{Proof of the Change of Variables Theorem}

We're now going to prove the Change of Variables theorem. Let's begin by considering just one direction:
\begin{theorem}[Change of Variables (One Direction)]
  Let \(A \subset \reals^n\) be open and let \(g: A \to \reals^n\) be one to one and continuously differentiable such that \(\det g'(x) \neq 0\) on \(A\). Then if \(f: g(A) \to \reals\) is integrable, \(f \circ g|\det g'|\) is integrable on \(A\) and
  \begin{equation}
    \int_{g(A)}f = \int_A(f \circ g)|\det g'|
  \end{equation}
  \label{changeofvar1d}
\end{theorem}
Note that the converse, and hence the full theorem, follows, since
\begin{itemize}
  \item As \(g\) is invertible \(f\) is locally bounded if and only if \(f \circ g|\det g'|\) is locally bounded
  \item The discontinuities of \(f\) have measure zero if and only if the discontinuities of \(f \cdot g|\det g'|\) have measure zero, since \(g\) is differentiable, using the lemma from last time.
  \item We can use \(g^{-1}\) to go from \(g(A)\) to \(A\).
\end{itemize}
We still have to show that if \(F = f \circ g|\det g'|\) is integrable on \(A\), then \(f\) is integrable on \(g(A)\). All we have to do is apply the theorem with \(F), g^{-1}\) instead of \(f, g\). Then
\begin{equation}
  F \circ g^{-1}|\det(g^{-1})'| = f \circ \cancel{g \circ g^{-1}} \cancel{|\det g' \circ g^{-1}|}\frac{1}{\cancel{|\det g' \circ g^{-1}|}} = f
\end{equation}
as desired.

We can now begin to prove the theorem in a number of steps. We're going to start by looking at a Jordan-measurable open subset \(U\) of \(A\) with closure in \(A\), e.g. an open rectangle with closure in \(A\). The first thing that we're going to do is show that it's ``good enough'' to prove the theorem on such an open subset. How? With a partition of unity! That's going to simplify things a lot, but just one thing I want to note first is that if \(f\) is integrable on \(g(A)\), then \(f\) is integrable on \(g(U)\) and \(f \circ g|\det g'|\) is integrable on \(U\).


We observe that, if \(U\) is a Jordan-measurable subset of \(A\) with closure in \(A\), then if \(f: g(A) \to \reals\) is integrable then \(f\) is integrable on \(g(U)\) and \(f \circ g|\det g'|\) is integrable on \(U\).

We're now going to attempt to prove Theorem \ref{changeofvar1d} in small steps:
\begin{enumerate}

  \item Suppose \(\mc{O}\) is an open cover of \(A\) by sets like \(U\) above. Suppose that for each \(U\), the theorem holds, i.e. \(f\) is integrable on \(g(U)\) and
  \begin{equation}
    \int_{g(U)}f = \int_U(f \circ g)|\det g'|
  \end{equation}
  Then the theorem is true. What we're saying here is it's good enough to, instead of proving the thoerem as stated, just prove it for thse Jordan-measurable \(U\)'s. So, why is this true?

  If we have an open covering \(\mc{O}\) of \(A\), taking the images of everything in the cover, the set
  \begin{equation}
    \mc{O}' = \{g(U) : U \in \mc{O}\}
  \end{equation}
  is an open cover of \(g(A)\) of some kind, since \(g\) is invertible. Now let \(\Phi\) be a \(\mc{C}^0\) partition of unity for \(g(A)\) subordinate to \(\mc{O}'\).

  That means each partition function \(\varphi \in \Phi\) is zero outside of a compact subset of some \(g(U)\). But notice that, if this is true, then \(\varphi \circ g\) must be zero outside of a compact subset of \(U\). This is because \(g\) is invertible. This is the last place in the proof that we're going to use the fact that \(g\) is one-to-one.

  So this tells us that
  \begin{equation}
    \Phi' = \{\varphi \circ g : \varphi \in \Phi\}
  \end{equation}
  forms a partition of unity for \(A\) subordinate to \(\mc{O}\). So we have that \textit{by our assumption}
  \begin{equation}
    \int_{g(A)}\varphi \circ g = \int_{g(U)}\varphi \circ f = \int_U(\varphi \cdot f) \circ g|\det g'| = \int_A(\varphi \cdot f) \circ g|\det g'|
  \end{equation}
  by the nature of partitions of unity. So, the thing that we're interested in, the integral of \(f\) over \(g(A)\), can be written, since \(\Phi\) is a partition of unity, as
  \begin{equation}
    \int_{g(A)}f = \sum_{\varphi \in \Phi}\int_{g(A)}\varphi f = \sum_{\varphi \in \Phi}\int_A(\varphi \cdot f) \circ g|\det g'| = \int_Af \circ g|\det g'|
    \label{thisequality}
  \end{equation}
  Note that the same equality as \ref{thisequality} with \(|f|\) in place of \(f\) shows that the RHS converges, which is our definition of intregrability.

  We could make the same conclusion that the theorem is true not by assuming this for an open covering of \(A\) but rather looking at the same things for an open covering of \(g(A)\). That is, it's equivalent for this claim to suppose that if we have an open covering \(\mc{O}\) of \(g(A)\) by open subsets of this kind (Jordan measurable, closure lying in \(g(A)\)), then, for all \(V \in \mc{O}\) and all \(f\) integrable on \(V\),
  \begin{equation}
    \int_Vf = \int_{g^{-1}(V)}(f \circ g)|\det g'|
  \end{equation}

  \item It's enough to prove that for every open rectangle \(V\) in \(g(A)\) with closure in \(g(A)\) we have
  \begin{equation}
    \int_Vf = \int_{g^{-1}(V)}(f \circ g)|\det g'|
  \end{equation}
  whenever \(f\) is integrable on \(V\). If I just said that, that would be no further reduction at all, since this is just what we already said. But that's not what I'm going to say here. What I'm going to say is that it's enough to prove this when \(f\) is the constant function \(1\). So this is a big reduction.

  Again, when I say it's ``enough'' to prove that, the meaning is that if you prove this, then the theorem follows, i.e. it is enough to prove the theorem. Why is this the case, however?

  Let \(P\) be a partition of \(V\). For every subrectangle \(S \in P\), let \(f_S\) be the constant function \(m_S(f)\). We have
  \begin{equation}
    \mc{L}(f, P) = \sum_{S \in P}m_S(f)V(S) = \sum_{S \in P}\int_{\Int S}f_S
    \label{partint}
  \end{equation}
  By this assumption for constant functions, we can rewrite equation \ref{partint} as
  \begin{equation}
    \sum_{S}\int_{g^{-1}(\Int S)}f_S \circ g|\det g'| \leq \sum_{S}\int_{g^{-1}(\Int S)}f \circ g|\det g'| \leq \int_{g^{-1}(V)}f \circ g|\det g'|
  \end{equation}
  So what does this tell us about the integral of \(f\) over \(V\)? We have that
  \begin{equation}
    \int_Vf = \sup_P\mc{L}(f, P) \leq \int_{g^{-1}(V)}f \circ g|\det g'|
  \end{equation}
  The same argument using upper sums gives \(\geq\).

  \item If the theorem is true for \(g : A \to \reals^n\), and \(h: g(A) \to \reals^n\), with the same hypotheses (one-to-one, \(\mc{C}^1\), \(\det h' \neq 0\)) then it is true for \(h \circ g\). Why?
  \begin{equation}
    \int_{(h \circ g)(A)}f = \int_{g(A)}(f \circ h)|\det h'| = \int_A((f \circ h) \circ g)|\det h' \circ g||\det g'| = \int_Af \circ (h \circ g)|\det (h \circ g)'|
  \end{equation}
  by the Chain Rule.
  \label{p3}


  \item The theorem is true if \(g\) is an (invertible) linear transformation. We already showed that, for any \(g\), it's good enough to prove that the theorem is true on an open rectangle \(U\) with the constant function \(f = 1\), i.e. that if \(U\) is an open rectangle in \(A\), then
  \begin{equation}
    \int_{g(U)}1 = \int_U|\det g'| \iff \Vol(g(U)) = |\det g|\Vol(U)
  \end{equation}
  Note that we've already proved this in linear algebra, using the volume of parallelepipeds. But let's do it analytically.

  We will make use of the fact that any invertible linear transformation (i.e. invertible \(n \times n\) matrix) is a composite of elementary transformations (elementary matrices). Last time we showed that if the theorem is true for \(g, h\), then it is true for \(g \circ h\). Hence, all we need to do is show that the theorem is true for an arbitrary elementary transformation (matrix). We have 3 kinds of elementary matrices:
  \begin{itemize}

    \item A matrix which has all 1's on the diagonal, except for \(\lambda\) in one place. In this case, \(\det g = \lambda\), so we could use Fubini's theorem to scale by \(\lambda\), giving \(\Vol(g(U)) = \lambda \Vol(U) = |\det g|\Vol(U)\) as desired.

    \item A matrix which has all 1's on the diagonal, except for two adjacent rows which have \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\) on the diagonal. We have \(\det g = \pm 1\). Since we're just changing two coordinates, the integral remains unchanged.

    \item The identity matrix, plus \(1\) in a position \(i, j\) with \(i \neq j\). In this case, the determinant is just \(1\). Furthermore, we're just translating the top of our rectangle into a parallelogram. Fubini's theorem tells us that the volume is the same since the limits in terms of \(y\) are the same, and for fixed \(y\) we are integrating over a shifted integral of the same length, the shifting not having an effect since we are integrating a constant.

  \end{itemize}
  \label{p4}


\end{enumerate}
We can now return to the proof of the theorem: by \ref{p3} and \ref{p4}, we can assume that, given \(a \in A, g'(a) = I\), where \(I\) is the identity. Why? Let \(T = g'(a)\) be an invertible linear transformation. Then we can write
\begin{equation}
  g = T \circ (T^{-1} \circ g)
\end{equation}
By \ref{p4}, this is true for \(T\). By 3, if this is true for \(T\) and \(T^{-1} \circ g\) then it's true for \(g\). So it's enough to prove for the case
\begin{equation}
    (T^{-1} \circ g)' = I
\end{equation}
To complete the proof, we use induction on \(n\):
\begin{itemize}

  \item \(n = 1\): by integration by substitution

  \item Assume it's true for \(n - 1\). It's enought to show that, for all \(a \in A\), we can find Jordan measurable open \(U \subset A\) containing \(a\), with closure in \(A\), so that the theorem is true on \(U\) when \(f = 1\).

  We can assume \(g'(A) = I\). We'll express \(g\) as the composition of 2 mappings, each of which changes at most \(n - 1\) coordinates. There are a number of ways to do this. One is defining \(h : A \to \reals^n\) by
  \begin{equation}
    h(x) = (g_1(x),...,g_n(x), x_n)
  \end{equation}
  Why is this a change of variable? \(h'(a) = I\), so there is an open neighborhood \(U'\) of \(a\) on which \(h\) is one to one and \(\det h'(a) \neq 0\). Now define
  \begin{equation}
    k: h(U') \to \reals^n, k(y) = (y_1,...,y_{n - 1}, g_n(h^{-1}(y_n)))
  \end{equation}
  We have \(g = k \circ h\),
  \begin{equation}
    g'(a) = k'(h(a))h'(a) = I \implies k'(h(a)) = I
  \end{equation}
  So there is an open neighborhood \(V\) of \(h(a)\) in \(U'\) on which \(k\) is one to one and
  \begin{equation}
    \det k'(g) = 0
  \end{equation}
  Let \(U = h^{-1}(V)\). On \(U\), \(g = k \circ h\), and \(h: U \to V\), \(k: V \to \reals^n\).

  So we have to prove the theorem for \(h\) and for \(k\). The proof is going to be the same for both cases, so let's do it for the harder case, \(h\), which changes \(n - 1\) coordinates (versus \(k\), which changes only one).

  We're going to use Fubini's theorem: we'll integrate with respect to \(x_n\). We've previously found that it's enough to show that, for every \(a \in A\), there is a Jordan measurable open neighborhood \(U\) of \(a\) with closure \(A\) so that the result is true on \(U\) in the case where we are integrating the function \(f = 1\). Furthermore, we have shown that we can assume that \(g'(a) = I\). We also expressed \(g\) as a composite of two functions, each of which changes fewer than \(n\) variables. More precisely, we wrote
  \begin{equation}
    g = (x \mapsto  (g_1(x),...,g_{n - 1}(x), x_n)) \circ (y \mapsto (y_1,...,y_{n - 1},g_n(h^{-1}(y))))
  \end{equation}
  We will proceed by induction, using the single variable case as a base case.

  We can work with any Jordan-measurable open neighborhood of \(a\), so let's work with an open rectangle. Consider specifically an open rectangle \(W\) containing \(a\) with closure in \(A\). Since we're separating one coordinate from the other \(n - 1\) coordinates, let's write \(W\) as \(D \times (a_n, b_n)\) where \(D\) is an open rectangle in \(\reals^{n - 1}\). Note that the transformation \(h\) does not change the \(n^{th}\) coordinate of it's input. Using this, we can consider the following:
  \begin{equation}
    \int_{h(W)}1
  \end{equation}
  For a fixed \(x_n\), let's consider the function given by \(h\), depending only on \(1,...,n - 1\), written
  \begin{equation}
    h_{x_n} : D \to \reals^{n - 1},
    h_{x_n}(x_1,...,x_{n - 1})(g_1(x_1,...,x_n),...,g_{n - 1}(x_1,...,x_n))
  \end{equation}
  This function is one to one, since we know that \(h\) is one to one, and \(h\) is just \(h_{x_n}\) with \(x_n\) appended (for each \(x_n\)), so two different values at a given \(x_n\) have got to go into different points. On the other hand, what is the determinant
  \begin{equation}
    (\det h'_{x_n})(x_1,...,x_{n - 1})
  \end{equation}
  Well, it's the same as the determinant of \(h'\), since the last row of \(h'\) is just the standard vector \(e_n\). So let's go back: first of all,
  \begin{equation}
    h(D \times \{x_n\}) = h_{x_n}(D) \times \{x_n\}
  \end{equation}
  since, like we said before, the \(n^{th}\) coordinate is unaffected. So we can proceed using Fubini's theorem,:
  \begin{equation}
    \int_{a_n}^{b_n}\left(\int_{h_{x_n}(D)}1\right)dx_n
  \end{equation}
  By induction, this is equal to
  \begin{equation}
    \int_{a_n}^{b_n}\int_D1|\det h'_{x_n}| = \int_{a_n}^{b_n}\int_D|\det h'| = \int_{D \times [a_n, b_n]}1 \cdot |\det h'| = \int_W 1 \cdot |\det h'|
  \end{equation}
  By strong induction, the theorem is also true for \(k\), giving the desired result.
\end{itemize}

Now that we finished this, what to do? We've been talking about the integral on \(\reals^n\). In the rest of the course, we're going to be talking about the integral but on a manifold.

\section{Manifolds}

\TODO{this}

\subsection{What is a manifold?}

\TODO{this}

\subsection{Functions Between Manifolds}

\TODO{this}

\subsection{Manifolds with Boundary}

\TODO{this}

\subsection{Multilinear Algebra}

\TODO{this}

\subsection{Vector Fields and Differential Forms}

\TODO{this}

\subsection{The Differential Operator}

\TODO{this}

\section{Integration on Manifolds}

We haven't talked about manifolds for a while, but I hope you can remember how to imagine one because I'm too lazy to {\LaTeX} one. So what's the point of integrating on our imaginary manifold picture? We know how to integrate on a line, and how to integrate now in \(\reals^n\), and in both cases we're integrating with some notion of \(n\)-dimensional volume. But now, on our \(k\)-dimensional manifold, we're going to have to try to figure out how to integrate using a \(k\)-dimensional measure of volume on it, which is nontrivial.

Another reason is the following: if we go back to one dimensional calculus, and we want to compue the integral of some function \(f\) from \(a\) to \(b\), the very important and aptly named Fundamental Theorem of Calculus tells us that if \(f\) has a primitive \(g\) it's integral is very easy to compute:
\begin{equation}
  \int_a^bf = g(b) - g(a)
\end{equation}
We want to try to do the same thing with a manifold: generalize the fundamental theory of calculus to relate the integral of a function on the manifold to the integral of a function on the boundary. But what sense does that make? A function on a manifold, unlike that on a line, depends on many variables. So we don't have a notion of primitive like in single variable calculus, since an \(n\) dimensional function has many partial derivatives.

So those are the two things we're going to have to understand to get started:
\begin{itemize}
  \item What is this \(k\) dimensional measure of volume?
  \item What is the appropriate notion of a primitive?
\end{itemize}
But before that, we're going to do a bit of motivation, and consider the case of a one-dimensional manifold, or as you know it, a curve:

\subsection{Integration of Parametrized Curves}

We now consider the case of a parametrized Curve \(C\) in \(\reals^n\). Let's start from the beginning: what is a parametrized curve? It means \(C\) is the image of a mapping
\begin{equation}
  \gamma : [a, b] \to \reals^n
\end{equation}
We could write, for example, \(x = \gamma(t)\). We're going to assume that our curves define something almost like a manifold, except we're going to allow that the curves cross each other. So assume that \(\gamma\) is \underline{piecewise \(\mc{C}^1\)}. What this means is, except for some finite number of points (since we're working on a compact interval), it's \(\mc{C}^1\). Furthermore, we assume that except for isolated points, \(\gamma' \neq 0\) and \(\gamma\) is one to one.
We don't want to allow, for example,
\begin{equation}
  \gamma(t): [0, 3\pi] \to \reals^2 = t \mapsto (\cos t, \sin t)
\end{equation}
since we don't want a whole big region (in this case a semicircle) where \(\gamma\) overlaps itself.

So the first item of business is: what's the measure of volume on such a curve? Length, of course. Arc length, specifically. We have that
\begin{equation}
  \ell(C) = \int_a^b|\gamma'(t)|dt
\end{equation}
We're not assuming that people have seen this before, so the question is, why? Let's deal with length in a more concrete, geometric way. How can you do that? In the same way that you define the integral: you try to approximate by something that is easy to compute. So one good way to approximate length is to divide up the integral into a partition and look at the corresponding line segments: we define length as the supremum of the length of polygonal approximations to the curve.

Let's let \(P = \{t_0,t_1,...,t_k\}\) be a partition of \([a, b]\), and let \(\ell_P(C)\) be the length of the associated polygonal approximation:
\begin{equation}
  \ell_p(C) = \sum_{j = 1}^k|\gamma(t_j) - \gamma(t_{j - 1})|
\end{equation}
We'll say that the curve has length if the supremum of these things exists, and we'll define it as the length. Formally,
\begin{definition}
  The curve \(C\) is \underline{rectifiable} if
  \begin{equation}
    \sup_P\ell_P(C)
    \label{lensupremum}
  \end{equation}
  exists, in which case we define \ref{lensupremum} it to be the \underline{length} of \(C\), written \(\ell_P(C)\).
\end{definition}
Here's a brief homework exercise: prove that if \(\gamma\) is piecewise \(\mc{C}^1\), then \(C\) is rectifiable and \(\ell(C)\) is given by equation \ref{lensupremum}. This is a first year calculus exercise using Riemann sums (I think it's in Spivak), so it shouldn't be too hard.



\TODO{this}

\subsection{Integral of a \(k\)-form over a \(k\)-cube}

\TODO{this}

\subsection{Integration of Differential Forms on Manifolds}

\TODO{this}

\subsection{Manifolds with Boundary}

\TODO{this}

\subsection{Stoke's Theorem on Manifolds}

\TODO{this}

\end{document}
