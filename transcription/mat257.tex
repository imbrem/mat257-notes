\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MAT257 Notes}
\author{Jad Elkhaleq Ghalayini}
\date{April 2019}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
  colorlinks,
  linkcolor={red!50!black},
  citecolor={blue!50!black},
  urlcolor={blue!80!black}
}

\usepackage[margin=1in]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem{exercise}{Exercise}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Vol}{vol}
\DeclareMathOperator{\D}{D}

\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\ints}[0]{\mathbb{Z}}
\newcommand{\rationals}[0]{\mathbb{Q}}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\sbrac}[1]{\left[#1\right]}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\eval}[3]{\left.#3\right|_{#1}^{#2}}
\newcommand{\ip}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\prt}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\hlfspc}[0]{\mathbb{H}}
\newcommand{\loint}[0]{\operatorname{L}\int}
\newcommand{\hiint}[0]{\operatorname{U}\int}
\newcommand{\indic}[1]{\chi_{#1}}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} #1}}


\begin{document}

\maketitle

\tableofcontents

This document is a collection of notes for the course MAT257: Analysis II, as taught by Professor Edward Bierstone in 2018 at the University of Toronto. The notes are a combination of notes I made in class (which can be found in their original form in the \verb|notes| folder in this repository) and scans of handwritten notes which Professor Bierstone has generously given me the permission to use.

\section{Introduction}

\subsection{Differentiability}
Let's recall for a moment what it means for a function to be differentiable at a point:
\begin{equation}\exists f'(a) = \lim_{h \to 0}\frac{f(a + h) - f(a)}{h}\end{equation}
Of course, we can bring everything to the right and write
\begin{equation}\lim_{h \to 0}\frac{f(a + h) - (f(a) + f'(a)h)}{h} = 0\end{equation}
The advantage of stating it this way is that we see that the linear function
\begin{equation}f(a) + f'(a)h\end{equation}
is the \textit{best linear approximation} to \(f(a + h)\) at \(h = 0\). In fact, it's the unique linear function \(y = b + mh\) such that
\begin{equation}\lim_{h \to 0}\frac{f(a + h) - (b + mh)}{h} = 0\end{equation}
Should this be called linear? I don't know. Maybe it would be better to call it affine.

From the point of view of linear algebra, we would say that a function which takes
\begin{equation}h \mapsto f'(a) \times h\end{equation}
is a linear transformation. So the derivative can be thought of as a number, but it can also be thought of as a linear transformation from the real line to itself. And this is how we're going to define, and work with, derivatives in higher dimensions.

\subsection{Functions of several variables}

We're not going to be working on the real line, instead, we'll be working on \(\reals^n\): the space of \(n\)-tuples of real numbers. Instead of functions \(f(x)\) of a single real variable \(x\), we will hence be working with functions of \(n\) real variables, or if you like, functions of a single vector variable, where \(x \in \reals^n\), of the form
\begin{equation}f(x) = f(x_1,...,x_n)\end{equation}
Such a function is differentiable at a point \(a = (a_1,...,a_n)\) if ``there's a thing that plays the role of the derivative.'' We could think about a deriative in \(\reals\) as a real number, but we could also think about it as a linear transformation, which is what we're going to do. That is, a derivative exists, if there exists a linear transformation
\begin{equation}\lambda: \reals^n \to \reals\end{equation}
such that
\begin{equation}\lim_{h \to 0}\frac{f(a + h) - (f(a) + \lambda(h))}{|h|} = 0\end{equation}
What's a linear transformation from \(\reals^n \to \reals\). Well first of all, what's \(h\)? \(h = (h_1,...,h_n)\) is a vector. So \(\lambda\) can be thought of as a sort of row matrix such that
\begin{equation}\lambda(h) = \begin{pmatrix}\lambda_1 & ... & \lambda_n\end{pmatrix}\begin{pmatrix} h_1 \\ \vdots \\ h_n\end{pmatrix}\end{equation}
Sometimes, we'll write \(\lambda(h)\) simply as \(\lambda h\). This right away is why linear algebra is important. As you can imagine, if we want to analyze the effect of a given derivative on this function, we're going to need to analyze this matrix, so linear algebra is going to be really crucial. In fact, a lot of the things that we do in the course can be done and were done classically with a really minimal amount of linear algebra. In fact this \(\lambda\) is a matrix, but it's entries are what we call the partial derivatives of \(f\), so we can work only with a collection of partial derivatives. However, what we'll see is, from a conceptual perspective, there's a lot to be gained from the linear agebraic approach.

Analysis involves making approximations and estimations, like \(\epsilon-\delta\) proofs in calculus. Of course, \(\epsilon-\delta\) proofs involve measuring the size of a quantity or distance, and those are going to be really essential concepts in this course. That already shows up in the \(|h|\) in the equation above, which is defined
\begin{equation}|h| = \sqrt{h_1^2 + ... + h_n^2}\end{equation}
the \underline{norm} of \(h\). It's a measure of the size of \(h\). If we write the norm
\begin{equation}|x - y|\end{equation}
this is a measure of the size of the difference between \(x\) and \(y\), or the \underline{distance} between \(x\) and \(y\). But especially in high dimension, even in one dimension but especially in high dimensions, there may be many different ways of measuring distance which are essentially equivalent.

\subsection{Equivalent Norms}
This is one norm, but in fact there are many different but equivalent norms. For example, some norms include
\begin{equation}|x| = \left(\sum_{i = 1}^nx_i^2\right)^{1/2}\end{equation}
\begin{equation}||x|| = \max\{|x_1|,...,|x_n|\}\end{equation}
\begin{equation}|||x||| = \sum_{i = 1}^n|x_i|\end{equation}
These are all equivalent, which means that if you pick any two of them, the first one is less than or equal to a constant times the second one. Why is that? What's the relationship between them?

It's a very good idea, in, anything, I guess, to understand things geometrically. And this actually can be seen just on the level of a picture of \(\reals^2\). After all, what if we take our first norm. What does it mean to say that the norm is less than or equal to \(r\)? It means that our point is inside a circle of radius \(r\). On the other hand, what does it mean for the \textit{second} norm to be less than or equal to \(r\)? It means that both norms have to be inside the square of side length \(r\) centered at the origin. And finally, what does it mean for the third norm to be less than or equal to \(r\)? That means we're inside the smaller square, or diamond, formed by the points
\begin{equation}(0, r), (0, -r), (r, 0), (-r, 0)\end{equation}
Of course, there would be \(n\)-dimensional versions of this picture. But what does this picture say about the relationship between these norms? It tells us that
\begin{equation}||x|| \leq |x| \leq |||x|||\end{equation}
How do we prove this analytically? We just take the square of everything!

Now how do we show that
\begin{equation}|||x||| \leq a|x|\end{equation}
for some constant \(a\)? We can also think about this geometrically: take our circle of radius \(r\). We need the first diamond which is outside of the circle. And what distance is the corner of the diamond from the origin? In the plane, it's \(\sqrt{2}r\). In higher dimensions, it's \(\sqrt{n}r\). So what this tells us is that
\begin{equation}|||x||| \leq \sqrt{n}|x|\end{equation}
or, analytically,
\begin{equation}|||x||| = \ip{x}{u(x)} \leq |x||u(x)|\end{equation}
where
\begin{equation}u_i(x) = \text{sgn}(x_i) = \left\{\begin{array}{ccc} 1 & \text{if} & x_i \geq 0 \\ -1 & \text{if} & x_i < 0 \end{array}\right.\end{equation}
Now, how do we show that
\begin{equation}|x| \leq a||x||\end{equation}
Well, let's again look at our square of side \(r\). Clearly, the circle circumscribing this square has radius \(\sqrt{n}r\), in \(n\) dimensions. We should finish this by doing the exercise of proving this analytically


You all know that \(\reals^n\) is the space of \(n\)-tuples of real numbers,
\begin{equation}x = (x_1,...,x_n)\end{equation}
\(\reals^n\), of course, is a vector space with the operations
\begin{equation}x + y = (x_1 + y_1,...,x_n + y_n)\end{equation}
\begin{equation}ax = (ax_1,...,ax_n)\end{equation}
These are the vector space operations on \(\reals^n\) that everyone should be familiar with from linear algebra. We, however, are going to be even more interested in the \textit{metric} properties of such a metric space, and these are going to be dependent on the idea of norm that we talked about last time.

We defined a number of norms, including the norm
\begin{equation}|x| = \sqrt{x_1^2 + ... + x_n^2}\end{equation}
Of course, this norm is intimately connected with the usual inner product on \(\reals^n\),
\begin{equation}\ip{x}{y} = x_1y_1 + ... + x_ny_n\end{equation}
What are the basic relations between this and the vector space structure? These should be things you're familiar with, but let's recall.

Let's consider two vectors \(x, y \in \reals^n\). We know that
\begin{enumerate}
  \item \(|x| \geq 0\) and \(|x| = 0 \iff x = 0\)
  \item \(|\ip{x}{y}| \leq |x||y|\), with equality iff \(x, y\) are linearly dependent.
  \item \(|x + y| \leq |x| + |y|\), with equality iff one is zero or if they point in the same direction.
\end{enumerate}
\begin{proof}
\begin{itemize}

  \item [2.] We have equality if \(x, y\) are linearly dependent. But what if \(x, y\) are linearly independent. Then, by definition
  \begin{equation}\forall \lambda \in \reals, y - \lambda x \neq 0\end{equation}
  \begin{equation}\implies 0 < |y - \lambda x|^2
  = \ip{y}{y} - 2\lambda\ip{x}{y} + \lambda^2\ip{x}{x}\end{equation}
  \begin{equation}= |y|^2 - 2\lambda\ip{x}{y} + \lambda^2|x|^2\end{equation}
  This is a quadratic polynomial in \(\lambda\) which is never 0, so hence has a discriminant
  \begin{equation}4\ip{x}{y}^2 - 4|x|^2|y|^2 < 0 \implies \ip{x}{y}^2 < |x|^2|y|^2\end{equation}
  \begin{equation}\implies |\ip{x}{y}| < |x||y|\end{equation}
  as desired.

  \item [3.] \(|x + y|^2  = \ip{x + y}{x + y} = |x|^2 + |y|^2 + 2\ip{x}{y}
  \leq |x|^2 + |y|^2 + 2|x||y| = (|x| + |y|)^2\).
\end{itemize}
\end{proof}

What if we have two vectors \(x\) and \(y\), and we want to define the angle between them. How should we do that? We can try

\begin{definition}
\begin{equation}\angle(x, y) = \arccos\frac{\ip{x}{y}}{|x||y|}\end{equation}
\end{definition}
Why should this, however, be the definition? It comes from the elementary geometric relationship between the length of \(y - x\) and the lengths of \(x\), \(y\) when formed into a triangle:
\begin{equation}|y - x|^2 = |x|^2 + |y|^2 - 2|x||y|\cos\theta\end{equation}
which can be rewritten to the familiar
\begin{equation}\ip{x}{y} = |x||y|\cos\theta\end{equation}

\subsection{Linear transformations from \(\reals^n \to \reals^m\)}

Usually, we represent linear transformations by a matrix. But the matrix, of course, corresponds to a choice of basis vectors. Let's recall what we usually denote the standard basis of \(\reals^n\)

\subsubsection{Standard basis of \(\reals^n\)}

The standard basis of \(\reals^n\) is composed of \(e_1,...,e_n\), where each \(e_k\) is 0 everywhere except in the \(k^{th}\) place, where it is 1.

Let's assume \(T: \reals^n \to \reals^m\) is a linear transformation. What do we mean by the matrix of \(T\) with respect to the standard basis? Let
\begin{equation}e_1,...,e_n \in \reals^n\end{equation}
\begin{equation}f_1,...,f_m \in \reals^m\end{equation}
be the standard bases.

Our transformation is a matrix \(A\) that we multiply a certain vector \(x = (x_1,...,x_n) \in \reals^n\) by to get a vector \(y = (y_1,...,y_m) \in \reals^m\). Let's label the elements of \(A\) \(a_{11},...,a_{mn}\), getting
\begin{equation}\begin{pmatrix} y_1 \\ \vdots \\ y_m \end{pmatrix} =
\begin{pmatrix}
  a_{11} & ... & a_{1n} \\
  \vdots & \ddots & \vdots \\
  a_{m1} & ... & a_{mn}
\end{pmatrix}\begin{pmatrix}
  x_1 \\ \vdots \\ x_n
\end{pmatrix}\end{equation}
But note we can write
\begin{equation}x = \sum_{i = 1}^nx_ie_i\end{equation}
And note that
\begin{equation}T(e_i) = \sum_{j = 1}^ma_{ji}f_j\end{equation}
i.e. the \(i^{th}\) column.

So what we're really doing is taking one sum of basis vectors into another.

Now what if we have a separate linear transformation? If \(S: \reals^m \to \reals^\ell\) is another linear transformation, that means we can compose them: we can apply \(S\) to \(T(x)\). Then
\begin{equation}S \circ T: \reals^n \to \reals^\ell\end{equation}
Let's suppose \(S\) has a matrix \(B\). And what's the matrix of \(S \circ T\)? It is, of course, \(BA\).

Ok, finally, what about \textit{bounding} the size of a matrix applied to a vector? This is going to be a basic bound that we use all the time.

If \(T: \reals^n \to \reals^m\) is a linear transformation, then there is a constant \(M \geq 0\) such that
\begin{equation}|Tx| \leq M|x|\end{equation}
Why?
\begin{equation}T(x) = T\left(\sum x_ie_i\right) = \sum x_iT(e_i)\end{equation}
Let's estimate the norm:
\begin{equation}|T(x)| \leq \sum_{i = 1}^n|x_iT(e_i)| = \sum_{i = 1}^n|x_i||T(e_i)|\end{equation}
So how can we use this to get that \(M\)? We can just take
\begin{equation}C = \max_{i}|T(e_i)|\end{equation}
then
\begin{equation}|T(x)| \leq C\sum|x_i|\end{equation}
This is not quite what we wanted, because this is not that single barred norm of \(x\). This was a different norm of \(x\). But what was the relationship between the two norms?
\begin{equation}\sum|x_i| \leq \sqrt{n}|x|\end{equation}
So we have
\begin{equation}|T(x) \leq \sqrt{n}C|x|\end{equation}

\(\reals^n\) with any of our norms is what's called a \textit{metric space}. All a metric space means is a set \(X\) together with a distance function
\begin{equation}d: X \times X \to \reals, (x, y) \mapsto d(x, y)\end{equation}
such that
\begin{itemize}
  \item \(\forall x, y \in X, d(x, y) \geq 0\)
  \item \(\forall x, y, z \in X, d(x, z) \leq d(x, y) + d(y, z)\)
  \item \(d(x, y) = 0 \iff x = y\)
\end{itemize}

In this course, we're going to talk a little bit about the topology of \(\reals^n\), that is, talking about what are called open and closed sets, what the relationship between them is, and how this notion relates to continuity.

In fact, we're only going to be talking about a very special class of topological spaces: metric spaces. In fact, they're even a very special class of metric spaces: \(\reals^n\) or certain subsets of \(\reals^n\) with the Euclidean metric.

That's what we're going to be talking about tonight (note: I cannot attend, so no notes are available for this lecture): the topology of \(\reals^n\).

\TODO{this}

\subsection{Topology of \(\reals^n\)}

\TODO{this}

\subsubsection{Continuity}

\begin{definition}
  Consider a function \begin{equation}f: A \to \reals^n\end{equation}
\end{definition}
\(f\) is \underline{continuous} at \(A\) if for every \(\epsilon > 0\), \(\exists \delta > 0\) such that \(|f(x) - f(a)| < \epsilon\) whenever \(|x - a| < \delta, x \in A\).
\begin{theorem}
  \(f\) is continuous if and only if for every open \(U \subset \reals^n\), \(f^{-1}(U) = A \cap V\) where \(V\) is open in \(\reals^n\).
\end{theorem}
\begin{proof}

  \begin{itemize}

    \item [\(\implies\)] consider \(U\) open in \(\reals^n\). Let \(a \in f^{-1}(U)\). For some \(\epsilon > 0\), \(\exists\) a ball \(B_{f(a)} = B(f(a), \epsilon) \subset U\) because \(U\) is open.

    Since \(f\) is continuous at \(a\), \(\exists \delta > 0\) such that \(f(x) \in B_{f(a)}\) for every \(x \in A \cap B(a, \delta) = B_a\). We can define
    \begin{equation}V = \bigcup_{a \in f^{-1}(U)}B_a\end{equation}

    \item [\(\impliedby\)] Consider \(a \in A\) and let \(\epsilon > 0\). Let \(U = B(f(a), \epsilon)\). Then \(f^{-1}(U) = A \cap V\) for some open \(V \subset \reals^n\).
    \begin{equation}a \in V \implies \exists \delta > 0, B(a, \delta) \subset V\end{equation}
    But everything which lies inside \(V\) gets mapped inside \(U\) which is that first ball that we started with. And this is the condition that we want.

  \end{itemize}
\end{proof}

\begin{corollary}
  A composite of continuous functions is continuous
\end{corollary}
\begin{proof}
  Let
  \begin{equation}f: A \to \reals^n, g: B \to \reals^p\end{equation}
  be continuous functions where \(B \subset \reals^n\) and \(f(A) \subset B\).

  Consider open \(U \subset \reals^p\). We know
  \begin{equation}\exists V \subset \reals^n \text{ open, } g^{-1}(U) = B \cap V\end{equation}
  So we can write
  \begin{equation}(g \circ f)^{-1}(U) = f^{-1}(g^{-1}(U)) = f^{-1}(B \cap V) = f^{-1}(V)\end{equation}
  We know that
  \begin{equation}\exists W \subset \reals^n \text{ open, }, f^{-1}(V) = A \cap W = (g \circ f)^{-1}(U)\end{equation}
  But this is exactly what we wanted to show
\end{proof}

\begin{exercise}
  Let \(f = (f_1,...,f_n)\) be a function from \(\reals\) to \(\reals^n\). Then \(f\) is continuous if and only if \(\forall i \in \{1,...,n\}\), \(f_i\) is continuous.
\end{exercise}
\begin{proof}
  \begin{itemize}

    \item [\(\impliedby\)] We need to estimate the norm \(|f(x) - f(a)|\) with the differences between \(f_i(x_i)\) and \(f_i(a_i)\). We could use a variety of inequalities for this, including
    \begin{equation}|f(x) - f(a)| \leq \sum_{i = 1}^n|f_i(x) - f_i(a)|\end{equation}
    The full \(\epsilon\)-\(\delta\) argument is left as an exercise.

    \item [\(\implies\)] We need to estimate the norm \(|f_i(x) - f_i(a)| < |f(x) - f(a)|\). Alternatively, we can do this topologically. The full argument is left as an exercise.

  \end{itemize}

\end{proof}
\begin{exercise}
  A linear transformation \(T: \reals^m \to\reals^n\) is \underline{uniformly} continuous
\end{exercise}
\begin{definition}
   There is a norm \(M > 0\) such that \(|T(x)| \leq M|x|\). Hence,
   \begin{equation}|T(x) - T(y)| = |T(x - y)| \leq M|x - y|\end{equation}
   Given \(\epsilon\), take \(\delta = \frac{\epsilon}{M}\).
\end{definition}
\begin{exercise}
  Are the following functions continuous?
  \begin{enumerate}
    \item \(f(x, y) = \frac{x^2 - y^2}{x^2 + y^2}\) - No
    \item \(f(x, y) = \frac{x^2 + 3xy + y^2}{x^2 + 4xy + y^2}\) - No
    \item \(f(x, y) = e^{-\frac{|x - y|}{x^2 - 2xy + y^2}} = e^{-\frac{1}{|x - y|}}\) - Yes
  \end{enumerate}
\end{exercise}
\begin{proof}
  \begin{enumerate}
    \item \(f\) is 1 along the line \(\{f(x, 0) : x \in \reals\}\), but -1 along the line \(\{f(0, y) : y \in \reals\}\), both of which traverse the origin.
    \item No for the same reason, but we can't check on the axes, since each axis is 1. Instead, we can check on the \(x\) axis and compare that with any line except the \(y\) axis, such as \(y = x\), where the value is \(\frac{5}{6}\)
    \item Composite of \(z \mapsto e^{\frac{1}{|z|}}\) and \((x, y) \mapsto x - y\)
  \end{enumerate}
\end{proof}
\begin{exercise}
  Let \(X \subseteq \reals^n\). We define the \underline{distance function}
  \begin{equation}d(x, X) = \inf_{a \in X}|x - a|\end{equation}
  \(\forall X \in \mc{P}(\reals^n), d(x, X)\) is uniformly continuous on \(\reals^n\)
\end{exercise}


\subsubsection{Compactness}


\begin{definition}
  A subset \(X\) of \(\reals^n\) is \underline{compact} if every open covering of \(X\) has a finite subcover.
\end{definition}

We're later going to prove a deep theorem about compactness:
\begin{theorem}
  A subset \(X\) of \(\reals^n\) is compact if and only if \(X\) is closed and bounded
\end{theorem}
We won't do this right now, because it'll involve some work, but we'll use something called the Heine-Borel Theorem: \([0, 1]\) is compact. We're going to prove this exactly the same way as we proved the ``Three Hard Theorems'' from first year calculus. It's a good exercise to try this.


Today, we'll do half of this theorem, the easy part: compact \textit{implies} closed and bounded.
\begin{lemma}
  If \(X \subset \reals^n\) is compact then \(X\) is closed and bounded.
\end{lemma}
\begin{proof}
  \begin{itemize}
    \item \(X\) is closed: to say that \(X\) is closed is of course the same thing as saying \(\reals^n \setminus X\) is open. To show this, we take any point in \(\reals^n \setminus X\), and show there's some ball centered at that point which is a subset of \(\reals^n \setminus X\).

    So let \(x \in \reals^n \setminus X\). We want to show that \(\exists \delta > 0\), \(B(a, \delta) \subset \reals^n \setminus X\) for some \(\delta > 0\). Let's just look at all possible balls centered at \(A\). Or we might as well just consider, for some \(k \in \nats\), the closed ball \(\overline{B(a, k^{-1})}\). Let's look at the complement of this closed ball,
    \begin{equation}U_k = \reals^n \setminus \overline{B(a, k^{-1})}\end{equation}
    We have that
    \begin{equation}\bigcup_kU_k = \reals^n\setminus\{a\}\end{equation}
    So \(\{U_k\}\) is an open cover of \(X\). But \(X\) is compact, so this open cover has a finite subcover. This fact tells us that \(\exists k\), \(X \subset U_k\). But \(U_k\) is the complement of the closed ball of radius \(\frac{1}{k}\), meaning the open ball
    \begin{equation}B(a, k^{-1}) \subseteq \reals^n \setminus X\end{equation}

    We can clearly see that this implies \(\reals^n \setminus X\) is open, implying \(X\) is closed.

    \item \(X\) is bounded:  consider a cover of \(X\) by all open balls of radius 1 in \(\reals^n\).
  \end{itemize}
\end{proof}

\begin{theorem}
  If \(X \subset \reals^m\) is compact and \(f: X \to \reals^n\) is continuous then \(f(X)\) is pact
\end{theorem}
\begin{proof}
  Let \(\mc{O}\) be an open cover of \(f(X)\). For every \(U \in \mc{O}\), \(f^{-1}(U) = X \cap V_U\) where \(V_U\) is open in \(\reals^m\). So
  \begin{equation}\{V_U : U \in \mc{O}\}\end{equation}
  is an open cover of \(X\). Since \(X\) is compact, there is a finite subcover
  \begin{equation}V_{U_1},...,V_{U_k}\end{equation}
  So \(U_1,...,U_k\) cover \(f(X)\).
\end{proof}

\begin{theorem}[Extreme Value Theorem]
  A continuous function \(f: X \to \reals\) on a nonempty compact subset \(X\) of \(\reals^n\) attains a maximum and minimum
\end{theorem}
\begin{proof}
  Let \(M = \sup\{f(x) : x \in X\}\). \(M < \infty\) since \(f(X)\) is bounded. What if \(M \notin f(X)\)? Since \(f(X)\) is closed, we can draw a small interval around \(M\) which is not in \(f(X)\), contradicting the fact that \(M = \mbox{lub} f(X)\).

  The proof for minima is analogous.
\end{proof}

\begin{definition}
  The \(\epsilon\)-neighborhood of \(X\) is given by
  \begin{equation}\bigcup_{x \in X}B(x, \epsilon) = \{y \in \reals^n : d(y, X) < \epsilon\}\end{equation}
\end{definition}

\begin{theorem}[\(\epsilon\)-neighborhood theorem]
  If \(X \subset U \subset \reals^n\) where \(X\) is compact and \(U\) is open, then there is \(\epsilon > 0\) such that the \(\epsilon\)-neighborhood of \(X\) lies in \(U\).
\end{theorem}
\begin{proof}
  Define \(f: X \to \reals\) by \(f(x) = d(x, \reals^n \setminus U)\). We showed this is a continuous function, which is strictly positive (since \(U\) is open). Hence, by the Extreme Value Theorem, it has a positive minimum. Take \(\epsilon\) to be this minimum.
\end{proof}

\section{Differentiation}

\subsection{Definitions}

\TODO{copy beginning}

Examples:
\begin{enumerate}
  \item Let \begin{equation}f(x, y) = \int_a^{x + y}g\end{equation} where \(g: \reals \to \reals\) is continuous. We compute \(Df(c, d)\) as follows:
  We have that \(f = q \circ s\), where
  \begin{equation}q(t) = \int_a^tg, s(x, y) = x + y\end{equation}
  We have
  \begin{equation}Df(c, d) =  g'(s(c, d))s'(c, d) = g(c + d)(c + d)\end{equation}
  since \(q'(t) = g(t)\).

  \item Let \begin{equation}f(x, y) = \int_a^{x^y}g\end{equation} We have
  \begin{equation}f = g \circ \xi, \xi(x, y) = x^y = e^{y\log x}\end{equation}
  Hence,
  \begin{equation}D\xi(x, y) = x^yD(y\log x) = x^y((0, 1)\log x + (1/x, 0)y) = x^y(y/x, \log x)\end{equation}
  \begin{equation}\implies Df(c, d) = q'(\xi(c, d))\xi'(c, d) = g(c^d)c^d(d/c, \log c)\end{equation}
\end{enumerate}

\subsubsection{Higher-order Derivatives}
Let \(f: U \to \reals\) be a function where \(U \subseteq \reals^m\) and suppose
\begin{equation}D_if = \prt{f}{x_i}: U \to \reals\end{equation}
exists for all \(i\). So we could now consider
\begin{equation}D_j(D_if) = \prt{}{x_j}\left(\prt{f}{x_i}\right)\end{equation}
We'll write \(D_{ij}f\) to denote the above. Notice \(i\) is applied \textit{first}. We may also write \(f_{x_ix_j}\) and \(\frac{\partial^2 f}{\partial x_j \partial x_i}\). We have that
\begin{equation}\frac{\partial^2 f}{\partial x_j \partial x_i}(a) = \frac{\partial^2 f}{\partial x_i \partial x_j}(a)\end{equation}
if both mixed partials exist and are continuous in a neighborhood of \(a\) (proof uses \(\int\)).

In general, we can consider taking higher order partials as well, as in
\begin{equation}\frac{\partial^{\alpha_1 + ... + \alpha_n}f}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}\end{equation}
Of course we have to worry about the order, but the order is irrelevant if \(f\) is \(\mc{C}^\infty\), i.e. that all partial derivatives of all orders exist (and are hence continuous).

\subsubsection{Multi-index notation}
In multi-index notation, \(\alpha = (\alpha_1,...,\alpha_m)\) is a vector of non-negative integers. We define the \textit{total order of \(\alpha\)}
\begin{equation}|\alpha| = \alpha_1 + ... + \alpha_m\end{equation}
Furthermore, we define
\begin{equation}x = (x_1,...,x_m) \implies x^\alpha = x_1^{\alpha_1}...x_m^{\alpha_m}\end{equation}
Once we look at Taylor's theorem, the notation
\begin{equation}x! = x_1!...x_m!\end{equation}
will also come in handy.

We can now perform another example:
\begin{itemize}

  \item [3.] Let
  \begin{equation}f(x, y) = \left\{\begin{array}{cc}
    xy\frac{x^2 - y^2}{x^2 + y^2} & (x, y) \neq (0, 0) \\
    0 & (x, y) = (0, 0)
  \end{array}\right.\end{equation}
  Is this function differentiable at the origin? Yes: \(f'(0, 0) = (0, 0)\). Let's check: we need to show that
  \begin{equation}\lim_{(x, y) \to (0, 0)}\frac{f(x, y) - f(0, 0) - \cancel{(0, 0)\begin{pmatrix} x \\ y \end{pmatrix}}}{\sqrt{x^2 + y^2}} = \lim_{(x, y) \to (0, 0)}xy\frac{x^2 - y^2}{x^2 + y^2}\frac{1}{\sqrt{x^2 + y^2}} = 0\end{equation}
  We have that
  \begin{equation}\left|xy\frac{x^2 - y^2}{x^2 + y^2}\frac{1}{\sqrt{x^2 + y^2}}\right| \leq \frac{|xy|}{\sqrt{x^2 + y^2}} \to 0\end{equation}

\end{itemize}

\subsection{Partial Derivatives}
\begin{theorem}
  If \(U \subset \reals^m\), \(f: U \to \reals^n\) is differentiable at \(a\) and \(f = (f_1,...,f_n)\) then each partial derivative \(\prt{f_i}{x_j}(a)\) exists, and
  \begin{equation}f'(a) = \begin{pmatrix}
    \prt{f_1}{x_1}(a) & ... & \prt{f_1}{x_m}(a) \\
    \vdots & \ddots & \vdots \\
    \prt{f_n}{x_1}(a) & ... & \prt{f_n}{x_m}(a)
  \end{pmatrix}\end{equation}
  with
  \begin{equation}\prt{f}{x_j}(a) = \left.\prt{}{x}f(a_1,...,a_{j - 1}, x, a_{j + 1},..., a_m)\right|_{x = a_j}\end{equation}
\end{theorem}
\begin{proof}
  \begin{itemize}

    \item Case \(n = 1\): Let
    \begin{equation}h(x) = (a_1,...,a_{j - 1},x,a_{j + 1},...,a_m): \reals \to \reals^m\end{equation}
    Then
    \begin{equation}\prt{f}{x_j}(a) = D(f \circ h)(a_j) = Df(h(a_j)) \circ Dh(a_j)
     = f'(a) \cdot \begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix}\end{equation}
     where the matrix is \(1 \times m\) and 1 in the \(j^{th}\) place, i.e. the \(j^{th}\) entry in \(f'(a)\).

     \item For \(n\) in general,
     \begin{equation}f = (f_1,...,f_n), f'(a) = \begin{pmatrix} f_1'(a) \\ \vdots \\ f_n'(a) \end{pmatrix}\end{equation}
     Hence, we have a row for each \(f_i\), and we can apply the case for \(n = 1\) to each row.

  \end{itemize}
\end{proof}
We're going to do some calculations right away with this, since it's very important that this is all straight in your head. One thing we want to look at is the converse of this theorem, which turns out not to be true unless we make additional hypotheses.

But let's fist do some computations, and we'll see that this notion of partial derivatives lets us rethink what the chain rule says, which is very important when you want to use the chain rule.

\subsubsection{The Chain Rule}

Suppose we have
\begin{equation}F(x) =  f(g_1(x),...,g_n(x)), x = (x_1,...,x_m), g = (g_1,...,g_n)\end{equation}
with \(g\) differentiable at \(a\) and \(f\) differentiable at \(g(a)\). We want to compute \(\prt{F}{x_i}(a)\). Let's say that \(f = f(y_1,...,y_n)\). Then what's the formula?
\begin{equation}\prt{F}{x_i}(a) = \sum_{j = 1}^n\prt{f}{y_j}(g(a))\prt{g_j}{x_i}(a)\end{equation}
Why is this? Well, what does the chain rule say?
\begin{equation}DF(a) = Df(g(a))Dg(a) =
\begin{pmatrix} \prt{f}{y_1}(g(a)) & ... & \prt{f}{y_n}(g(a)) \end{pmatrix}
\begin{pmatrix}
  \prt{g_1}{x_1}(a) & ... & \prt{g_1}{x_m}(a) \\
  \vdots & \ddots & \vdots \\
  \prt{g_n}{x_1}(a) & ... & \prt{g_n}{x_m}(a)
\end{pmatrix}
\end{equation}
So the result of this is a row matrix, and by taking \(\prt{F}{x_i}\) is just indexing into it. You should really know this, you should even memorize it. When you come out of calculus, this should be as natural as addition when you come out of elementary school.

These things, sometimes people write them in a kind of symbolic, simpler way, which is useful. Here we have
\begin{equation}y_j = g_j(x_1,...,x_m)\end{equation}
and then we're composing to get
\begin{equation}z = f(y_1,...,y_n) = f(g(x))\end{equation}
So if you want to take the derivative with respect to \(x_i\), you could write that as
\begin{equation}\prt{z}{x_i}\end{equation}
But this is like short form, since \(z\) is not a function of \(x\), it's a function of \(y\). So when we write \(\prt{z}{x_i}\), we're thinking of it as already composite. And the chain rule thinks of this as
\begin{equation}\prt{z}{x_i} = \sum_{j = 1}^n\prt{z}{y_j}\prt{y_j}{x_i}\end{equation}
with evaluation, this becomes
\begin{equation}\prt{z}{x_i}(a) = \sum_{j = 1}^n\left.\prt{z}{y_j}\right|_{y = g(a)}\left.\prt{y_j}{x_i}\right|_{x = a}\end{equation}
Now how does this notation sometimes show up in a confusing way? Some examples!
\begin{enumerate}
  \item Suppose \(F(t) = f(x(t), y(t), z(t), t)\). Let's call \(F(t) = w\). Let's take the derivative with respect to \(t\):
  \begin{equation}\frac{dw}{dt} = \prt{f}{x}\frac{dx}{dt} + \prt{f}{y}\frac{dy}{dt} + \prt{f}{z}\frac{dz}{dt} + \\ ?\end{equation}
  What do we write in the fourth place? We want the derivative of \(f\) with respect to the fourth variable, so we just write \(\prt{w}{t}\). So we have
  \begin{equation}\frac{dw}{dt} \neq \prt{w}{t}\end{equation}
  See how this is confusing?

  This is \textit{shorthand}, so there can be ambiguity. That ambiguity is somewhat resolved by the different ordinary versus partial derivative notation, but it's still confusing. That's why you've got to unserstand the full, genuine meaning behind the shorthand notation.

  \item Let \(w = f(x(s, t), y(s, t), s, t)\). Now it gets even worse...
\end{enumerate}



\subsection{Basic Results}

\TODO{this}

\subsection{The Inverse Function Theorem}


In this course, we have three important theorems, of which this is one. Each is a generalization of something you've seen in first year calculus, but we'll see that these generalizations are very far-reaching and involve new techniques.

Recall: suppose \(f: \reals \to \reals\) is a continuously differentiable function in an open inverval containing \(a\), and \(f'(a) \neq 0\). Then \(f'\) is either greater than or lessthan zero in an open interval containing \(a\). Therefore \(f\) is one-to-one, and so has an inverse defined on an open interval \(W\) containing \(f(a)\). Moreover, \(f^{-1}\) is differentiable, and
\begin{equation}(f^{-1})'(f(a)) = \frac{1}{f'(a)}\end{equation}
We're going to discuss a generalization of this idea to several variables.

\begin{theorem}[Inverse Function Theorem]
  Let \(f: U \to \reals^n\) be a continuously differentiable (\(\mc{C}^1\)) on an open set \(U \subset \reals^n\). Let \(a \in U\) be such that \(\det f'(a) \neq 0\). Then there exist open sets \(a \in V, f(a) \in W\) such that \(f: V \to W\) with a continuous inverse \(f^{-1}: W \to V\). Moreover, \(f^{-1}\) is differentiable on \(W\) and
  \begin{equation}\forall y \in W, (f^{-1})'(y) = (f'(f^{-1}(y)))^{-1}\end{equation}
\end{theorem}
Remark: if we know already that \(f^{-1}\) is differentiable, the formula for it follows from the chain rule:
\begin{equation}f(f^{-1})(y) = y \implies f'(f^{-1}(y))(f^{-1})'(y) = I \iff (f^{-1})'(y) = (f(f^{-1}(y)))^{-1}\end{equation}
\begin{corollary}
  \begin{enumerate}
    \item \(f^{-1}\) is continuously differentiable (\(\mc{C}^1\))
    \item If \(f\) is \(\mc{C}^r\) then \(f^{-1}\) is also \(\mc{C}^r\)
  \end{enumerate}
\end{corollary}
\begin{proof}
  \begin{enumerate}

    \item  We know that since \(f'\) is continuous, and \(f^{-1}\) is continuous, \(f' \circ f^{-1}\) is continuous. Now what about the inversion? What is the inverse of a matrix? It's given by a formula. So this is really a composite of 3 functions. So what about the formula for the inverse of a matrix? It follows from Cramer's rule that this formula is continuous, since the entries of the inverse matrix \(B\) of \(A\) are given as rational functions of the entries of \(A\), that is,
    \begin{equation}b_{ij} = \frac{(-1)^{i + j}\det A^{ji}}{\det A}\end{equation}
    where \(A^{ji}\) is the matrix obtained by deleting the \(j^{th}\) row and \(i^{th}\) column from \(A\). So hence the derivative, being the composition of 3 continuous functions, must be continuous.

    \item We proceed by induction on \(r\). Assume that if \(f\) is \(\mc{C}^{r - 1}\), then \(f^{-1}\) is \(\mc{C}^{r - 1}\).

    If \(f\) is \(\mc{C}^r\), then \(f\) is \(\mc{C}^{r - 1}\) implying that \(f^{-1}\) is \(\mc{C}^{r - 1}\) by the inductive hypotheis. So, using the formula
    \begin{equation}(f^{-1})' = (f'(f^{-1}))^{-1}\end{equation}
    is \(\mc{C}^{r - 1}\) implying that \(f^{-1}\) is \(\mc{C}^r\).

  \end{enumerate}
\end{proof}
This corollary is just to show that we could have stated the above theorem in a stronger way. Examples:
\begin{enumerate}
  \item Continuity of \(f'\) cannot be removed from the hypotheses: consider \(f: \reals \to \reals\),
  \begin{equation}f(x) = \left\{\begin{array}{cc}x + x^2\sin\frac{1}{x} & x \neq 0 \\ 0 & x = 0\end{array}\right.\end{equation}
  We have that
  \begin{equation}x \neq 0 \implies f'(x) = 1 + 2x\sin\frac{1}{x} - \cos\frac{1}{x}\end{equation}
  but the limit does not exist as \(x \to 0\).

  \item Consider \(f: \reals^2 \to \reals^2\),
  \begin{equation}f(x, y) = (e^x\cos y, e^x\sin y)\end{equation}
  We have
  \begin{equation}f'(x, y) = \begin{pmatrix} e^x\cos y & -e^x\sin y \\ e^x\sin y & e^x\cos y \end{pmatrix}\end{equation}
  implying that
  \begin{equation}\det f'(x, y) = (e^x)^2 = e^{2x} \neq 0 \forall x \in \reals\end{equation}
  But this function is not 1-1, since it is periodic in \(y\). The inverse function theorem says that we can make some neighborhood around a point where \(f\) is one to one, but it \textit{doesn't} say that it's \textit{globally} one to one.

\end{enumerate}
Remarks:
\begin{enumerate}
  \item \(f\) may be invertible even though \(f'(a) = 0\)
\end{enumerate}

\subsection{The Implicit Function Theorem}


Assume we have a function \(F(x, y) = 0\). Can we solve for \(y\) as a function \(y = g(x)\) near a point \((a, b)\) such that \(F(a, b) = 0\)?
For example, assume \(f(x, y) = x^2 + y^2 - 1\).
This relationship, of course, defines the unit circle. Can we solve for \(y\) at a function of \(x\) at any point on the unit circle? No: we can't do so where \(y = 0\), we can only get \(x\) as a function of \(y\).
If we pick a point \((a, b) \in \reals^2\) however, and assume \(a \neq \pm 1\), then there are open intervals \(I, J\) such that for every \(x \in I\), there is a unique \(y = g(x) \in J\) such that \(f(x, y) = 0\).

Let's restrict our attention to a small enough interval such that \(b > 0\). Of course, then, we can explicitly write down what the solution is:
\begin{equation}g(x) = \sqrt{1 - x^2}\end{equation}

Sometimes in this context people say that one of the variables is dependent and the other is independent. Which is which? How should you understand that language? Well, the dependent variable is the one that is determined by the independent one you specify. But that notion, it depends on where you are: at the poles \(x = \pm 1\), we can only solve for \(x\), whereas at the poles \(y \pm 1\), we can only solve for \(y\).

Furthermore, we may not be able to find a nice formula for \(g\). However, by what's known as ``implicit differentiation,'' we can find a formula for the \textit{derivative of} \(g\).

\subsubsection{Implicit Differentiation}
Let's assume we have a function
\begin{equation}F(x, g(x)) = 0\end{equation}
Whether or not we have a formula for \(g\), we can use the chain rule to obtain
\begin{equation}\frac{d}{dx}F(x, g(x)) = \partial_1(x, g(x)) + \partial_2(x, g(x))g'(x) = 0\end{equation}
So we find that
\begin{equation}g'(x) = -\frac{\partial_1(x, g(x))}{\partial_2(x, g(x))}\end{equation}
Now this formula depends on \(g\). Is this weird or expected? Well, it makes sense. Consider the example above: we might have two solutions (points on the circle) on top of each other for a given \(x\)-coordinate. This formula works for \textit{both} of them.


\TODO{fill in here}


\subsubsection{The Inverse Function Theorem Implies the Implicit Function Theorem}

We have to start with the hypotheses of the implicit function theorem: given a \(\mc{C}^r\) (where \(r \geq 1\)) function
\(f: U \to \reals^n\)
where \(U \in \reals^{m + n}\), \(f(a, b) = 0\) and \(\det M \neq 0\), where
\begin{equation}M = \left(\prt{f_i}{y_j}(a, b)\right)\end{equation}
Define
\begin{equation}F: U \to \reals^m\times\reals^n, (x, y) \mapsto (x, f(x, y))\end{equation}
In particular,
\begin{equation}F(a, b) = (a, 0)\end{equation}
This is what we're going to apply the inverse function theorem to. So we've got to show that this function satisfies the hypotheses of the inverse function theorem. So we've got to show that its derivative matrix at the point \((a, b)\) is invertable. So let's compute: the derivative is given by
\begin{equation}F'(a, b) = \left(\begin{array}{c|c} I & 0 \\ \hline * & M \end{array}\right) \implies \det F'(a, b) = \det I \det M = \det M \neq 0\end{equation}
These are the conditions under which we can apply the inverse function theorem. So by the inverse function theorem, there exists an open neighborhood \(V\) of \((a, b)\), and an open neighborhood \(W\) of \((0, 0)\) so that \(F: V \to W\) has a \(\mc{C}^r\) inverse
\(F^{-1}: W \to V\). We can assume \(V = A \times B\), where \(A, B\) are open neighborhoods of \(a, b\). \(F^{-1}(u, v)\) has the form \((u, h(u, v))\). So
\begin{equation}F(F^{-1}(u, v)) = F(u, h(u, v)) = (u, f(u, h(u, v))) = (u, v) \implies f(u, h(u, v)) = v\end{equation}
\begin{equation}\implies f(u, h(u, 0)) = 0\end{equation}
Let \(g(x) = h(x, 0)\), which is \(\mc{C}^r\). Then
\begin{equation}f(x, g(x)) = 0\end{equation}
Remark: we can find \(g'(x)\) by \underline{implicit differentiation}. We have
\begin{equation}\forall i \in \{1,...,n\}, f_i(x, g(x)) = 0\end{equation}
We can write
\begin{equation}\prt{f_i}{x_j}(x, g(x)) + \sum_{k = 1}^n\prt{f_i}{y_k}(x, g(x))\prt{g_k}{x_j}(x) = 0\end{equation}
We can solve for \(\prt{g_k}{x_j}\) because \(\left(\prt{f_i}{y_k}(x, y)\right)\) is invertible near \((a, b)\).

\subsubsection{The Implicit Function Theorem Implies the Inverse Function Theorem}

This time, we start with the hypotheses of the \textit{inverse} function theorem. So here we have a \(\mc{C}^r\) function \(f: U \to \reals^n\) with \(\det f'(a) \neq 0\).

Let \(b = f(a)\), and define
\begin{equation}F(x, y) = y - f(x)\end{equation}
This is a \(\mc{C}^r\) function of \((a, b)\) and \(F(a, b) = 0\). We have
\begin{equation}\prt{F}{x}(a, b) = \det(-f'(a)) \neq 0\end{equation}
By the implicit function theorem there exist open neighborhoods \(A, B\) of \(a, b\) respectively such that for all \(y \in A\), there is a unique \(\mc{C}^r\) \(x = g(y)\) in \(B\) such that
\begin{equation}F(g(y), y) = 0 \iff y - f(g(y)) = 0\end{equation}
Take \(V = f^{-1}(A) \cap B\), \(W = A\). Then
\begin{equation}x \in V \implies f(x) \in A\end{equation}
and \(x\) is the unique element of \(B\) such that \(g(f(x)) = 0\), i.e. \(x = g(f(x))\).

\subsubsection{Proof of the Inverse Function Theorem}

Let \(f: \reals^n \to \reals^n\) be \(\mc{C}^1\) on an open set containing \(a \in \reals^n\) and let \begin{equation}\det f'(a) \neq 0\end{equation}
We can assume, as shown in last lecture, that \(f'(a) = I\).
We show that there are open \(V, W\) containing \(a\) and \(f(a)\) respectively such that \(f: V \to W\) with a continuous inverse \(f': W \to V\)

We will begin by showing that we can't have \(f(x) = f(a)\) if \(x \neq a\) is sufficiently close to \(a\):
\begin{equation}f'(a) = I \land Ih = h \implies \lim_{h \to 0}\frac{|f(a + h) - f(a) - h|}{|h|} = 0\end{equation}
If \(f(a + h) = f(a)\) then the quotient is 1. So the quotient being zero means that there exists a closed ball \(B\) centered at \(a\) such that
\begin{equation}x \in B \setminus \{a\} \implies f(x) \neq f(a)\end{equation}
But that's not enough to mean that \(f\) is one to one in \(B\): it could be, for example, that \(f(\partial B)\), the image of the boundary of \(B\), intersects itself, for example. What we want to do is find an even smaller ball where \(f\) really is one-to-one. So, since \(f\) is \(\mc{C}^1\) in a neighborhood of \(a\) we can also assume:
\begin{enumerate}

  \item \(x \in B \implies \det f'(x) \neq 0\) since \(f'\) is continuous

  \item Given \(\epsilon\), we can make, for all \(i, j\), \begin{equation}
    \left|\prt{f_i}{x_j}(x) - \prt{f_i}{x_j}(a)\right| < \epsilon
  \end{equation}
  Note that, by our previous assumption that \(f'(a) = I\),
  \begin{equation}\prt{f_i}{x_j}(a) = I_{ij}\end{equation}

\end{enumerate}
Apply the Mean Value Theorem to \(g(x) = f(x) - x\). According to the MVT, we can also assume that
\begin{equation}|g(x_1) - g(x_2)| \leq cb|x_1 - x_2|\end{equation}
where \(c\) is a constant and \(b\) is a bound on the partial derivatives of \(g\). But what is \(\prt{g_i}{x_j}\)? It's the same as what's in the absolute value brackets in assumption (2), since
\begin{equation}\prt{x_i}{x_j} = \delta_{ij} = \prt{f_i}{x_j}\end{equation}
So we have that
\begin{equation}|g(x_1) - g(x_2)| \leq c\epsilon|x_1 - x_2|\end{equation}
for some constant \(c\). That was for any given \(\epsilon\). Let's choose \(\epsilon\) such that \(c\epsilon = \frac{1}{2}\), giving
\begin{equation}|g(x_1) - g(x_2)| \leq \frac{1}{2}|x_1 - x_2|\end{equation}
If you remember that problem we did on the MVT, we gave a precise bound on \(\epsilon\), specifically it suffices to choose \(\epsilon = \frac{1}{2n}\). So what does this tell us?
\begin{equation}|f(x_1) - x_1 - f(x_2) + x_2| \leq \frac{1}{2}|x_1 - x_2|\end{equation}
using the Triangle Inequality, we obtain
\begin{equation}|x_1 - x_2| \leq 2|f(x_1) - f(x_2)|\end{equation}
So what does \textit{this} tell us? It tells us that \(f\) is one-to-one on \(B\), since if \(f(x_1) = f(x_2)\) and \(x_1 \neq x_2\) then
\begin{equation}|x_1 - x_2| > 2|f(x_1) - f(x_2)| = 0\end{equation}
We have that \(f(\partial B)\) is a compact set not including \(a\). So we have that
\begin{equation}d = d(f(a), f(\partial B)) > 0\end{equation}
Let
\begin{equation}W = \{y : |y - f(a)| \leq d/2\}\end{equation}
Then if \(y \in W\), \(x \in \partial B\), then what's the relationship between
\(|y - f(a)|\) and \(|y - f(x)|\)? Since \(W\) is the ball of radius \(\frac{d}{2}\), we have that
\begin{equation}|y - f(a)| < |y - f(x)|\end{equation}
This tells us that \(W\) is completely in the image of \(B\). It follows that for all \(y \in W\), there is some \(x \in \Int B\) such that \(f(x) = y\). I could have said unique \(x\), but I didn't bother because it must be unique anyways since we already checked one to one. But let's check this. We're going to do so by solving an extreme value problem. Let's define \(h: B \to \reals\) by
\begin{equation}x \mapsto |y - f(x)|^2 = \sum_{i = 1}^n|y_i - f_i(x)|^2\end{equation}
\(h\) is continuous, and therefore it has a minimum on the compact set \(B\). So this is the usual business of a max-min problem: it could be that the minimum occurs on the bounary. So can it? No, since, as we saw above,
\begin{equation}\forall y \in W, |y - f(a)| < |y - f(x)|\end{equation}
So that means it occurs at the interior, and since \(f\) and hence \(|y - f(x)|^2\) is differentiable, at a critical point, a point where all the partial derivatives are zero, \(x \in \Int B\). But what does the following exactly mean:
\begin{equation}\prt{h}{x_j}(x) = 0\end{equation}
Expanding, we get
\begin{equation}\forall j \in \{1,...,n\}, \sum_{i = 1}^n2(y_i - f_i(x))\prt{f_i}{x_j}(x) = 0\end{equation}
So what does this tell us? This is a system of \(n\) equations linear in \(y_i - f_i(x)\) with coefficients \(\prt{f_i}{x_j}(x)\). But since the matrix
\begin{equation}\left(\prt{f_i}{x_j}\right)\end{equation}
is invertible, since \(\det f'(x) \neq 0\), we have
\begin{equation}[\forall i \in \{1,...,n\}, (y_i - f_i(x)) = 0] \iff y = f(x)\end{equation}
which is what we wanted to show.

So back to what we wanted to do, we wanted to find those open sets \(V\) and \(W\). So what should we use as \(V\)?
\begin{equation}V = f^{-1}(W) \cap \Int B\end{equation}
Then \(f: V \to W\) has an inverse \(f^{-1}: W \to V\), since \(f\) is one to one. We also wanted to show that the inverse is continuous. So how do we see that? Well, we go back to what we showed above:
\begin{equation}\forall x_1, x_2 \in V, |x_1 - x_2| \leq 2|f(x_1) - f(x_2)| \implies \forall y_1, y_2 \in W, |f^{-1}(y_1) - f^{-1}(y_2)| \leq 2|y_1 - y_2|\end{equation}
implying \(f^{-1}\) is continuous. So the only thing remaining is to show that \(f^{-1}\) is differentiable. So let's take a point \(x_0 \in V\), \(y_0 = f(x_0)\). Call \(\mu = Df(x_0)\). We'll show \(f^{-1}\) is differentiable at \(y_0\) and \((f^{-1})'(y_0) = \mu^{-1}\). So what do we know. We know that
\begin{equation}f(x) = f(x_0) + \mu(x - x_0) + \varphi(x)\end{equation}
where
\begin{equation}\lim_{x \to x_0}\frac{|\varphi(x)|}{|x - x_0|} = 0\end{equation}
Each \(y \in W\) can be written \(y = f(x), x \in V\). We have
\begin{equation}\mu^{-1}(y - y_0) = \mu^{-1}(f(x) - f(x_0)) = x - x_0 + \mu^{-1}\varphi(x)\end{equation}
So what this says is that
\begin{equation}f^{-1}(y) = f^{-1}(y_0) + \mu^{-1}(y - y_0) - \mu^{-1}\varphi(f^{-1}(y))\end{equation}
That's like what we need to show that \(f^{-1}\) is differentiable. Specifically, we have to show
\begin{equation}\lim_{y \to y_0}\frac{\mu^{-1}\varphi(f^{-1}(y))}{|y - y_0|} = 0\end{equation}
We have
\begin{equation}\frac{\varphi(f^{-1}(y))}{|y - y_0|} = \frac{\varphi(f^{-1}(y))}{|f^{-1}(y) - f^{-1}(y_0)|} \cdot \frac{|f^{-1}(y) - f^{-1}(y_0)|}{|y - y_0|} \leq 2\frac{\varphi(f^{-1}(y_0))}{|f^{-1}(y) - f^{-1}(y_0)}\end{equation}
And that's the proof of the inverse function theorem, and hence the implicit function theorem.


\subsection{Tangent Space}

We begin with some examples:
\begin{enumerate}

  \item Consider an ellipsoid
  \begin{equation}\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1\end{equation}
  The tangent plane at a point \((x_0, y_0, z_0)\) of the ellipsoid is given by
  \begin{equation}\frac{\cancel{2}x_0}{a^2}(x - x_0) + \frac{\cancel{2}y_0}{b^2}(y - y_0) + \frac{\cancel{2}z_0}{c^2}(z - z_0) = 0\end{equation}
  \begin{equation}\frac{x_0}{a^2}x + \frac{y_0}{b^2}y + \frac{z_0}{c^2}z = 1\end{equation}

  \item Let \(f(x, y) = 0\) and \(g(x, y) = 0\) be curves in \(\reals^2\). Assume the curves are smooth and intersect at point \((a, b)\). The two curves are \underline{orthogonal} at \((a, b)\) if
  \begin{equation}\prt{f}{x}\prt{g}{x} + \prt{f}{y}\prt{g}{y} = 0\end{equation}
  at \((a, b)\). The two curves are \underline{tangent} at \((a, b)\) if the gradients are proportional to each other, i.e.
  \begin{equation}\prt{f}{x}\prt{g}{y} - \prt{f}{y}\prt{g}{x} = 0\end{equation}
  at \((a, b)\).

  \item Consider the family of parabolas
  \begin{equation}y^2 - 2p(x + p/2) = 0\end{equation}
  Here, \(p\) is the parameter. These parabolas open along the \(x\) axis, either to the right or two the left, with \(x\) acting like a function of \(y\).

  This kind of family is what's called \textit{confocal}: these parabolas all have the same focus, 0. Of course, they don't all intersect, but all those which open to the right intersect all those which open to the left. Let's look at the intersections.

  If \(p_1 > 0\) and \(p_2 > 0\) then the two parabolas
  \begin{equation}f(x, y) = y^2 - 2p_1(x + p_1/2) = 0, g(x, y) = y^2 - 2p_2(x + p_2/2) = 0\end{equation}
  intersect where
  \begin{equation}f(x, y) = y^2 - 2p_1(x + p_1/2) = 0 = g(x, y) = y^2 - 2p_2(x + p_2/2)\end{equation}
  Let's show that all the intersections among all these possible parabolas are right angles.

  Let's try to compute the inner product of the gradients. That is, we want to show
  \begin{equation}f_xg_x + f_yg_y = 0\end{equation}
  at an intersection point. So what's the product of the \(x\) derivatives? Well it'll just be
  \begin{equation}f_xg_x = (-2p_1)(-2p_2) = 4p_1p_2\end{equation}
  And what's the product of the \(y\) derivatives?
  \begin{equation}f_yg_y = (2y)(2y) = 4y^2\end{equation}
  So we want to see that
  \begin{equation}f_xg_x + f_yg_y = 4p_1p_2 + 4y^2\end{equation}
  vanishes when both \(f\) and \(g\) are zero.

  If you take \(p_2\) times \(f\), and subtract \(p_1\) times \(g\), you get rid of the \(x\) terms:
  \begin{equation}p_2f - p_1g = p_2y^2 - 2p_2p_1(x + p_1/2) - p_1y^2 + 2p_1p_2(x + p_2/2) = (p_1 - p_2)y^2 + (p_2 - p_1)p_1p_2\end{equation}
  So we have
  \begin{equation}f_xg_x + f_yg_y = 4p_1p_2 + 4y^2 = 4\frac{p_2f - p_1g}{p_2 - p_1} = 0\end{equation}
  Since \(f = g = 0\) and \(p_2 - p_1 \neq 0\) at the intersection point.

\end{enumerate}
So these are some examples of computations with tangent spaces.

\TODO{this}

\subsection{Extremum Problems}


We're going to be considering functions, for open sets \(U \subseteq \reals^n\),
\begin{equation}f: U \to \reals\end{equation}
If \(f\) has a local maximum or minimum at a point \(a\) where
\begin{equation}\prt{f}{x_i}(a)\end{equation}
exists, then it must be zero. This is true because the partial derivative is just the derivative along a one-dimensional subspace: a line. That is,
\begin{equation}\prt{f}{x_i}(a) = g_i'(0) = 0, g_i(t) = f(a + t\mb{e}_i)\end{equation}
If \(f\) is differentiable at \(a\), what is the direction of maximum increase (we could also ask for decrease) of \(f\) at \(a\)? It's the direction of the gradient, which I guess is what's called the gradient. Why is this? If we want to look at the increase of \(f\) in a particular direction, we have to look at the value of the directional derivative of \(f\) in that particular direction. We we want to maximize
\begin{equation}D_{\mb{e}}f(a) = Df(a)(e) = \ip{\grad{f(a)}}{\mb{e}}\end{equation}
for some unit vector \(\mb{e}\). In general, of course, what is this inner product? It's
\begin{equation}|\mb{e}||\grad f(a)|\cos\theta = |\grad f(a)|\cos\theta\end{equation}
This, of course, is maximized when \(\theta = 0\), i.e. \(\mb{e}\) points in the direction of \(\grad f(a)\).

What about the converse: does a point where the derivative is zero imply an extremum. Here, just like in first year, this is false, however, in this case, the converse is false \textit{even} assuming \(g_i''(0) \neq 0\). For example, consider
\begin{equation}f(x, y) = x^2 - y^2\end{equation}
This graph is ``saddle shaped''. The second derivative with respect to both \(x\) and \(y\) are nonzero at the origin, the first partial derivatives are zero, and yet we have neither a local maximum or a minimum: in fact, we have a maximum in the \(y\) direction and a minimum in the \(x\) direction!

Let's do an example: consider an acute angled triangle with vetices \(P_i = (x_i, y_i)\). Find a fourth point \(P = (x, y)\) such that the sum of distances to each \(P_i\) is as small as possible. We have
\begin{equation}f(x, y) = r_1 + r_2 + r_3\end{equation}
where
\begin{equation}r_i = \sqrt{(x - x_i)^2 + (y - y_i)^2}\end{equation}
One thing that's we can see is that there \textit{is} a minimum, and it occurs on a closed disk whose boundary contains \(P_i\). \(f\) is differentiable except at \(P_i\), but the minimum can't occur at \(P_i\), since the triangle is acute, implying we can take the intersection of an edge and the perpendicular from a vertex and get a smaller value for \(f\). This isn't the only place where we'll use the fact that the triangle is acute angled, however.

So we know the maximum must occur at a critical point. At a critical point:
\begin{equation}\prt{f}{x} = \frac{x - x_1}{r_1} + \frac{x - x_2}{r_2} + \frac{x - x_3}{r_3} = 0\end{equation}
\begin{equation}\prt{f}{y} = \frac{y - y_1}{r_1} + \frac{y - y_2}{r_2} + \frac{y - y_3}{r_3} = 0\end{equation}
i.e. if
\begin{equation}u_i = \left(\frac{x - x_i}{r_i}, \frac{y - y_i}{r_i}\right), u_1 + u_2 + u_3 = 0\end{equation}

\TODO{rest}

\subsubsection{Lagrange's Method}


Today, we'll do some example problems regarding Lagrange's method:

\begin{enumerate}

  \item Prove that
  \begin{equation}uv \leq \frac{1}{\alpha}u^\alpha + \frac{1}{\beta}v^\beta\end{equation}
  for all \(u, v \in \reals^+_0\), \(\alpha, \beta \in \reals^+\) such that \(\frac{1}{\alpha} + \frac{1}{\beta} = 1\).
  \begin{proof}
    This is trivially true if \(u = 0\) or \(v = 0\), so we can assume \(uv \neq 0\).

    The enequality holds for \(u, v\) if and only if it holds for \(ut^{1/\alpha}\), \(vt^{1/\beta}\),
    \begin{equation}(ut^{1/\alpha})(vt^{1/\beta}) = 1 \iff t = \frac{1}{uv}\end{equation}
    So it's enough to show that
    \begin{equation}1 \leq \frac{1}{\alpha}u^{\alpha} + \frac{1}{\beta}v^{\beta}\end{equation}
    for all \(u, v \in \reals^+\) such that \(uv = 1\).
    We have that
    \begin{equation}\frac{1}{\alpha}u^\alpha + \frac{1}{\beta}v^\beta \to \infty\end{equation}
    on \(uv = 1\) when either \(u \to \infty\) or \(v \to \infty\). So there is a minimum at a finite point.
    The critical points of
    \begin{equation}\frac{1}{\alpha}u^\alpha + \frac{1}{\beta}v^\beta + \lambda(uv - 1)\end{equation}
    are given by
    \begin{equation}u^{\alpha - 1} + \lambda v = 0 \land v^{\beta - 1} + \lambda u = 0 \implies u^\alpha = v^\beta = -\lambda\end{equation}
    This gives, combined with condition \(uv = 1\), solution
    \begin{equation}u = v = 1\end{equation}
    So the minimum value of
    \begin{equation}\frac{1}{\alpha}v^\alpha + \frac{1}{\beta}v^\beta\end{equation}
    is simply
    \begin{equation}\frac{1}{\alpha} + \frac{1}{\beta} = 1\end{equation}
    which implies the above inequality and hence what we desired to prove. We will use these kinds of techniques to prove a veriety of different problems.
  \end{proof}

  \item Prove \underline{Holder's inequality}:
  \begin{equation}\sum_{i = 1}^nu_iv_i \leq \left(\sum_{i = 1}^nu_i^\alpha\right)^{1/\alpha}\left(\sum_{i = 1}^nv_i^\beta\right)^{1/\beta}\end{equation}
  where \(u_i, v_i \in \reals^+_0\), \(\alpha, \beta \in \reals^+\) such that
  \begin{equation}\frac{1}{\alpha} + \frac{1}{\beta} = 1\end{equation}

  \begin{proof}
    It's a good observation that this is similar to what we did before (1), and in fact we can deduce it just from what we did before instead of starting from scratch.

    We begin by assuming
    \begin{equation}\sum_{i = 1}^nu_i^\alpha, \sum_{i = 1}^nv_i\alpha \neq 0\end{equation}
    since if either are zero then the inequality holds trivially.

    We can now apply the preceding inequality to
    \begin{equation}u = \frac{u_i}{\left(\sum_{i = 1}^nu_i^\alpha\right)^{1/\alpha}}, v = \frac{v_i}{\left(\sum_{i = 1}^nv_i^\beta\right)^{1/\beta}}\end{equation}
    for each \(u_i, v_i\), from which the result follows.
  \end{proof}

  \item An example from last year's term test: find the rectangular parallelepiped of greatest volume inscribed in an ellipsoid
  \begin{equation}\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1\end{equation}
  Some people in MAT257 last year didn't like this question. That was true for most of the questions. But part of the reason why they didn't like this question is that they looked at it in a way which was more difficult than I intended, because I was essentially thinking that you should assume that the rectangular parallelepiped is situated in a standard way within the ellipse. We will do it this way.

  In fact, maybe there's an ambiguity in the meaning of the question, because what do you mean by a rectangular parallelepiped inscribed in an ellipse. Like, I thought that meant that all four vertices lied on the ellipse. If you want to do the exercise, it's a good geometric exercise, but it's not a required part of the exercise, to show that if you do have all vertices on the ellipse, it has to be centered at the origin.

  So let's begin. Let \((x, y, z)\) be the vertex in the positive orthent (like the positive quadrant but in 3D). So what's the problem we have to solve? First of all, what's the volume in terms of this vertex?
  \begin{equation}(2x)(2y)(2z) = 8xyz\end{equation}
  So we have to maximize \(8xyz\), which is the same as maximizing \(xyz\), on the ellipsoid  given, with \(x, y, z \in \reals^+\).

  This means we have to find the critical points. This (the ellipse) is a nice smooth surface of course, technically this is something you need to check, in that its gradient is nonzero at every point. To find the extrema, we hence need to find the critical points of
  \begin{equation}F(x, y, z) = xyz + \lambda\left(\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1\right)\end{equation}
  This means we need to find the derivatives in terms of \((x, y, z)\) and set them to zero, which are
  \begin{equation}F_x = yz + 2\lambda\frac{x}{a^2} = F_y = xz + 2\lambda\frac{y}{b^2} = F_z = xy = 2\lambda\frac{z}{c^2} = 0\end{equation}
  So we have four equations in three unknowns: these equations together with the equation of the ellipsoid.

  We multiply the first equation by \(x\), the second by \(y\) and the third by \(z\) to obtain
  \begin{equation}xyz + 2\lambda\frac{x^2}{a^2} = 0, xyz + 2\lambda\frac{y^2}{b^2} = 0, xyz + 2\lambda\frac{z^2}{b^2} = 0\end{equation}
  and add them up to obtain
  \begin{equation}3xyz + 2\lambda\left(\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}\right) = 0\end{equation}
  Plugging in the equation of the ellipse, we get
  \begin{equation}3xyz + 2\lambda = 0\end{equation}
  To solve this for \(x, y, z\), we begin solving for \(x\) by substituting \(yz\) for \(-2\lambda\frac{x}{a^2}\) (from the first equation) to get
  \begin{equation}-3\frac{x^2}{a^2} + 1 = 0 \implies x = \frac{a}{\sqrt{3}}\end{equation}
  Likewise,
  \begin{equation}y = \frac{b}{\sqrt{3}}, z = \frac{c}{\sqrt{3}}\end{equation}
  We know this is not a minimu, since the volume of the minimum is zero.

\end{enumerate}

\subsection{Multivariate Taylor Series}


\subsubsection{Review of Single Variable Taylor Series}
Recall the definition of the \(n^{th}\) degree Taylor polynomial of \(f\) centered at \(a\)
\begin{equation}P_{n, a}(f) = \sum_{k = 0}^n\frac{f^{(k)}(a)}{k!}(x - a)^k\end{equation}
We define \(R_{n, a}\) to be the ``remainder'', the difference between \(P_{n, a}\) and \(f\). We can write this remainder in ``Lagrange form'' as
\begin{equation}\frac{f^{(n + 1)}(t)}{(n + 1)!}(x - a)^{n + 1}\end{equation}
Clearly,
\begin{equation}\lim_{x \to a}\frac{R_{n, a}(x)}{(x - a)^n} = 0\end{equation}
We can now define the \underline{Taylor series} of \(f\) at \(a\) to be
\begin{equation}T_af = \sum_{k = 0}^\infty\frac{f^{(k)}(a)}{k!}(x - a)^k\end{equation}
Why these coefficients?
Suppose
\begin{equation}f(x) = \sum_{k = 0}^\infty a_k(x - a)^k\end{equation}
Note
\begin{equation}\frac{d^n}{dx^n}(x - a)^k = \left\{\begin{array}{cc}
  0 & \text{if } k < n \\
  k! & \text{if } n = k \\
  k(k - 1)...(k - n + 1)(x - a)^{k - n} & \text{if } k > n
\end{array}\right.\end{equation}

\subsubsection{Multivariable Case}
For example: What's your favorite polynomial? Zero? I'm sure you meant something more like
\begin{equation}f(x, y, z) = a_{12,8,2}x^{12}y^8z^2 + a_{0,22,0}y^{22} + a_{0,0,1}z + a_{20,20,20}x^{20}y^{20}z^{20}\end{equation}
How to do spot \(a_{12, 8, 2}\)? Apply:
\begin{equation}\left.\frac{\partial^{22}f}{\partial x^2 \partial y^8 \partial z^2}\right|_{(x, y, z) = (0, 0, 0)} = a_{12, 8, 2}12!8!2!\end{equation}
In general, if \(f\) is a sum of terms like
\begin{equation}a_{\alpha_1,...,\alpha_n}x^{\alpha_1}....x^{\alpha_n}\end{equation}
Then
\begin{equation}a_{\alpha_1,...,\alpha_n} = \left.\frac{\partial^{\alpha_1 + ... + \alpha_n}f}{\partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}}\right|_{(x_1,...,x_n) = (0,...,0)}\frac{1}{\alpha_1!...\alpha_n!}\end{equation}

\subsubsection{Multi-index notation}
We're mathematicians here, and what do mathematicians do, we generalize. But we generalize in such a way such that the general case looks like the specialized case so we don't have to remember different notation when teaching first-years and when doing research. Multi-index notation is a prime example. Compare:
\begin{itemize}

  \item Let \(f: \reals \to \reals\). Then
  \begin{equation}f = \sum_{k \in \nats}\frac{1}{k!}f^{(k)}(a)(x - a)^k\end{equation}

  \item Let \(f: \reals^n \to \reals\). Then
  \begin{equation}f = \sum_{\alpha \in \nats^n}\frac{1}{\alpha!}\frac{\partial^{|\alpha|}}{\partial x^\alpha}(a)(x - a)^\alpha\end{equation}

\end{itemize}
To make this to work, all we have to do is define, for
\begin{equation}\alpha = (\alpha_1,...,\alpha_n) \in \nats^n\end{equation}
the notation
\begin{equation}\alpha! = \alpha_1!...\alpha_n!\end{equation}
\begin{equation}|\alpha| = \alpha_1 + ... + \alpha_n\end{equation}
\begin{equation}\partial x^\alpha = \partial x_1^{\alpha_1} ... \partial x_n^{\alpha_n}\end{equation}
\begin{equation}(x - a)^\alpha = (x_1 - a_1)^{\alpha_1}...(x_n - a_n)^{\alpha_n}\end{equation}
Now, for some examples
\begin{enumerate}

  \item
  \begin{equation}\frac{1}{6!y!}12x^6y^7 = \frac{12}{(6, 7)!}(x, y)^{6, 7}\end{equation}

  \item
  \begin{equation}x^5 + x^4y + x^3y^2 + ... + y^5 = \sum_{\beta + \gamma = 5}(x, y)^{(\beta, \gamma)} = \sum_{|\alpha| = 5}(x, y)^\alpha\end{equation}
  Note that we're assuming here \(\beta, \gamma\) are non-negative. On good days you can just do what we did above, but if it worries you, write it down.

  \item
  \begin{equation}a_{0, 0} + a_{1, 0}x + a_{0, 1}y + a_{1, 1}xy + a_{2, 0}x^2 + a_{0, 2}y^2 = \sum_{|\alpha| \leq 2}a_{\alpha}(x, y)^{\alpha}\end{equation}

\end{enumerate}

\subsubsection{Multivariable Taylor Series}

\begin{theorem}
  If \(f: \reals^n \to \reals\) is \(\mc{C}^{k + 1}\) then
  \begin{equation}f(a + h) = \sum_{|\alpha| \leq k}\frac{1}{\alpha!}\frac{\partial^{|\alpha|}f}{x^\alpha}(a)h^\alpha + \sum\_{|\alpha| = k + 1}\frac{1}{\alpha!}\frac{\partial^{|\alpha|}f}{\partial x^\alpha}(a + \theta h)h^\alpha\end{equation}
  where \(\theta \in (0, 1)\)
\end{theorem}
\begin{proof}
  The key idea is
  \begin{equation}F(t) = f(a + th)(\text{Old Taylor } + \text{ Chain Rule})\end{equation}
  More rigorously, we know from single variable calculus that
  \begin{equation}F(t) = F(0) + \frac{F'(0)}{1!}t + ... + \frac{F^{(k)}(0)}{k!}t^k + \frac{F^{k + 1}(\theta)}{(k + 1)!}t^{k + 1}\end{equation}
  Set \(t = 1\) to get \(F(1) = f(a + h)\).
  We know \(F(0) = f(a)\). We have
  \begin{equation}\left.\frac{dF}{dt}\right|_{t = 0} = \left.\frac{d}{dt}f(a + th)\right|_{t = 0} = \sum_{i = 1}^n\left.\prt{f}{x}(a + th)\right|_{t = 0}\prt{x_i}{t} = \sum_{i = 1}^n\left[\left.\prt{f}{x_i}(a + th)\right|_{t = 0}\right]h_i
  = \sum_{i = 1}^n\prt{f}{x_i}(a)h_i\end{equation}
  We have
  \begin{equation}\left.\frac{d^2F}{dt}\right|_{t = 0} = \sum_{i = 1}^n\left[
    \sum_{j = 1}^n\frac{\partial^2f}{\partial x_j \partial x_i}(a)h_j
  \right]h_i\end{equation}
  By the magical process of ...,
  \begin{equation}\left.\frac{d^kF}{dt^k}\right|_{t = 0} = \sum_{i_1,...,i_k = 1}^n\frac{\partial^kf}{\partial x_{i_1} ... \partial x_{i_k}}(a)h_{i_1}...h_{i_k}\end{equation}
  Now all we need is some combinatorics, combined with the equality of mixed partials (for \(\mc{C}^{k + 1}\) functions), to complete the proof. Recall the \textit{multinomial coefficient}
  \begin{equation}{{|\alpha|} \choose {\alpha_1,...,\alpha_n}} = \frac{|\alpha|!}{\alpha_1!...\alpha_n!}\end{equation}
  is the number of ways of taking \(|\alpha|\) things and making \(k\) groups of size \(\alpha_1,...,\alpha_n\).
  We can use this to rewrite the above as
  \begin{equation}F^{(S)}(0) = \sum_{|\alpha| = S}\frac{S!}{\alpha_1!...\alpha_S!}\frac{\partial^{|\alpha|}f}{\partial x^\alpha}(a)h^\alpha\end{equation}
  So plugging this into our original expression for the Taylor series of \(F\), where we divide the \(S^{th}\) term by \(\frac{1}{S!}\), we have
  \begin{equation}F(t) = F(0) + \sum_{i = 1}^k\sum_{|\alpha| = i}\frac{1}{\alpha_1!...\alpha_k!}\frac{\partial^{|\alpha|}f}{\partial x^\alpha}(a)h^\alpha
  = f(a) + \sum_{|\alpha| \leq k}\frac{1}{\alpha!}\frac{\partial^{|\alpha|}f}{\partial x^\alpha}(a)h^\alpha\end{equation}
  as desired.

\end{proof}

\TODO{fill in here}

\section{Integration}

\subsection{The (Riemann) Integral Over a Rectangle}

\TODO{beginning}

We said what the integral of a closed rectangle on \(\reals^n\) was last time, in complete analogy with first year calculus:
\begin{definition}
  If \(f: A \to \reals\) is a bounded function on a closed rectangle \(A \subset \reals^n\), \(f\) is \underline{integrable} on \(A\) if, where \(\mc{P}\) is the set of partitions of \(A\),
  \begin{equation}\sup_{P \in \mc{P}} L(f, P) = \inf_{P \in \mc{P}} U(f, P)\end{equation}
  then the common value is called the integral of \(f\) over \(A\), written
  \begin{equation}\int_Af\end{equation}
\end{definition}
Throughout this lecture we'll see that most of the properties you learned in first year calculus will carry over to this more general case. One thing we'll be carrying over today is that instead of expressing things in terms of \(\sup\) and \(\inf\), we can instead express things in terms of \(\epsilon\):
\begin{lemma}
  If \(f: A \to \reals\) is a bounded function on a closed rectangle \(A\) then \(f\) is integrable on \(A\) if and only if for every \(\epsilon > 0\), there is apartition \(P\) of \(A\) such that
  \begin{equation}U(f, P) - L(f, P) < \epsilon\end{equation}
\end{lemma}
The proof is exactly like first year, and one direction should be immediate:
\begin{proof}
\begin{itemize}
  \item \(\impliedby\): if there's a partition \(P\) for every \(\epsilon\), then there's no ``room'' between the \(\sup\) and the \(\inf\)
  \item \(\implies\): if \(\sup = \inf\), then there are partitions \(P, P'\) such that
  \begin{equation}U(f, P) - \inf_{P \in \mc{P}} U(f, P) < \frac{\epsilon}{2} \land \sup_{P \in \mc{P}}L(f, P) - L(f, P') < \frac{\epsilon}{2} \implies U(f, P'') - L(f, P'') < \epsilon\end{equation}
  where \(P''\) is a refinement of \(P\) and \(P'\).
\end{itemize}
\end{proof}
Just like in first year. And also, the kind of first calculations of itnegrals that you've done in first year work out the same way here. For example,
\begin{enumerate}

  \item If \(f = c\) is a constant, then \(f\) is integrable and
  \begin{equation}\int_Af = cv(A)\end{equation}
  where \(v(A)\) denotes the volume of \(A\). This is because, for any subrectangle \(S\) of \(P\),
  \begin{equation}m_S(f) = c = M_S(f) \implies L(f, P) = \sum m_s(f)v(S) = c\sum v(S) = cV(A)\end{equation}
  with an analogous calculation for \(U(f, P)\)

  \item Let \(f: [0, 1] \times [0, 1] \to \reals\). Then
  \begin{equation}f(x, y) = \left\{\begin{array}{cc}
    0 & \text{if } x \in \rationals \\
    1 & \text{otherwise}
  \end{array}\right.\end{equation}
  is not integrable, since for any partition \(P\),
  \(m_S(f) = 0 \land M_S(f) = 1 \implies L(f, P) = 0 \land U(f, P) = 1\)

\end{enumerate}

\subsubsection{Basic Properties of the Integral}

Suppose we have two integrable functions
\begin{equation}f, g: A \to \reals\end{equation}
Then
\begin{enumerate}

  \item \(f + g\) is integrable and
  \begin{equation}\int_A(f + g) = \int_Af + \int_Ag\end{equation}

  \item \begin{equation}f \leq g \implies \int_Af \leq \int_Ag\end{equation}

  \item \(f \cdot g\) is integrable

  \item \(|f|\) is integrable and \begin{equation}
  \int_A|f| \geq \left|s22\int_Af\right| \geq \int_Af\end{equation}
  It's enough to show that \(|f|\) is integrable

  \item Exercise: any increasing function is integrable

\end{enumerate}


\subsection{Integrals Over More General Bounded Sets}

\TODO{this}

\subsection{Fubini's Theorem}

\TODO{rewrite}

We are now going to talk about Fubini's theorem, which is about how to integrate over a high-dimensional rectangle by repeatedly performing individual integrals. Let's start with an example. Suppose we want to integrate over a rectangle
\begin{equation}
  A = [a, b] \times [c, d] \subseteq \reals^2
\end{equation}
and let's suppose, to make it easy, we have a continuous, non-negative function \(f\), defined on this rectangle, say
\begin{equation}
  z = f(x, y) \geq 0
\end{equation}
The idea is that if we fix a point on the \(x\)-axis, say \(x\), we can consider the ``slice'' in the \(y\)-direction determined by this point. We could find, for example, the area of that slice, and it's reasonable to expect that the integral of \(f\) on the rectangle, we could obtain by integrating the area of that slice along the length of the rectangle.

The idea then is to try to find the area of such a slice, which would be in this case, for some \(x\),
\begin{equation}
  h(x) = \int_c^dg_x(y)dy = \int_c^df(x, y)dy
\end{equation}
where \(g_x(y) = f(x, y)\) fixing \(x\). It's reasonable to expect that
\begin{equation}
  \int_Af = \int_a^bh = \int_a^b\left(\int_c^df(x, y)dy\right)dx
\end{equation}
So that's the idea of Fubini's theorem: that we should be able to integrate a function over a rectangle by repeated one-dimensional integrals. Of course, we're not interested in integrating only continuous functions, we want to look at more general, integrable functions, but if you think about that, supposing \(f\) is integrable, this could run into a problem: one of the functions \(g_x\) might not be integrable on \([c, d]\). After all, it's set of discontinuities could be \(x_0 \times [c, d]\) for some \(x_0\), so \(\int_c^dg_{x_0}\) makes no sense!

So we'll have to formulate something maybe a little bit more technical, but it's to capture that problem. Suppose we just have \(f: A \to \reals\) bounded. The function may or may not be integrable, meaning the supremum of lower sums is equal to the infimum of lower sums. Whether the function is integrable or not, we can still look at the supermum of lower sums and the infimum of lower sums, which is what we'll do.

We'll define the \underline{lower} and \underline{upper integrals} of \(f\) on \(A\), \(\loint_Af\) and \(\hiint_Af\) respectively, to be the supremum of all the lower sums \(L(f, \mc{P})\) and infimum of all the upper sums \(U(f, \mc{P})\) respectively. We can now write down our theorem as the previous formula, but taking into account that we don't know that the function \(g_x\) mentioned before, we just replace it by the lower or upper integral:
\begin{theorem}
  Suppose \(A \subset \reals^m\) and \(B \subset \reals^n\) are closed rectangles and \(f: A \times B \to \reals\) is integrable. For all \(x \in A\), define
  \begin{equation}
    g_x: B \to \reals, g_x(y) = f(x, y)
  \end{equation}
  Set
  \begin{equation}
    \mc{L}(x) = \loint_Bg_x = \loint_Bf(x, y)dy, \mc{U}(x) = \hiint_Bg_x = \hiint_Bf(x, y)dy
  \end{equation}
  Then \(\mc{L}(x), \mc{U}(x)\) are integrable on \(A\) and
  \begin{equation}
    \int_{A \times B}f = \int_A\mc{L} = \int_A\mc{U} = \int_A\left(\loint_Bf(x, y)dy\right)dx = \int_A\left(\hiint_Bf(x, y)dy\right)dx
  \end{equation}
\end{theorem}
Before we prove this, some remarks:
\begin{enumerate}

  \item We also have
  \begin{equation}
    \int_{A \times B}f = \int_B\left(\loint_A(f(x, y)dx)\right)dy = \int_B\left(\hiint_A(f(x, y)dx)\right)dy
  \end{equation}

  \item If \(g_x\) is integrable on \(B\) for every \(x \in A\), then
  \begin{equation}
    \int_{A \times B}f = \int_A\left(\int_Bf(x, y)dy\right)dx
  \end{equation}

  \item If
  \begin{equation}
    A = [a_1, b_1] \times [a_2, b_2] \times ... \times [a_n, b_n]
  \end{equation}
  we can apply Fubini's theorem repeatedly to obtain
  \begin{equation}
    \int_Af = \int_{a_n}^{b_n} \left(... \int_{a_2}^{b_2}\left(\int_{a_1}^{b_1}f(x_1,...,x_n)dx_1\right)dx_2...\right) dx_n
  \end{equation}
  if \(f\) is ``sufficiently nice'' (otherwise we'll have to sprinkle in some L's or U's).

\end{enumerate}

Now here's an application to \(\int_Cf\) when \(f\) is integrable on a Jordan-measurable set \(C \subset \reals^n\). What we're going to use is Fubini's theorem in a rectangle with the formula
\begin{equation}
  \int_Cf = \int_Af\indic{C}
\end{equation}
where \(A \supseteq C\) is a closed rectangle.
Let's do some quick examples:
\begin{enumerate}

\item Let's say we were integrating on the region
\begin{equation}
  C = [-1, 1] \times [-1, 1] \setminus \{x \in \reals^2, x_1^2 + x_2^2 < 1\}
\end{equation}
i.e. the unit square with the unit circle removed from it. We have
\begin{equation}
  \int_Cf = \int_{[-1, 1]^2}f\indic{C}
\end{equation}
We can write
\begin{equation}
  \indic{C} = \left\{\begin{array}{cc}
    1 & \text{if } -1 \leq y \leq -\sqrt{1 - x^2} \text{ or } \sqrt{1 - x^2} \leq y \leq 1 \\
    0 & \text{otherwise}
  \end{array}\right.
\end{equation}
We can write
\begin{equation}
  \int_{-1}^1f(x, y)\indic{C}(x, y)dy = \int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy
\end{equation}
i.e. the limits of integration correspond to the boundary of \(C\). So we get
\begin{equation}
  \int_Cf = \int_{-1}^1\left(\int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy\right)dx
\end{equation}

\item Say we want to integrate over a triangle \(C\), under the line from \((0, 0)\) to \((a, a)\). We have
\begin{equation}
  \int_Cf = \int_0^a\left(\int_y^af(x, y)dx\right)dy
\end{equation}
We could have done it the other way, integrating first with respect to \(y\) and then with respect to \(x\), and get
\begin{equation}
  \int_Cf = \int_0^a\left(\int_0^xf(x, y)dy\right)dx
\end{equation}
This shows the form might not be the same in different directions. And it could be important to exploit the difference. For example, what if our function \(f(x, y)\) only depends on one of the variables, say \(y\) (i.e. \(f\) is \textit{independent} of \(x\)). Then
\begin{equation}
  \int_0^a\left(\int_0^xf(y)dy\right)dx
\end{equation}
doesn't simplify very much, but
\begin{equation}
  \int_0^a\left(\int_y^af(y)dx\right)dy = \int_0^a(a - y)f(y)dy
\end{equation}
So this double integral reduces to a single integral with respect to \(y\).

\end{enumerate}

\TODO{proof}

\TODO{polish}
Before we move on, we will give some more examples of computing integrals, using Fubini's theorem and the change of variables theorems as tools.

Let us begin by recalling some facts about the regions on whuch we can integrate. What are the kinds of region on which we integrate? One of the kinds of regions would be the region between the graphs of two functions that are defined on a Jordan-measurable set. Specifically, if we have a subset \(C \subset \reals^n\) that is Jordan-measurable, and two integrable functions \(\varphi(x) \leq \psi(x)\) defined on \(C\), we're interested in integrating over the region bounded by the graphs of the two functions Let's call this region \(S\), that is, define
\begin{equation}
  S = \{(x, y) \in \reals^n \times \reals : x \in C, y \in [\varphi(x), \psi(x)]\}
\end{equation}

We have that \(S\) is a Jordan-measurable set. We want to be able to integrate a continuous function on \(S\). If \(f(x, y)\) is a bounded continuous on \(S\), then we can compute using Fubini's theorem that
\begin{equation}
  \int_Sf = \int_C\left(\int_{\varphi(x)}^{\psi(x)}f(x, y)dy\right)dx
\end{equation}
We went through the proof of this before, but I want to use it to compute some examples.
\begin{enumerate}

  \item Consider
  \begin{equation}
    \int_0^2\int_{y/2}^1ye^{-x^3}dxdy
  \end{equation}
  There's no hope in proceeding naively, since \(e^{-x^3}\) doesn't even have an elementary primitive, i.e. you cannot write down it's integral with only elementary functions. But let's consider what region we're integrating along: the area under the line between \((0, 0)\) and \((2, 1)\). So using the above observation, we can rewrite the integral to be
  \begin{equation}
    \int_0^1\left(\int_0^{2x}ye^{-x^3}dy\right)dx = \int_0^1\left.\frac{y^2}{2}e^{-x^3}\right|_0^{2x}dx = 2\int_0^1x^2e^{-x^3}dx = \left.-\frac{2}{3}e^{-x^3}\right|_{0}^{1} = \frac{2}{3}(1 - e^{-1})
  \end{equation}
  Here, the change of variable was a useful thing to do, because it enabled us to write down an original integral, which we couldn't do in the original form.

  \item Consider
  \begin{equation}
    \int_2^4\int_{4/x}^{\frac{20 - 4x}{8 - x}}(y - 4)dydx
  \end{equation}
  So here, again, should we just go ahead and do it as it's written? Well then we'll get \(\frac{y^2}{2} - 4y\) and we'll substitute those things in and we'll just get a terrible mess. But what's the region we're integrating over? \(\frac{4}{x}\) is like a hyperbola, which is 2 when \(x\) is 2 and 1 when \(x\) is 4. The other function on top, what does it look like? It's also a hyperbola: it's a constant plus something over \(8 - x\), so it'll open down. And this is the region we're integrating on: the area between a hyperbola open up and another opening down.

  If we change the order of integration, we'll be integrating on the outside with respect to \(y\), which goes from 1 to 2, and we'll be integrating on the inside with respect to \(x\) with \(x\) going from \(4/y\) to... let's solve:
  \begin{equation}
    y = \frac{20 - 4x}{8 - x} = 4 + \frac{12}{x - 8} \implies x - 8 = \frac{12}{y - 4} \iff x = 8 + \frac{12}{y - 4}
  \end{equation}
  Hence we can write the above as
  \begin{equation}
    \int_1^2\left(\int_{4/y}^{8 + 12/(y - 4)}(y - 4)dx\right)dy = \left.\int_1^2(y - 4)x\right|_{4/y}^{8 + 12/(y - 4)}dy = \int_1^2(y - 4)\left(8 + \frac{12}{y - 4} - \frac{4}{y}\right)dy
  \end{equation}
  which is all stuff that's very easy to integrate

  \item Let's see how to integrate a simple function \(z\) over a region of \(3\)-space
  \begin{equation}
    S = \{(x, y, z) \in \reals^3 : x^2 + y^2 \leq z^2 \land x^2 + y^2 + z^2 \leq 1\}
  \end{equation}
  So, what is this region? The second equation says we're inside the closed ball of radius 1 centered at the origin. The first equation is the region inside two  cones, opening up and down from the origin (kind of like an hour glass). So we want the intersection of these two regions. In terms of the theorem that we wrote down at the beginning, the set \(C\) is like the discs inside the circles formed by the intersections of the top and bottom of the cones and circles, and we're integrating on the region between the semicircles above and below the plane, and the cones above and below the plane. So what's this going to be? Well, by symmetry, it's going to be \textit{zero}.

  Let's make it a little harder. Let's consider only the top, i.e.
  \begin{equation}S^+ = \{(x, y, z) \in S : z \geq 0\}\end{equation}
  So, rewriting our integral to be over the disc \(C\), we obtain
  \begin{equation}
  \begin{split}
    \int\int_{x^2 + y^2 \leq \frac{1}{2}}\left(\int_{\sqrt{x^2 + y^2}}^{\sqrt{1 - (x^2 + y^2)}}zdz\right)dxdy \\
    = \int\int_{x^2 + y^2 \leq \frac{1}{2}}\frac{1}{2}(1 - (x^2 + y^2) - (x^2 + y^2))dxdy \\
    = \frac{1}{2}\int\int_{x^2 + y^2 \leq \frac{1}{2}}dxdy - \int\int_{x^2 + y^2 - \frac{1}{2}}(x^2 + y^2)dxdy = \frac{1}{2}\pi\frac{1}{2} = \frac{\pi}{4}
  \end{split}
  \end{equation}
  How do we justify that last, ``magical'' step? Well, the prettiest way to do so is to change to polar coordinates, but we haven't justified this quite yet, which is the point of this example. You'll remember from first year caclulus that we can write
  \begin{equation}
    \int_a^bf(g(x))g'(x)dx = \int_{g(a)}^{g(b)}f(u)du
    \label{firstyearsub}
  \end{equation}
  This is what we're going to be doing in several variables. But first: polar coordinates? That means writing
  \begin{equation}
    x = r\cos\theta, y = r\cos\theta, \theta \in [0, 2\pi], r \in [0, \infty)
  \end{equation}
  This gives that
  \begin{equation}
    \prt{(x, y)}{(r, \theta)} = \begin{pmatrix}\cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta\end{pmatrix} \implies \det\prt{(x, y)}{(r, \theta)} = r \implies dxdy = drd\theta
  \end{equation}
  ``generalizing'' equation \ref{firstyearsub}. On the other hand, we have that
  \begin{equation}
    x^2 + y^2 = r^2
  \end{equation}
  Since we have that, for this disc, \(\theta\) ranges over the whole interval \([0, 2\pi]\) whereas \(r\) ranges over \([0, 1/\sqrt{2}]\), we can hence rewrite the above integral as
  \begin{equation}
    \int_0^{2\pi}\int_0^{\frac{1}{\sqrt{2}}}r^2 \cdot rdrd\theta =
    \int_0^{2\pi}d\theta\int_0^{\frac{1}{\sqrt{2}}}r^3dr =
    \left.2\pi\frac{r^4}{4}\right|_{0}^{\frac{1}{\sqrt{2}}} = \frac{\pi}{8}
  \end{equation}

\end{enumerate}


\subsection{Partitions of Unity}


\subsubsection{Bump Functions}

A bump function, in the one dimensional case, is a function which goes up to 1 from 0, stays there for a while, and then returns to zero. But we're interested in doing this in \(n\)-dimensions. How? Given an open \(U \subset \reals^n\), and compact \(C \subset U\), we want to construct a \(\mc{C}^\infty\) function \(f: U \to \reals\) with \(0 \leq f \leq 1\) such that \(\forall x \in C, f(x) = 1\) and \(f(x) = 0\) outside some compact subset of \(U\). So it looks like the 1-dimensional bump function, except over an \(n\)-dimensional area \(C\).

How can we construct such a function? We'll do it as a kind of series of exercises, based on one particular function, which you can imagine ought to be the kind of thing that you have to build this function, that is,
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    e^{1/x^2} & \text{if} & x \neq 0 \\
    0 & \text{if} & x = 0
  \end{array}\right.
  \label{bumpbuild}
\end{equation}
We proceed as follows, building up by steps:
\begin{enumerate}

  \item There is a \(\mc{C}^\infty\) function \(g: \reals \to \reals\) such that \(g > 0\) on \((-1, 1)\), 0 elsewhere. It's easy to write down a formula for such a function using the function given in \ref{bumpbuild}, as follows:
  \begin{equation}
    g(x) = \left\{\begin{array}{ccc}
      e^{-\frac{1}{(x - 1)^2}}e^{-\frac{1}{(x + 1)^2}} & \text{if} & x \in (-1, 1) \\
      0 & \text{if} & x \notin (-1, 1)
    \end{array}\right.
    \label{g}
  \end{equation}
  This works because \(e^{-\frac{1}{(x - 1)^2}}\) becomes completely flat at \(x = 1\), whereas \(e^{-\frac{1}{(x + 1)^2}}\) becomes completely flat at \(x = -1\).

  \item Given \(a \in \reals^n, \delta > 0\), there is a \(C^\infty\) function \(g_{a, \delta}: \reals^n \to \reals\) such that
  \begin{equation}
    g_{a, \delta} > 0 \text{ on } |x - a| < \delta, g_{a, \delta} = 0 \text{ outside}
  \end{equation}
  So how could achieve this? We could use the function defined in \ref{g} to write
  \begin{equation}
    g_{a, \delta} = g\left(\frac{|x - a|^2}{\delta^2}\right)
  \end{equation}

  \item There is a \(\mc{C}^\infty\) function \(h: U \to \reals\) such that \(h\) is greater than 0 on \(C\) and \(h = 0\) outside a compact subset of \(U\). So how do we do this?

  We could cover \(C\) with a finite set of open balls \(U_1,...,U_k\) with centers \(a_1,...,a_k\) and radii \(\delta_1,...,\delta_k\) whose closures lie in \(U\). We can do this since \(C\) is compact. Once we do this, we can define quite simply
  \begin{equation}
    h(x) = \sum_{i = 1}^kg_{a_k, \delta_k}(x)
    \label{compactsum}
  \end{equation}

  \item As a fourth thing, going back to the real line, let's show that if we're given any \(\epsilon \in \reals^+\), we can find a \(\mc{C}^\infty\) function \(\psi_\epsilon\) which is zero up to 0, becomes 1 at \(\epsilon\), and then remains 1 forever after. How can we do this from what we've already done? We're going to do it starting with a \(\mc{C}^\infty\) function like that in equation \ref{g}, but instead of on \((-1, 1)\), on \((0, \epsilon)\). More generally, actually, we can use any function \(g\) such that \(g > 0\) on \((0, \epsilon)\) and is zero outside. We can then define \(\psi_\epsilon\) by
  \begin{equation}
    \psi_\epsilon(x) = \frac{\int_0^xg}{\int_0^\epsilon g}
    \label{riseup}
  \end{equation}

  \item Now, putting everything together, how can we define the bump function that we wanted to begin with? We begin with the function \(h\) as defined in equation \ref{compactsum}, and we're going to modify it using equation \ref{riseup} as follows: letting \(\epsilon = \min_Ch > 0\), which exists since \(C\) is compact, let
  \begin{equation}
    f(x) = \psi_\epsilon \circ h
  \end{equation}

\end{enumerate}

Now there are other ways of doing it. One of them is to write down one integral formula from the beginning, but it would be harder to see just where this formula comes from in the first place.

\subsubsection{Definition}

We can now get to defining partitions of unity:
\begin{theorem}
  Given \(A \subset \reals^n\) and an open cover \(\mc{O}\) of \(A\), there is a countable collection \(\Phi\) of \(\mc{C}^\infty\) functions \(\varphi\) on \(\reals^n\) with the following properties:
  \begin{enumerate}

    \item \(0 \leq \varphi \leq 1\) \label{cond:partition_bounded}

    \item For all \(x \in A\), there is an open neighborhood \(V_x\) of \(x\) in \(\reals^n\) such that all but finitely many \(\varphi\) vanish on \(V_x\) \label{cond:finite_vanish}

    \item For all \(x \in A\), \(\sum_{\varphi \in \Phi}\varphi(x) = 1\) (sum is finite in a neighborhood of \(x\)) \label{cond:partition_sum_finite}

    \item For all \(\varphi \in \Phi\), there is some open \(U \in \mc{O}\) such that \(\varphi = 0\) outside a compact subset of \(U\). \label{cond:subordinate}

  \end{enumerate}

  \label{cheapway}
\end{theorem}
\begin{definition}
  A collection \(\varphi\) satisfying conditions
  \ref{cond:partition_bounded},
  \ref{cond:finite_vanish},
  \ref{cond:partition_sum_finite}
  above is called a \underline{\(\mc{C}^\infty\) partition for \(A\)}.
  If condition \ref{cond:subordinate} holds, this partition is called \underline{subordinate to \(\mc{O}\)}
\end{definition}
This is all \(\mc{C}^\infty\) stuff, but we could have made something weaker. We could have asked for a \(\mc{C}^r\) partition, or even a partition that was merely continous. So suppose we did that, and only cared about a \(\mc{C}^0\) partition. Then, we could also, though the proof will have to come next time, we could use functions that look like trapezoids, using \(|x|\) instead of \(e^{-\frac{1}{x^2}}\). If we wanted a \(\mc{C}^r\) partition of unity, then of course, we could do the same thing, but only caring about \(\mc{C}^r\) instead of \(\mc{C}^\infty\), and hence using the function given by
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    |x^{r + 1}| & \text{on} & [0, \infty) \\
    -|x^{r + 1}| & \text{on} & (-\infty, 0]
  \end{array}\right.
\end{equation}

Before we try to construct such a (\(\mc{C}^\infty\)) partition, let's discuss why this is an interesting idea. We're going to use this for several things in this course:
\begin{itemize}
  \item To extend the definition of the integral to more general regions
  \item To prove the change of variables theorem
  \item To extend the definition of the integral to manifolds
  \item To prove Stoke's theorem
\end{itemize}
So why? Suppose we wanted to extend the definition of the integral to manifolds. Take some shape in 3-space, with a boundary, which is a manifold. Right now, we can integrate over a rectangle. Once we prove the change of variables theorem, we'll be able to integrate over a space diffeomorphic to a rectangle, a sort of deformed rectangle. Now, if we have a coordinate chart, a diffeomorphism from \(\reals^n\) to some part of the manifold, we could use this to extend the definition of the integral from the rectangle to at least some kind of ``rectangular'' piece of the manifold. The question remains: how can we extend the definition to the \textit{entire} manifold? A reasonable way to try to do this is to try to cover the entire manifold by a grid of rectangles. If we add up the integrals on each of those pieces...

That was the way this was doen classically: you'd take a manifold and divide it up by some kind of rectangular grid, and just add up the contributions of every piece. This classical approach, however, ran into a big obstacle, which was: how do you know that you can really cover every manfiold with such a grid? How can you really divide up a really complex surface into rectangles? That is an extremely hard problem: it's true you can do it, but it's an extremely hard problem, and was solved way after the extension of integrals to manifolds.

So how can we avoid this issue? The way this was solved was, you do something a million times simpler. When you cover the manifold with rectangles, you don't care that they all fit together in a nice grid. Just, for every point, you take a rectangle containing that point, and we don't care whether they overlap or not. How do we exploit that? We use a partition of unity! Because what we'll do is, we'll pick a partition of unity such that every one of these bump functions sits in teh interior of one of these rectangles. Now, if we want to integrate our fucntion \(f\) over the manifold, and we have our partition of unity \(\Phi\), and we look at \(\varphi f\), it vanishes outside one of these rectangles, so we can just integrate over the rectangle and get the answer for the manifold. We can then write
\begin{equation}
  \int f = \sum\int\varphi f
\end{equation}
So the key is to take a function, and make it's support very small, and then work with those small pieces. And we're going to use this in the same way to prove Stoke's theorem. Stoke's theorem is like the FTC. What does the FTC say? It says that
\begin{equation}
  \int_a^bf' = f(a) - f(b)
\end{equation}
So the integral of \(f\) over the \underline{boundary} of \([a, b]\) is the same as the integral of the derivative of \(f\) over the whole interval. So Stoke's theorem is that the integral of a function over the boundary of a manifold is the same as the integral of the derivative over the whole manifold.

We can prove this for rectangles. Then, if we want to prove it for manifolds, again we need to try to cover the manfolds with a grid like this. If we can do that, we can prove Stoke's theorem for surfaces that we know can be covered by a grid. But we can do it for general manifolds in a really cheap way, again by multiplying with partitions of unity. So that's whay we're going to do in the rest of the course. But first, we have to prove Theorem \ref{cheapway}, because that's what we're going to use to glue together little pieces in a very nice but cheap way.

\begin{proof}

  The way we're going to do this is by considering a number of cases building up to the general case.

  \begin{enumerate}

    \item Assume \(A\) is compact. That means finitely many \(U \in \mc{O}\) cover \(A\). Call them \(U_1,...,U_q\). In this case we're going to construct a \textit{finite} (not countable) partition of unity subordinate to \(\{U_1,...,U_q\}\).

    For each \(i \in \{1,...,q\}\), the first thing that we're going to do is show that we can find a compact set \(D_i \subset U_i\) such that the interiors of the \(D_i\) cover \(A\). How?

    For all \(x \in A\), let \(D_x\) be a closed ball with centre \(x\) lying inside some \(U_i\). Since \(A\) is compact, finitely many of the interiors of the \(D_x\) cover \(A\). Number them \(D_{x_1},...,D_{x_p}\). For each \(i \in \{1,...,q\}\), let
    \begin{equation}
      D_i = \bigcup_{D_{x_j} \subset U_i}D_{x_j}
    \end{equation}

    Let \(\psi_i\) be a \(\mc{C}^\infty\) function \(0 \leq \psi_i \leq 1\) such that \(\psi_i > 0\) on \(D_i\), and is 0 outside a compact subset of \(U_i\). Notice if we do this, then, since the interior of the \(D_i\)'s cover's \(A\),
    \begin{equation}
      \forall x \in U \supseteq A, \sum_{i = 1}^q\psi_i(x) > 0
    \end{equation}
    where \(U\) is an open neighborhood of \(A\). SO we define our partition of unity as
    \begin{equation}
      \Phi = \{\varphi_1,...,\varphi_q\}, \varphi_i = \frac{\psi_i}{\sum_{i = 1}^q\psi_i}
    \end{equation}
    \label{compactcase}

    \item Now assume \(A = \bigcup_{i \in \nats}A_i\) where each \(A_i\) is compact and a subset of the interior of \(A_{i + 1}\). Let \(B_i = A_i \setminus \Int A_{i - 1}\). Let \(\mc{O}_i\) be covers of \(B_i\) by the open sets
    \begin{equation}
      U \cap (\Int A_{i + 1} \setminus A_{i - 2})
    \end{equation}
    for each \(U \in \mc{O}\). By Case \ref{compactcase} there is a finite partition of unity \(\Phi_i\) for \(B_i\) subordinate to \(\mc{O}_i\). Consider
    \begin{equation}
      \sigma = \sum_i\sum_{\psi_i \in \Phi_i}\psi_i
    \end{equation}
    This is a finite sum in some neighborhood of every \(x \in A\). Take for a partition
    \begin{equation}
      \varphi = \frac{\psi}{\sigma}
    \end{equation}
    for each \(\psi \in \Phi_i\), for every \(i\).

    \label{compacttowercase}

    \item Assume \(A\) is open. Then \(A = \bigcup_{i \in \nats}A_i\) as in Case \ref{compacttowercase}. How can we get such a representation? Try
    \begin{equation}
      A_i = \{x : |x| \leq i, d(x_, \reals^n \setminus A) > i^{-1}\}
    \end{equation}

    \label{opencase}

    \item Let \(A\) be arbitrary. Apply Case \ref{opencase} to \(\bigcup_{U \in \mc{O}}U\). A partition of unity subordinate to \(\mc{O}\) is also a partition of unity for \(A\) subordinate to \(\mc{O}\)

  \end{enumerate}

\end{proof}

\subsection{Extended Definition of the Integral (``Improper Integral'')}
Suppose that \(A\) is open and \(f\) is bounded and continuous. Then under the current definition, the expression
\begin{equation}
  \int_Af
\end{equation}
may not exist, since the boundary may not be Jordan-measurable. So we now want to try and rectify that, by attempting to extend the definition of the integral to open subsets \(A \subset \reals^n\) and \textit{locally} bounded functions \(f: A \to \reals\), i.e. for every point in \(A\), there is an open neighborhood around \(A\) such that \(f\) is bounded around \(A\). Furthermore, we assume the set of discontinuities is of measure zero.

Just like the improper integral from first year calculus, it won't always exist. But we'll want to know the value when it does. We'll begin with a useful note: if \(f\) vanishes outside of a compact subset \(C\) of \(A\), then in fact \(f\) \textit{is} integrable on \(C\) (since we can put that compact subset in a big rectangle and multiply by the characteristic function). In fact, we can say that \(f\) is integrable on any bounded open subset \(U\) of \(A\) containing \(C\), for the same reason, and
\begin{equation}
  \int_Uf = \int_Cf
\end{equation}
So it makes sense to say
\begin{equation}
  \int_Af = \int_Cf
  \label{conventioncompact}
\end{equation}
This is going to be our starting point. Now to go further, we're going to talk about a partition of unity. Let \(\mc{O}\) be an open cover of \(A\) such that \(U \subset A\) for all \(U \in \mc{O}\) and let \(\Phi\) be a partitionof unity for \(A\) subordinate to \(\mc{O}\). Note that this is the same as saying let \(\Phi\) be a partition of unity for \(A\) subordinate to \(\{A\}\), since that counts as an open covering of \(A\). As in equation \ref{conventioncompact},
we can define, \(\forall \varphi \in \Phi\),
\begin{equation}
  \int_A\varphi|f|
\end{equation}
We can now extend the definition of the integral as follows:
\begin{definition}
  \(f\) is \underline{integrable} on \(A\) if
  \begin{equation}
    \sum_{\varphi \in \Phi}\int_A\varphi|f|
    \label{absolute}
  \end{equation}
  converges. Then we define
  \begin{equation}
    \int_Af = \sum_{\varphi \in \Phi}\varphi f
    \label{relative}
  \end{equation}
\end{definition}
Note that equation \ref{relative} must converge, and in fact converges absolutely, if equation \ref{absolute} converges, because
\begin{equation}
  \sum_{\varphi \in \Phi}\left|\int_A\varphi f\right| \leq \sum_{\varphi \in \Phi}\int_A|\varphi||f| = \sum_{\varphi \in \Phi}\int_A\varphi f
\end{equation}
since \(\varphi\) is nonnegative.
\begin{theorem}
  Suppose \(A \subset \reals\) is open, \(f: A \to \reals\) is locally bounded and the set of discontinuities of \(f\) has measure 0.
  \begin{enumerate}

    \item Given partitions \(\Phi, \Psi\) as above, not necessarily subordinate to the same open covering, though this doesn't matter since both will be subordinate to \(\{A\}\), then if equation \ref{absolute} converges for \(\Phi\), then it converges for \(\Psi\) and the integral defined using \(\Phi\) and the integral defined using \(\Psi\) are equal, i.e.
    \begin{equation}
      \sum_{\psi \in \Psi}\int_A\psi f = \sum_{\varphi \in \Phi}\int_A\varphi f
      \label{integralequal}
    \end{equation}
    Note that equation \ref{integralequal} would not necessarily be true if equation \ref{absolute} does not converge.

    \item The new definition of the integral \textit{always} exists if both \(A\) and \(f\) are bounded, showing it truly generalizes the old definition. Specificially, if both \(A\) and \(f\) are bounded, whether or not the boundary of \(A\) has measure zero,
    \begin{equation}
      \sum_{\varphi \in \Phi}\int_A\varphi|f|
    \end{equation}
    (equation \ref{absolute}) converges

    \item If \(A\) is Jordan-measurable and \(f\) is bounded, then
    \begin{equation}
      \int_Af = \sum_{\varphi \in \Phi}\int_A\varphi f
    \end{equation}
    where the left is as defined before.

  \end{enumerate}
\end{theorem}
There are other ways of defining the integral, some of which look more like the improper integral from first year calculus. Another thing that you could do is the following: since you can always write an open set as a kind of ``expanding union'' of compact sets \(C_i \subseteq C_{i + 1}\), you can actually do that in such a way such that each \(C_i\) is Jordan-measurable, meaning the integral on each of these \(C_i\)'s exist. We could then define
\begin{equation}
  \int_Af = \lim_{i \to \infty}\int_{C_i}f
\end{equation}
After we finish proving this theorem, either I'll prove this or stick it on the next problem set.
\begin{proof}
  \begin{enumerate}

    \item \(\varphi \cdot f\) vanishes outside a compact set, and only finitely many \(\psi\) are nonzero on this set. So
    \begin{equation}
      \sum_{\psi \in \Psi}\psi = 1
      \implies \sum_{\varphi \in \Phi}\int_A\varphi f
      = \sum_{\varphi \in \Phi}\int_A\left(\sum_{\psi \in \Psi}\psi\right)\varphi f
      = \sum_{\varphi \in \Phi}\sum_{\psi \in \Psi}\int_A\varphi\psi f
    \end{equation}
    A priori, we don't know that we can interchange the order in the double sum of the last equal expression, but we could write down exactly the same stuff with the absolute value in it, which tells us we can. Interchanging them, we get the desired result.

    \item Let \(B\) be a big rectangle containing \(A\), and assume \(|f| \leq M\) on \(A\). For any finite subset \(F\) of \(\Phi\),
    \begin{equation}
      \sum_{\varphi \in F}\int_A\varphi|f| \leq M\int_A\sum_{\varphi \in F}\varphi \leq MV(B)
    \end{equation}
    implying the desired result. We still have to show that in the situation that \(\int_Af\) is defined according to our earlier definition, we get the same result. We'll do so next time.

  \end{enumerate}
\end{proof}

Let's recap. What we've shown so far is that
\begin{enumerate}

  \item The existence of the integral and it's value is independent of the parititon of unity chosen to compute it.

  \item The integral \textit{always} exists if \(A\) and \(f\) are both bounded.

  \item We \textit{want} to show that if we're in a situation where the integral exists according to our old definition, so that in particular \(A\) and \(f\) will be bounded, then our new definition agrees with the old one. That is, if moreover, \(A\) is Jordan measurable, then
  \begin{equation}
    \int_Af = \sum_{\varphi \in \Phi}\int_A\varphi f
  \end{equation}
  where the left hand side is computed using the old definition
\end{enumerate}
\begin{proof}
  For every \(\epsilon > 0\), there is a compact Jordan-measurable subset \(C \subset A\) such that \begin{equation}
    V(A \setminus C) = \int_{A \setminus C}1 = \int_A\chi_{A \setminus C} < \epsilon
  \end{equation}
  So how do we get this \(C\)? We're given that \(A\) is Jordan measurable, so as \(A\) is bounded we can cover the boundary of \(A\) by the interiors of finitely many closed balls \(\mc{B}\) (or rectangles) of total volume \(< \epsilon\). So we can take
  \begin{equation}
    C = A \setminus \Int\bigcup\mc{B}
  \end{equation}
  (we could also take the union of interiors). Why is \(C\) Jordan measurable? Since the boundary of \(C\) is a subset of the boundary of \(\Int\bigcup\mc{B}\), which is has measure zero, it follows that \(C\) has boundary of measure zero and is hence Jordan measurable.

  Since \(C\) is compact, only finitely many of our partition functions \(\varphi \in \Phi\) are nonzero on \(C\). So we'll take a finite sum, look at the difference, and show that that difference goes to zero: consider any finite subset \(F\) of \(\Phi\) including the ones which are nonzero on \(C\). Consider
  \begin{equation}
    \left|\int_Af - \sum_{\varphi \in F}\int_A\varphi f\right|
    \label{absint}
  \end{equation}
  As the absolute value of the integral is less than or equal to the integral of the absolute value, we have that equation \ref{absint} is less than or equal to
  \begin{equation}
    \int_A\left|f - \sum_{\varphi \in F}\varphi f\right|
    \label{intabs}
  \end{equation}
  Since \(f\) is bounded, we can choose \(M\) such that \(|f| \leq M\). So equation \ref{intabs} is less than or equal to
  \begin{equation}
    M\int_A\left(1 - \sum_{\varphi \in F}\varphi\right)
    = M \int_A\sum_{\varphi \in \Phi \setminus F}\varphi
    \label{intrem}
  \end{equation}
  Since
  \begin{equation}
    \forall \varphi \in \Phi \setminus F, \forall c \in C, \varphi(c) = 0
  \end{equation}
  we have that equation \ref{intrem} is less than or equal to
  \begin{equation}
    MV(A \setminus C) \leq M\epsilon
  \end{equation}

\end{proof}
As I mentioned last time, you can do this in another way which perhaps looks more like the improper integral from first year calculus, and that's integrating with bigger and bigger sets. Let me state that, but I won't prove it, and rather put it on the problem set:
\begin{theorem}
  Let \(A\) be open, \(f: A \to \reals\) be locally bounded and let the set of discontinuties of \(f\) have measure zero. Write
  \begin{equation}
    A = \bigcup_{n \in \nats}C_n
  \end{equation}
  where each \(C_n\) is compact and Jordan Measurble and satisfy \(C_n \subset \Int C_{n + 1}\). Part of the exercise is to show that we can do this. Then \(f\) is integrable on \(A\) if and only if
  \begin{equation}
    \left\{\int_{C_i}|f| : n \in \nats\right\}
  \end{equation}
  is bounded. Note that if this is the case, the sequence
  \begin{equation}
    \left\{\int_{C_i}f : n \in \nats\right\}
  \end{equation}
  converges absolutely, and we can write
  \begin{equation}
    \int_Af = \lim_{n \to \infty}\int_{C_n}f
  \end{equation}
\end{theorem}
\begin{proof}
  Exercise.
\end{proof}
It's good to appreciate that the improper integral which we defined using a partition of unity can be expressed in this way. It's also really good to do the above exercise as an exercise in using partitions of unity.

There's a little gap in the treatment of partitions of unity last time which I want to fill here. In the first case, where \(A\) was compact, we defined
\begin{equation}
  \psi_1,...,\psi_p
\end{equation}
such that \(\psi_1 + ... \psi_p > 0\) on an open set \(U \supseteq A\). We then let
\begin{equation}
  \varphi_i = \frac{\psi_i}{\psi_1 + ... \psi_p}
\end{equation}
This is all well and good, but individual \(\varphi_i\) might be defined on sets that go outside \(U\), and we have to worry about dividing by zero to guarantee that each bump function is \(\mc{C}^\infty\) everywhere. To deal with this, we can simply multiply everything by another bump function (we can find one which is 1 on \(A\) and zero outside a compact set \(U\)).

Let \(f(x)\) be a \(\mc{C}^\infty\) bump function such that \(f = 1\) on \(A\), \(f = 0\) outside a compact subset of \(U\). Then the set of functions
\begin{equation}
  \{f \cdot \varphi_i : i \in 1,...,p\}
\end{equation}
is a partition of unity as required.

Before we finish, I want to say something about integration by substitution, or change of variables.

\subsection{Change of Variables}

Recall the single-variable \underline{integration by substitution} formula
\begin{equation}
  \int_{g(a)}^{g(b)} f = \int_a^b(f \circ g)g'
  \label{onesubform}
\end{equation}
This holds whenever ``both sides make sense'', e.g. if \(f\) is continuous and \(g\) is continuously differentiable. The hypotheses, however, could be weaker. When we talk about integration by substitution, our \(g\) is going to be the change of variables. But change of variables means like ``invertible''. So not just any \(g\), but in particular 1-to-1. I wanted to point out how, in terms of our formula, we should think about this in the case where \(g\) is one-to-one. So suppose it is so. Then equation \ref{onesubform} can be rewritten as
\begin{equation}
  \int_{g([a, b])}f = \int_{[a, b]}(f \circ g)|g'|
\end{equation}
since otherwise the equaton would not be correct if \(g\) was decreasing. So this is what our change-of-variables formula is going to look like. Let's formalize this into a theorem:

\begin{theorem}[Change of Variables]
  Let \(A \subset \reals^n\) be open, \(g: A \to \reals^n\) be one to one, continuously differentiable and let, for all \(x \in A\), \(g'(x) \neq 0\). Then
  \begin{equation}
    f : g(A) \to \reals
  \end{equation}
  is integrable if and only if
  \begin{equation}
    f \circ g|\det g'|
  \end{equation}
  is integrable on \(A\). In this case,
  \begin{equation}
    \int_{g(A)}f = \int_Af \circ g|\det g'|
  \end{equation}
  \label{covtheorem}
\end{theorem}

\subsubsection{Examples}

Let's look at some examples, starting with polar coordinates: we use coordinates \(r \in \reals^+_0, \theta \in [0, 2\pi]\) and write
\begin{equation}
  x = r\cos\theta, y = r\sin\theta
\end{equation}
We have
\begin{equation}
  D = \prt{(x, y)}{(r, \theta)} = \begin{pmatrix}
    \cos\theta & -r\sin\theta \\
    \sin\theta & r\cos\theta
  \end{pmatrix} \implies \det D = r
\end{equation}
So we can write
\begin{equation}
  \iint_Af(x, y)dxdy = \iint_Af(r\cos\theta, r\sin\theta)rdrd\theta
\end{equation}
We now examine a corollary of Theorem \ref{covtheorem}.

\begin{corollary}
  Let \(A \subset C \subset \reals^n\) where \(A\) is open, \(C\) is compact and Jordan-measurable and \(C \setminus A\) has measure zero. If \(g\) is a continuously differentiable function from a neighborhood of \(C\) to \(\reals^n\) wich satisfies the conditions of theorem \ref{covtheorem} on \(A\), then
  \begin{equation}
    f: g(C) \to \reals
  \end{equation}
  is integrable if and only if
  \begin{equation}
    f \circ g|\det g'|
  \end{equation}
  is iintegrable on \(C\), and in this case
  \begin{equation}
    \int_{g(C)}f = \int_Cf \circ g|\det g'|
  \end{equation}
  \label{corcov}
\end{corollary}
\begin{lemma}
  Assume \(A \subset \reals^n\) is open and \(g: A \to \reals^n\) is continuously differentiable. If \(B \subset A\) has measure zero, then \(g(B)\) has measure zero.
\end{lemma}
\begin{proof}
  Enough to prove that \(g(B \cap C)\) has measure zero for any \(C \subset A\) compact, since \(A\) has an exhaustion by countably many compact sets \(C_1 \subset C_2 \subset ...\)

  To do so, remember that a countable intersection of measure 0 sets is measure 0. Using \(\mc{C}_1\), which is more than uniformly continuous, we have that
  \begin{equation}
    \forall x \in C, \forall y \in U, |g(x) - g(y)| \leq c|x - y|
  \end{equation}
  where \(U\) is some neighborhood of \(C\). So \(g\) maps a ball of radius \(\epsilon\) to a ball of radius \(c\epsilon\).
\end{proof}
We now proceed to prove Corollary \ref{corcov}
\begin{proof}
  \(g(C) \setminus g(A) \subseteq g(C \setminus A)\), and so it is of measure zero. We hence have that
  \begin{equation}
    \int_{g(A)}f = \int_{g(C)}f
  \end{equation}
  \begin{equation}
    \int_A(f \circ g)|\det g'| = \int_{C}(f \circ g)|\det g'|
  \end{equation}
  giving the desired equality by Theorem \ref{covtheorem}
\end{proof}
Let's move on to another example: what are called spherical coordinates. We use coordinates \(r \in \reals^+_0, \phi \in [0, 2\pi], \theta \in [0, \pi]\) where
\begin{equation}
  x = r\cos\phi\sin\theta, y = r\sin\phi\sin\theta, z = r\cos]theta
\end{equation}
We have... this is going to hurt...
\begin{equation}
  D = \prt{(x, y, z)}{(r, \theta, \phi)} = \begin{pmatrix}
    \cos\phi\sin\theta & r\cos\phi\cos\theta & -r\sin\phi\sin\theta \\
    \sin\phi\sin\theta & r\sin\phi\cos\theta & r\cos\phi\sin\theta \\
    \cos\theta & -r\sin\theta & 0
  \end{pmatrix} \implies \det D = r^2\sin\theta
\end{equation}
Now, for some problems:
\begin{enumerate}

  \item Find the volume of an ellipsoid
  \begin{equation}
    E = \left\{(x, y, z) \in \reals^3 : \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} \leq 1\right\}
  \end{equation}
  We can make a change of variable to ``transform'' this into a sphere. Specifically, we can use the transformation
  \begin{equation}
    g : \reals \to \reals^3, (x, y, z) \mapsto (x/a, y/b z/c)
  \end{equation}
  to take the ellipsoid to the unit ball, where \((u, v, w) = g(x, y, z)\). So
  \begin{equation}
    \iiint_{B_1(\mb{0})}1dudvdw = \iiint_{E}\frac{1}{abc}dxdydz = \frac{1}{abc}\iiint_{E}1dxdydz = \frac{4}{3}\pi \implies V(E) = \frac{4}{3}\pi abc
  \end{equation}
  Now what if we didn't know the volume of a ball of radius 1? We could figure it out using change of variables, using spherical coordinates:
  \begin{equation}
    \int_{0}^{2\pi}\int_{0}^{\pi}\int_0^1r^2\sin\theta drd\theta d\phi = \int_0^1r^2dr\int_0^\pi\sin\theta d\theta\int_0^{2\pi}d\phi = \frac{1}{3} \cdot 2 \cdot 2\pi = \frac{4\pi}{3}
  \end{equation}
  Of course, the volume of the sphere of radius \(r\) can be treated as a special case of the ellipsoid where \(a = b = c = r\), giving volume \(\frac{4}{3}\pi r^3\).

  \item What's the simplest kind of change of variable, save the identity? Of course, a linear change of variable. Consider a linear transformation \(T: \reals^n \to \reals^n\) and suppose \(A \subset \reals^n\) is Jordan measurable and \(B = T(A)\). Show that \(B\) is Jordan measurable and \(V(B) = V(A)|\det T|\). We split this into two cases:
  \begin{enumerate}

    \item Assume \(T\) is not invertible, i.e. \(\det T = 0\). Then the image \(I\) of \(T\) is a linear subspace of \(\reals^n\) of dimension less than \(N\), and \(B \subset I\). So all of \(B\) has \(n\)-dimensional measure zero, and hence \(V(B) = 0 = V(A)|\det T|\) as desired.

    \item Assume \(T\) is invertible, i.e. \(\det T \neq 0\). Then \(B\) is Jordan measurable and now by the change of variable theorem
    \begin{equation}
      V(B) = \int_B1 = \int_A1 \cdot |\det T| = |\det T|\int_A1 = V(A)|\det T|
    \end{equation}
    as desired.

  \end{enumerate}

\end{enumerate}

\subsubsection{Proof of the Change of Variables Theorem}

We're now going to prove the Change of Variables theorem. Let's begin by considering just one direction:
\begin{theorem}[Change of Variables (One Direction)]
  Let \(A \subset \reals^n\) be open and let \(g: A \to \reals^n\) be one to one and continuously differentiable such that \(\det g'(x) \neq 0\) on \(A\). Then if \(f: g(A) \to \reals\) is integrable, \(f \circ g|\det g'|\) is integrable on \(A\) and
  \begin{equation}
    \int_{g(A)}f = \int_A(f \circ g)|\det g'|
  \end{equation}
  \label{changeofvar1d}
\end{theorem}
Note that the converse, and hence the full theorem, follows, since
\begin{itemize}
  \item As \(g\) is invertible \(f\) is locally bounded if and only if \(f \circ g|\det g'|\) is locally bounded
  \item The discontinuities of \(f\) have measure zero if and only if the discontinuities of \(f \cdot g|\det g'|\) have measure zero, since \(g\) is differentiable, using the lemma from last time.
  \item We can use \(g^{-1}\) to go from \(g(A)\) to \(A\).
\end{itemize}
We still have to show that if \(F = f \circ g|\det g'|\) is integrable on \(A\), then \(f\) is integrable on \(g(A)\). All we have to do is apply the theorem with \(F), g^{-1}\) instead of \(f, g\). Then
\begin{equation}
  F \circ g^{-1}|\det(g^{-1})'| = f \circ \cancel{g \circ g^{-1}} \cancel{|\det g' \circ g^{-1}|}\frac{1}{\cancel{|\det g' \circ g^{-1}|}} = f
\end{equation}
as desired.

We can now begin to prove the theorem in a number of steps. We're going to start by looking at a Jordan-measurable open subset \(U\) of \(A\) with closure in \(A\), e.g. an open rectangle with closure in \(A\). The first thing that we're going to do is show that it's ``good enough'' to prove the theorem on such an open subset. How? With a partition of unity! That's going to simplify things a lot, but just one thing I want to note first is that if \(f\) is integrable on \(g(A)\), then \(f\) is integrable on \(g(U)\) and \(f \circ g|\det g'|\) is integrable on \(U\).


We observe that, if \(U\) is a Jordan-measurable subset of \(A\) with closure in \(A\), then if \(f: g(A) \to \reals\) is integrable then \(f\) is integrable on \(g(U)\) and \(f \circ g|\det g'|\) is integrable on \(U\).

We're now going to attempt to prove Theorem \ref{changeofvar1d} in small steps:
\begin{enumerate}

  \item Suppose \(\mc{O}\) is an open cover of \(A\) by sets like \(U\) above. Suppose that for each \(U\), the theorem holds, i.e. \(f\) is integrable on \(g(U)\) and
  \begin{equation}
    \int_{g(U)}f = \int_U(f \circ g)|\det g'|
  \end{equation}
  Then the theorem is true. What we're saying here is it's good enough to, instead of proving the thoerem as stated, just prove it for thse Jordan-measurable \(U\)'s. So, why is this true?

  If we have an open covering \(\mc{O}\) of \(A\), taking the images of everything in the cover, the set
  \begin{equation}
    \mc{O}' = \{g(U) : U \in \mc{O}\}
  \end{equation}
  is an open cover of \(g(A)\) of some kind, since \(g\) is invertible. Now let \(\Phi\) be a \(\mc{C}^0\) partition of unity for \(g(A)\) subordinate to \(\mc{O}'\).

  That means each partition function \(\varphi \in \Phi\) is zero outside of a compact subset of some \(g(U)\). But notice that, if this is true, then \(\varphi \circ g\) must be zero outside of a compact subset of \(U\). This is because \(g\) is invertible. This is the last place in the proof that we're going to use the fact that \(g\) is one-to-one.

  So this tells us that
  \begin{equation}
    \Phi' = \{\varphi \circ g : \varphi \in \Phi\}
  \end{equation}
  forms a partition of unity for \(A\) subordinate to \(\mc{O}\). So we have that \textit{by our assumption}
  \begin{equation}
    \int_{g(A)}\varphi \circ g = \int_{g(U)}\varphi \circ f = \int_U(\varphi \cdot f) \circ g|\det g'| = \int_A(\varphi \cdot f) \circ g|\det g'|
  \end{equation}
  by the nature of partitions of unity. So, the thing that we're interested in, the integral of \(f\) over \(g(A)\), can be written, since \(\Phi\) is a partition of unity, as
  \begin{equation}
    \int_{g(A)}f = \sum_{\varphi \in \Phi}\int_{g(A)}\varphi f = \sum_{\varphi \in \Phi}\int_A(\varphi \cdot f) \circ g|\det g'| = \int_Af \circ g|\det g'|
    \label{thisequality}
  \end{equation}
  Note that the same equality as \ref{thisequality} with \(|f|\) in place of \(f\) shows that the RHS converges, which is our definition of intregrability.

  We could make the same conclusion that the theorem is true not by assuming this for an open covering of \(A\) but rather looking at the same things for an open covering of \(g(A)\). That is, it's equivalent for this claim to suppose that if we have an open covering \(\mc{O}\) of \(g(A)\) by open subsets of this kind (Jordan measurable, closure lying in \(g(A)\)), then, for all \(V \in \mc{O}\) and all \(f\) integrable on \(V\),
  \begin{equation}
    \int_Vf = \int_{g^{-1}(V)}(f \circ g)|\det g'|
  \end{equation}

  \item It's enough to prove that for every open rectangle \(V\) in \(g(A)\) with closure in \(g(A)\) we have
  \begin{equation}
    \int_Vf = \int_{g^{-1}(V)}(f \circ g)|\det g'|
  \end{equation}
  whenever \(f\) is integrable on \(V\). If I just said that, that would be no further reduction at all, since this is just what we already said. But that's not what I'm going to say here. What I'm going to say is that it's enough to prove this when \(f\) is the constant function \(1\). So this is a big reduction.

  Again, when I say it's ``enough'' to prove that, the meaning is that if you prove this, then the theorem follows, i.e. it is enough to prove the theorem. Why is this the case, however?

  Let \(P\) be a partition of \(V\). For every subrectangle \(S \in P\), let \(f_S\) be the constant function \(m_S(f)\). We have
  \begin{equation}
    \mc{L}(f, P) = \sum_{S \in P}m_S(f)V(S) = \sum_{S \in P}\int_{\Int S}f_S
    \label{partint}
  \end{equation}
  By this assumption for constant functions, we can rewrite equation \ref{partint} as
  \begin{equation}
    \sum_{S}\int_{g^{-1}(\Int S)}f_S \circ g|\det g'| \leq \sum_{S}\int_{g^{-1}(\Int S)}f \circ g|\det g'| \leq \int_{g^{-1}(V)}f \circ g|\det g'|
  \end{equation}
  So what does this tell us about the integral of \(f\) over \(V\)? We have that
  \begin{equation}
    \int_Vf = \sup_P\mc{L}(f, P) \leq \int_{g^{-1}(V)}f \circ g|\det g'|
  \end{equation}
  The same argument using upper sums gives \(\geq\).

  \item If the theorem is true for \(g : A \to \reals^n\), and \(h: g(A) \to \reals^n\), with the same hypotheses (one-to-one, \(\mc{C}^1\), \(\det h' \neq 0\)) then it is true for \(h \circ g\). Why?
  \begin{equation}
    \int_{(h \circ g)(A)}f = \int_{g(A)}(f \circ h)|\det h'| = \int_A((f \circ h) \circ g)|\det h' \circ g||\det g'| = \int_Af \circ (h \circ g)|\det (h \circ g)'|
  \end{equation}
  by the Chain Rule.
  \label{proof:cov_p3}


  \item The theorem is true if \(g\) is an (invertible) linear transformation. We already showed that, for any \(g\), it's good enough to prove that the theorem is true on an open rectangle \(U\) with the constant function \(f = 1\), i.e. that if \(U\) is an open rectangle in \(A\), then
  \begin{equation}
    \int_{g(U)}1 = \int_U|\det g'| \iff \Vol(g(U)) = |\det g|\Vol(U)
  \end{equation}
  Note that we've already proved this in linear algebra, using the volume of parallelepipeds. But let's do it analytically.

  We will make use of the fact that any invertible linear transformation (i.e. invertible \(n \times n\) matrix) is a composite of elementary transformations (elementary matrices). Last time we showed that if the theorem is true for \(g, h\), then it is true for \(g \circ h\). Hence, all we need to do is show that the theorem is true for an arbitrary elementary transformation (matrix). We have 3 kinds of elementary matrices:
  \begin{itemize}

    \item A matrix which has all 1's on the diagonal, except for \(\lambda\) in one place. In this case, \(\det g = \lambda\), so we could use Fubini's theorem to scale by \(\lambda\), giving \(\Vol(g(U)) = \lambda \Vol(U) = |\det g|\Vol(U)\) as desired.

    \item A matrix which has all 1's on the diagonal, except for two adjacent rows which have \(\begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}\) on the diagonal. We have \(\det g = \pm 1\). Since we're just changing two coordinates, the integral remains unchanged.

    \item The identity matrix, plus \(1\) in a position \(i, j\) with \(i \neq j\). In this case, the determinant is just \(1\). Furthermore, we're just translating the top of our rectangle into a parallelogram. Fubini's theorem tells us that the volume is the same since the limits in terms of \(y\) are the same, and for fixed \(y\) we are integrating over a shifted integral of the same length, the shifting not having an effect since we are integrating a constant.

  \end{itemize}
  \label{proof:cov_p4}


\end{enumerate}
We can now return to the proof of the theorem: by \ref{proof:cov_p3} and \ref{proof:cov_p4}, we can assume that, given \(a \in A, g'(a) = I\), where \(I\) is the identity. Why? Let \(T = g'(a)\) be an invertible linear transformation. Then we can write
\begin{equation}
  g = T \circ (T^{-1} \circ g)
\end{equation}
By \ref{proof:cov_p4}, this is true for \(T\). By 3, if this is true for \(T\) and \(T^{-1} \circ g\) then it's true for \(g\). So it's enough to prove for the case
\begin{equation}
    (T^{-1} \circ g)' = I
\end{equation}
To complete the proof, we use induction on \(n\):
\begin{itemize}

  \item \(n = 1\): by integration by substitution

  \item Assume it's true for \(n - 1\). It's enought to show that, for all \(a \in A\), we can find Jordan measurable open \(U \subset A\) containing \(a\), with closure in \(A\), so that the theorem is true on \(U\) when \(f = 1\).

  We can assume \(g'(A) = I\). We'll express \(g\) as the composition of 2 mappings, each of which changes at most \(n - 1\) coordinates. There are a number of ways to do this. One is defining \(h : A \to \reals^n\) by
  \begin{equation}
    h(x) = (g_1(x),...,g_n(x), x_n)
  \end{equation}
  Why is this a change of variable? \(h'(a) = I\), so there is an open neighborhood \(U'\) of \(a\) on which \(h\) is one to one and \(\det h'(a) \neq 0\). Now define
  \begin{equation}
    k: h(U') \to \reals^n, k(y) = (y_1,...,y_{n - 1}, g_n(h^{-1}(y_n)))
  \end{equation}
  We have \(g = k \circ h\),
  \begin{equation}
    g'(a) = k'(h(a))h'(a) = I \implies k'(h(a)) = I
  \end{equation}
  So there is an open neighborhood \(V\) of \(h(a)\) in \(U'\) on which \(k\) is one to one and
  \begin{equation}
    \det k'(g) = 0
  \end{equation}
  Let \(U = h^{-1}(V)\). On \(U\), \(g = k \circ h\), and \(h: U \to V\), \(k: V \to \reals^n\).

  So we have to prove the theorem for \(h\) and for \(k\). The proof is going to be the same for both cases, so let's do it for the harder case, \(h\), which changes \(n - 1\) coordinates (versus \(k\), which changes only one).

  We're going to use Fubini's theorem: we'll integrate with respect to \(x_n\). We've previously found that it's enough to show that, for every \(a \in A\), there is a Jordan measurable open neighborhood \(U\) of \(a\) with closure \(A\) so that the result is true on \(U\) in the case where we are integrating the function \(f = 1\). Furthermore, we have shown that we can assume that \(g'(a) = I\). We also expressed \(g\) as a composite of two functions, each of which changes fewer than \(n\) variables. More precisely, we wrote
  \begin{equation}
    g = (x \mapsto  (g_1(x),...,g_{n - 1}(x), x_n)) \circ (y \mapsto (y_1,...,y_{n - 1},g_n(h^{-1}(y))))
  \end{equation}
  We will proceed by induction, using the single variable case as a base case.

  We can work with any Jordan-measurable open neighborhood of \(a\), so let's work with an open rectangle. Consider specifically an open rectangle \(W\) containing \(a\) with closure in \(A\). Since we're separating one coordinate from the other \(n - 1\) coordinates, let's write \(W\) as \(D \times (a_n, b_n)\) where \(D\) is an open rectangle in \(\reals^{n - 1}\). Note that the transformation \(h\) does not change the \(n^{th}\) coordinate of it's input. Using this, we can consider the following:
  \begin{equation}
    \int_{h(W)}1
  \end{equation}
  For a fixed \(x_n\), let's consider the function given by \(h\), depending only on \(1,...,n - 1\), written
  \begin{equation}
    h_{x_n} : D \to \reals^{n - 1},
    h_{x_n}(x_1,...,x_{n - 1})(g_1(x_1,...,x_n),...,g_{n - 1}(x_1,...,x_n))
  \end{equation}
  This function is one to one, since we know that \(h\) is one to one, and \(h\) is just \(h_{x_n}\) with \(x_n\) appended (for each \(x_n\)), so two different values at a given \(x_n\) have got to go into different points. On the other hand, what is the determinant
  \begin{equation}
    (\det h'_{x_n})(x_1,...,x_{n - 1})
  \end{equation}
  Well, it's the same as the determinant of \(h'\), since the last row of \(h'\) is just the standard vector \(e_n\). So let's go back: first of all,
  \begin{equation}
    h(D \times \{x_n\}) = h_{x_n}(D) \times \{x_n\}
  \end{equation}
  since, like we said before, the \(n^{th}\) coordinate is unaffected. So we can proceed using Fubini's theorem,:
  \begin{equation}
    \int_{a_n}^{b_n}\left(\int_{h_{x_n}(D)}1\right)dx_n
  \end{equation}
  By induction, this is equal to
  \begin{equation}
    \int_{a_n}^{b_n}\int_D1|\det h'_{x_n}| = \int_{a_n}^{b_n}\int_D|\det h'| = \int_{D \times [a_n, b_n]}1 \cdot |\det h'| = \int_W 1 \cdot |\det h'|
  \end{equation}
  By strong induction, the theorem is also true for \(k\), giving the desired result.
\end{itemize}

Now that we finished this, what to do? We've been talking about the integral on \(\reals^n\). In the rest of the course, we're going to be talking about the integral but on a manifold.

\section{Manifolds}

\subsection{What is a manifold?}

\TODO{rewrite, copy}


Consider the following definitions of a manifold:
\begin{definition}
  A \(k\)-dimensional manifold manifold is a set \(M \subseteq \reals^n\) such that for all \(a \in M\), there is a \(\mc{C}^r\) mapping \(f: U \to \reals^{n - k}\) defined on an open neighborhood \(U\) of \(a\) such that \(M \cap U = f^{-1}(0)\). \(f\) has rank \(n - k\) at every \(x \in M \cap U\).
  \label{def:manifold1}
\end{definition}

\begin{definition}
  A \(k\)-dimensional manifold manifold is a set \(M \subseteq \reals^n\) such that every point of \(M\) has an open neighborhood \(U\) in \(\reals^n\) such that \(M \cap U\) is the graph of a \(\mc{C}^r\) function \(z = g(y)\), \(z = (z_1,...,z_{n - k}), y = (y_1,...,y_k)\) where
  \begin{equation}(y, z) \neq (x_{\sigma(1)},...,x_{\sigma(n)})\end{equation}
  where \(\sigma\) is a permutation of \(1,...,n\).
  \label{def:manifold2}
\end{definition}

\begin{definition}
  A \(k\)-dimensional manifold manifold is a set \(M \subseteq \reals^n\) such that for all \(a \in M\), there is an open neighborhood \(U\) of \(a\), an open set \(V \subset \reals^n\) and a \(\mc{C}^r\) \underline{diffeomorphism} (a \(\mc{C}^r\) mapping with a \(\mc{C}^r\) inverse) \(h: U \to V\) such that
  \begin{equation}h(M \cap U) = V \cap (\reals^k \times \{\mb{0} \in \reals^{n - k}\}) =
  \{y = (y_1,...,y_n) \in V : y_{k + 1} = ... = y_n\}\end{equation}
  \label{def:manifold3}
\end{definition}
\begin{claim}
  The definitions given for a manifold are equivalent
\end{claim}
\begin{proof}
\begin{itemize}

  \item \ref{def:manifold1} \(\implies\) \ref{def:manifold2}: By the implicit function theorem

  \item \ref{def:manifold2} \(\implies\) \ref{def:manifold1}: Let \(h(y, z) = z - g(y)\).

  \item \ref{def:manifold2} \(\implies\) \ref{def:manifold3}: Let \(h(y, z) = (y, z - g(y))\)

  \item \ref{def:manifold3} \(\implies\) \ref{def:manifold1}: Let \(f(x) = (h_{k + 1}(x),...,h_n(x))\)

\end{itemize}
\end{proof}

To illustrate the first definition in action, consider the example of the unit sphere \(S^{n - 1} \subset \reals^n\), given by
\begin{equation}S^{n - 1} = \{x = (x_1,...,x_n) : f(x) - 1 = 0)\} \text{ where } f(x) = x_1^2 + ... + x_n^2\end{equation}
We have
\begin{equation}\forall x \in S^{n - 1}, \grad f(x) = 2(x_1,...,x_n) = 2x \neq 0\end{equation}
so \(f\) has rank 1 at every point of \(S^{n - 1}\). Hence \(S^{n - 1}\) is a manifold.

\subsection{Tangent Spaces}

We now discuss the tangent space \(TM_a\) or \(M_a\) at a point \(a \in M\). If we have a function \(f: \reals^n \to \reals^p\), then \(Df(a)\) is a linear transformation, a function from \(\reals^n\) to \(\reals^p\). But we're going to consider it as a function from \(\reals^n_a\), the copy of \(\reals^n\) at \(a\), to \(\reals^p_{f(a)}\), the copy of \(\reals^p\) at \(f(a)\). To be clear, these copies of \(\reals^n, \reals^p\) are vector spaces we attach to the points \(a, f(a)\), with these points identified with the origin.

\begin{enumerate}

  \item The definition of the tangent space according to Definition \ref{def:manifold1} is \(\Ker Df(a) = Df(a)^{-1}(0)\), a vector subspace of \(\reals^n_a\)

  \item To think of the tangent space in terms of Definition \ref{def:manifold2}, write \(x = (y, z), a = (b, c)\). Then we can think of the tangent space as an affine space through the point \((b, c)\) by writing
  \begin{equation}z - c = Dg(b)(y - b)\end{equation}
  But we want to think of it as a vector space with \(a\) as the origin, a vector subspace of \(\reals^n_a\). So we should write it as
  \begin{equation}Z = Dg(b)(Y)\end{equation}
  where \(Z \in \reals_{f(a)}^p\), \(Y \in \reals_a^n\). This is because \(f(y, g(y)) = 0\), so
  \begin{equation}Df(a)\begin{pmatrix} I \\ Dg(b) \end{pmatrix} = 0 \implies \Ker Df(a) \supseteq \Ima\begin{pmatrix} I \\ Dg(b) \end{pmatrix}\end{equation}
  But in fact, they're equal, because their dimensions are the same.

  So the kernel of \(Df(a)\), and the image of \(\begin{pmatrix} I \\ Dg(b) \end{pmatrix}\), are both equal to the set of points such that
  \begin{equation}Z = Dg(b)(Y)\end{equation}

  \item In terms of the Definition \ref{def:manifold3}, we can write the tangent space as
  \begin{equation}Dh(a)^{-1}(\reals^k \times \{\mb{0} \in \reals^{n - k}\})\end{equation}

\end{enumerate}

\subsection{Definition by Coordinate Charts}

Let's try defining a (\(k\)-dimensional) manifold in another way: we want every point \(a \in M\) to have a ``neighborhood'' around it in \(M\) which ``looks like'' \(\reals^k\).

\begin{definition}[Coordinate Charts]
A \(k\)-dimensional manifold \(M \subset \reals^n\) is a set such that for all \(a \in M\) there is an open neighborhood \(U\) of \(a\) (in \(\reals^n\), \textbf{not} in \(M\)) and an open set \(W \subset \reals^k\) and a \(\mc{C}^r\) mapping \(\varphi: W \to \reals^n\) such that
\begin{itemize}
  \item \(\varphi\) is a bijection
  \item \(\varphi(W) = M \cap U\)
  \item \(\varphi\) has rank \(k\) at every point of \(W\), i.e. \(D\varphi(x)\) has rank \(k\) for every \(x \in W\).
  \item We'd like to say ``\(\varphi^{-1}: \varphi(W) = M \cap U \to W\)'' is continuous. But we don't know how to say it, because we haven't given \(M\) a topology. So we can't talk about a function defined on a piece of \(M\) being continuous. So the way we're going to say it is we're going to build the definition of the topology into the phrase that we'll say instead of this.

  We require instead that
  \((\varphi^{-1})^{-1}(\Omega)\), i.e. \(\varphi(\Omega)\), is \(\varphi(W)\) intersected with an open set of \(\reals^n\) for all \(\Omega\) open in \(W\).
\end{itemize}
\label{def:manifoldbycoords}
\end{definition}
Note this first and last condition on \(\varphi\) are equivalent to requiring that \(\varphi\) is a homeomorphism between the neighborhood \(M \cap U\) and \(W\).

We'll now show that Definition \ref{def:manifoldbycoords} implies Definition \ref{def:manifold3}:
\begin{proof}
  Say \(a = \varphi(b)\) for some \(b \in W\). We can assume
  \begin{equation}\prt{(\varphi_1,...,\varphi_k)}{(y_1,...,y_k)}\end{equation}
  has rank \(k\) on \(W\).
  Define \(\psi: W \times \reals^{n - k} \to\reals^n\) by
  \begin{equation}(y, z) \mapsto \varphi(y) + (0, z)\end{equation}
  Then we get the block matrix
  \begin{equation}\psi'(y, z) = \begin{pmatrix}
    \prt{(\varphi_1,...,\varphi_k)}{(y_1,...,y_k)} & 0 \\
    * & I
  \end{pmatrix}\end{equation}
  This shows that \(\psi\) has rank \(n\) for all \(y \in W\), since it's determinant is nonzero. But that means that we can apply the inverse function theorem. So by the inverse function theorem, there are open neighborhoods \(V_1'\) of \((b, 0)\) and \(U_1'\) of \(\psi(b, 0) = \varphi(b) = a\) such that \(\psi: V_1' \to U_1'\) has a \(\mc{C}^r\) inverse \(\psi^{-1}: U_1' \to V_1'\).

  We have that
  \begin{equation}\psi^{-1}(\varphi(y)) = (y, 0) \in V_1' = \varphi(W) \cap \widetilde U\end{equation}
  where \(U\) is open in \(\reals^n\). Take \(U_1 = U_1' \cap \widetilde U\) and \(V_1 = \psi^{-1}(U_1)\). We have
  \begin{equation}M \cap U_1 = \{\varphi(y) : (y, 0) \in V_1\}\end{equation}
  So
  \begin{equation}h = \psi^{-1}|_{U_1}\end{equation}
  satisfies the conditions implied by (3) since
  \begin{equation}h(M \cap U_1) = \psi^{-1}(M \cap U_1) = \{(y, 0) : (y, 0) \in V_1\} = V_1 \cap (\reals^k \times \{\mb{0}\})\end{equation}
\end{proof}
This is quite a delicate topological argument, and gi\TODO{finish this sentence}

We begin by looking at the following two definitions for the tangent space to a manifold in terms of definitions \ref{def:manifold3} and \ref{def:manifoldbycoords} given for a manifold in the previous lecture
\begin{definition}
  \(TMa = Dh(a)^{-1}(\reals^k \times \{\mb{0}\})\)
\end{definition}
\begin{definition}
  \label{vfdef} \(TMa = D\varphi(b)(\reals^k_b)\)
\end{definition}
We're forgetting about the first two definitions, but why are \textit{these} two definitions equivalent? We used \(h\) to find a coordinate map \(\varphi\) by taking \(\varphi\) to be \(h^{-1}\) restricted to \(\reals^k \times \{\mb{0}\}\). Note that definition \ref{vfdef} is independent from the choice of \(\varphi\). To understand why, consider two maps \(\varphi_1(b_1) = a = \varphi_2(b_2)\) which take some interesecting open subsets \(M_1, M_2\) of \(M\) to spaces \(W_1, W_2\). Then
\begin{equation}\varphi_1 = \varphi_2 \circ (\varphi_2^{-1} \circ \varphi_1)\end{equation}
on \(M_1 \cap M_2\), giving
\begin{equation}D\varphi_1(b_1) = D\varphi_2(b_2) \circ D(\varphi_2^{-1}\circ\varphi_1)(b_1)\end{equation}
I want to think of this in a different way now. I want to give another definition for the tangent space that doesn't depend on \(h\), doesn't depend on \(\varphi\), doesn't depend on any data other than \(M\). We give the following:
\begin{definition}
  \begin{equation}TM_a = \{\text{tangent vectors } \gamma'(0) \text{ of } \mc{C^1} \text{ curves } \gamma: (-\delta, \delta) \to M \text{ s.t. } \gamma(0) = a\}\end{equation}
\end{definition}
So what does this even mean? Recall the following definition:
\begin{definition}
  A \(\mc{C}^1\) curve in \(\reals^n\) is a \(\mc{C}^1\) mapping, for some open interval \((c, d)\),
  \begin{equation}\gamma: (c, d) \to \reals^n, \gamma(t) = (\gamma_1(t),...,\gamma_n(t))\end{equation}
  \label{newdef}
\end{definition}
If we fix a point on \(\gamma\), what's the tangent vector to \(\gamma\) at \(\gamma(t_0)\)? It's \(\gamma'(t_0) \in T\reals^n_{\gamma(t_0)}\). We now generalize the above definition
\begin{definition}
  Let \(M \subset \reals^n\) be a \(\mc{C}^r\) submanifold of dimension \(k\). We say \(\gamma\) is a \(\mc{C}^1\) curve in \(M\) if \(\gamma((a, b)) \subset M\) for some open inteval \((a, b)\). In this case \(\gamma'(t_0) \in TM_{\gamma(t_0)}\).
  \label{c1mcurve}
\end{definition}
So why is \(\gamma'(t_0)\) an element of \(TM_{\gamma(t_0)}\)? In terms of a previous definition,
\begin{equation}TMa = \Ker Df(a)\end{equation}
In our case, we have
\begin{equation}f(\gamma(t)) = 0 \implies Df(\gamma(t_0)) \cdot \gamma'(t_0) = 0 \implies \gamma'(t_0) \in \Ker Df(a) = TMa\end{equation}
So, considering this, why is definition \ref{newdef} the tangent space? Let's think about that, first of all, in a simple special case. What if \(M = \reals^k\)? Take \(a \in \reals^k\). We are simply asking that every vector \(v \in \reals_a^k\) through the point \(a\) is the tangent vector to some curve through \(a\), i.e. is \(\gamma'(0)\) where \(\gamma(t) = a + tv\). So if \(M = \reals^k\), for sure that's the right definition of the tangent space.

So now why is this true in general? Which definition of the tangent space do you think we should use? Well, the coordinate chart definition, i.e. definition \ref{vfdef}, is the one that best fits our purposes. In general, consider a coordinate chart \(\varphi\) for \(M\) at \(a\). Say we have a \(\mc{C}^1\) curve in \(W\) such that \(\delta(0) = b\). Then
\(\gamma(t) = (\varphi \circ \delta)(t)\)
is a \(\mc{C}^1\) curve in \(M\) such that \(\gamma(0) = a\). So using the Chain Rule,
\begin{equation}\gamma'(0) = D\varphi(b) \circ \delta'(0)\end{equation}
But any \(\mc{C}^1\) curve \(\gamma(t)\) in \(M\) such that \(\gamma(0) = b\) is of the form
\(\gamma = \varphi \circ \delta\)
where \(\delta\) is a \(\mc{C}^1\) curve in \(W\). We get this from the description of \(\varphi^{-1}\) as restricted to \(M\) of a \(\mc{C}^r\) function \(h\).

What's nice about definition \ref{newdef} is that it depends only on \(M\), not on any external information. So now we know what manifolds and tangent spaces are.

This course is somewhat like first year calculus, in that first you do differentiation and then you do integration, and then you put them together with the fundamental theorem of calculus and examine all the interesting consequences. And we are not at the end of quite an important section of the course about the implict function theorem. Soon, we'll stop and begin learning about integration, and then later return and put the two together to obtain some interesting results, including Stokes theorem.

But we have one thing left to do. Usually, in mathematics, we don't just define spaces, we also define the functions between them. And we should do that for manifolds: if we have a mapping between two manifolds, what does it mean for this mapping to be continuous? Furthermore, we will introduce the slightly more general concept of a manifold with boundary.

\subsection{Functions Between Manifolds}

\TODO{this}

\subsection{Manifolds with Boundary}


Recall the following definitions:
\begin{definition}
  A half space \(\hlfspc^k \subset \reals^k\) is given by
  \begin{equation}\{x = (x_1,...,x_k) : x_k \geq 0\}\end{equation}
\end{definition}
\begin{definition}
  A subset \(M \subseteq \reals^n\) is a \(k\)-dimensional \(\mc{C}^r\) \underline{manifold with boundary} if for every \(a \in M\), there is an open neighborhood \(U\) of \(a\) in \(\reals^n\), an open subset \(V\) of \(\reals^n\) and a \(\mc{C}^r\) diffeomorphism \(h: U \to V\) such that either
  \begin{enumerate}[label=(\arabic*)]

    \item The usual condition for a manifold,
    \begin{equation}h(M \cap U) = V \cap (\reals^k \times \{\mb{0} \in \reals^{n - k}\})\end{equation}
    \label{mnfcnd}

    \item The manifold looks like a half space around \(a\), i.e.
    \begin{equation}h(M \cap U) = V \cap (\hlfspc^k \times \{\mb{0} \in \reals^{n - k}\}) = \{y = (y_1,...,y_n) : y_1,...,y_k \geq 0 \land y_{k + 1} = ... = y_n = 0\}\end{equation} \label{bndcnd}

  \end{enumerate}
  \label{mwb}
\end{definition}
One thing you could ask is whether both conditions \ref{mnfcnd}, \ref{bndcnd} could be satisfied by the same point \(a\). The answer is no. To show why, suppose \(h_1: U_1 \to V_1\) satisfies \ref{mnfcnd} and \(h_2: U_2 \to V_2\) satisfies \ref{bndcnd} for some \(a\). How can we get a contradiction? If we take
\begin{equation}h_2 \circ h_1^{-1}: V_1 \to V_2\end{equation}
we get a \(\mc{C}^r\) diffeomorphism in an open set containing \(a\) taking an open subset of \(\reals^k\) onto an open subset of \(\hlfspc^k\) which is \textit{not} open in \(\reals^k\). What we're contradicting here is the inverse function theorem, which says there would be open neighborhoods \(h_1(a)\) and \(h_2(a)\) which are mapped together with an inverse. Alternatively, we could use the topological arguments that diffeomorphisms, which are homeomorphisms, are open maps.

Once we've established this result, we can really distinguish these two kinds of points. That means, precisely, that we can make the following definitions
\begin{definition}
  The \underline{boundary} of \(M\), written \(\partial M\), is the set of points satisfying condition \ref{bndcnd} in definition \ref{mwb}
\end{definition}

We're going to be interested in manifolds because they generalize the idea of intervals with endpoints in the real line. In fact, an interval with endpoints is a one dimensional manifold with boundary.


\subsection{Multilinear Algebra}

\TODO{this}

\subsection{Vector Fields and Differential Forms}


Recall the definitions of the \underline{tangent space to \(\reals^n\) at \(a\)}, \(\text{T}\reals^n_a\) or \(\reals^n_a\). We write the standard bases of \(\reals^n_a\) as
\begin{equation}
  e_{1,a},...,e_{n,a}
\end{equation}
and define the stadard inner product
\begin{equation}
  \ip{v_a}{w_a}_a = \ip{v}{w}
\end{equation}
Using the definitions from last time, we obtain a \underline{standard orientation} of \(\reals^n_a\)
\begin{equation}
  [e_{1,a},...,e_{n,a}]
\end{equation}
Recall the following definition:
\begin{definition}
  The \underline{tangent vector} to a \(\mc{C}^1\) curve \(\gamma: [0, 1] \to \reals^n\) at \(t \in [0, 1]\) is given by
  \begin{equation}
    \gamma'(t)_{\gamma(t)} = (\gamma_1'(t),...,\gamma_n'(t))_{\gamma(t)} = \sum_{i = 1}^n\gamma_i'(t)e_{i, \gamma(t)} \in \reals^n_{\gamma(t)}
  \end{equation}
  We write
  \begin{equation}
    \gamma'(t)_{\gamma(t)} = \gamma_{*t}(e_{1, t})
  \end{equation}
  where \(e_{1, t} \in \reals^1_t\), thinking of the derivative of \(\gamma\) \(\D\gamma\) inducing a mapping
  \begin{equation}
    \gamma_{*t}: \reals_t^1 \to \reals^n_{\gamma(t)}, \gamma_{*t}(ae_{1, t}) = \D\gamma(a + t)(e_1)_{\gamma(t)}
  \end{equation}
\end{definition}
Consider now a function \(\varphi : \reals^n \to \reals^p\), and consider the curve \((\varphi \circ \gamma): [0, 1] \to \reals^p\). What's the tangent vector to \(\varphi \circ \gamma\) at \(t\)? Well, by definition, it's
\begin{equation}
  (\varphi \circ \gamma)_{*t}(e_{1, t}) = \D(\varphi \circ \gamma)(t)(e_1)_{\varphi(\gamma(t))} \label{starphi}
\end{equation}
By the Chain Rule, \ref{starphi} simplifies to
\begin{equation}
  \D\varphi(\gamma(t)) \circ \D\gamma(t) (e_1)_{\varphi(\gamma(t))} = \varphi_{*\gamma(t)}(\D\gamma(t)(e_1)_{\gamma(t)}) = \varphi_{*\gamma(t)} \circ \gamma_{*t}(e_{1, t})
\end{equation}
Writing this in words, the tangent vector to \(\varphi \circ \gamma\) at \(t\) is equal to the linear mapping between tangent spaces \(\varphi_{*\gamma(t)}: \reals^n_{\gamma(t)} \to \reals^p_{\varphi(\gamma(t))}\) applied to the tangent vector to \(\gamma\) at \(t\).

We're now going to consider functions which, for every point in \(\reals^n\), give us a vector in the tangent space at that point. And that's what's called a \textit{vector field}. Formally,
\begin{definition}
  A \underline{vector field} \(F\) on \(\reals^n\) is a function such that for every point \(a\), \(F(a) \in \reals^n_a\), i.e.
  \begin{equation}
    F(a) = (F_1(a),...,F_n(a))_a
  \end{equation}
\end{definition}
This trivially gives the following definition
\begin{definition}
  A vector field \(F\) is \(\mc{C}^r\) if each component \(F_i\) is \(\mc{C}^r\).
\end{definition}
Now, why is it that we really want to think of these things as having values in different tangent spaces? Let's look at some examples:
\begin{enumerate}

  \item In the plane, let's look at \(F(x, y) = (x, y)_{(x, y)}\), i.e. the vector \((x, y)\) ``pointing out of'' \(x, y\). In terms of the basis,
  \begin{equation}
    xe_{1, (x, y)} + ye_{2, (x, y)}
  \end{equation}
  We can imagine plotting this with every point having, coming out of it, the line between it and the origin, with a ``source'' at the origin. Imagining these to be the velocity vectors of a particle at each point, the points accelerate outwards from the origin with increasing rapidity.

  \item Consider now \(F(x, y) = (x, -y)\). If we trace a particle following the velocity vectors, this gives us a ``saddle'' shape.

  \item Consider the following important example
  \begin{equation}
    F(x) = \grad f(x) = \left(\prt{f}{x_1}(x),...,\prt{f}{x_n}\right)_x
  \end{equation}
  \(F\) is \(\mc{C}^r\) if \(f\) is \(\mc{C}^{r + 1}\). Finding if such an \(f\) exists is an interesting problem in differential equations. We can think of it as finding a potential function whose gradient at each point is equal to the vector field.

\end{enumerate}
We can apply operations on vectors pointwise to vector fields:
\begin{equation}
  (F + G)(x) = F(x) + G(x)
\end{equation}
\begin{equation}
  \ip{F}{G}(x) = \ip{F(x)}{G(x)}
\end{equation}
\begin{equation}
  (f \cdot F)(x) = f(x)F(x)
\end{equation}
In \(\reals^n\) we can take the cross product
\begin{equation}
  (F_1 \times ... \times F_{n - 1})(x) = F_1(x) \times ... \times F_{n - 1}(x)
\end{equation}
There are several very important classical operations we're going to be interested in, chief of them being \(\grad\), \(\Div\) and \(\curl\).
\begin{definition}
 We define the \underline{divergence of \(F\)}, \(\Div F\), as
 \begin{equation}
  \Div F = \sum_{i = 1}^n\D_iF_i = \sum_{i = 1}^n\prt{F_i}{x_i}
 \end{equation}
 This is often considered to be the ``inner product'' of \(F\) with a certain \underline{operator},
 \begin{equation}
  \ip{\nabla}{F}
 \end{equation}
 where
 \begin{equation}
  \nabla = \sum\prt{}{x_i}e_i
 \end{equation}
\end{definition}
\begin{definition}
  We define the \underline{curl of \(F\)} (in \(\reals^3\)), \(\curl F\), as
  \begin{equation}
    \curl F = \nabla \times F = (\D_2F_3 - \D_3F_2)e_1 + (\D_3F_1 - \D_1F_2)e_2 + (\D_1F_2 - \D_2F_1)e_3
  \end{equation}
\end{definition}
We're going to prove a very general version of Stokes' theorem, but we want to see what holds in these special cases. Maybe that's enough for today

\subsection{The Differential Operator}

\TODO{this}

\subsection{Pullbacks and Pushforwards}


Let's begin with a \(\mc{C}^r\) mapping \(f: \reals^n \to \reals^p\). Well first of all, we know what the tangent mapping is, or rather we know what the derivative is. And we want to interpret this derivative as a linear mapping induced by \(f\)
\begin{equation}
  f_{*a}: \reals^n_a \to \reals^p_{f(a)}, f_{*a}(v_a) = Df(a)(v)_{f(a)}
\end{equation}
Now, by duality, we want to say that \(f_{\*a}\) induces in the other direction a mapping that we'll call \(f^*_a\) that takes alternating \(k\)-tensors on the tangent space \(\reals^p_{f(a)}\) to alternating \(k\)-tensors on the tangent space \(\reals^n_a\), i.e.
\begin{equation}
  f_a^*: \Omega^k(\reals^p_{f(a)}) \to \Omega^k(\reals^n_a)
\end{equation}
Let's try the case where \(k = 1\). In this case, alternating 1-tensors are just 1-tensors, which are just elements of the dual space, i.e.
\begin{equation}
  \Omega^1(\reals^n_a) = \mc{T}^1(\reals^n_a) = (\reals^n_a)^*
\end{equation}
So, if \(T \in (\reals^p_{f(a)})^*\), we can write
\begin{equation}
  f_a^*(T)(v_a) = T(f_{*a}(v_a))
\end{equation}
In general, we do the exact same thing for \(\omega \in \Omega^k(\reals^p_{f(a)})\):
\begin{equation}
  f_a^*(\omega)(v_{1,a},...,v_{k,a}) = \omega(f_{*a}(v_1),...,f_{*a}(v_k))
\end{equation}
Now, we want to show that \(f\) induces a mapping \(f^*\) inducing a linear mapping whih takes, instead of pointwise objects as above, \(k\)-forms on \(\reals^p\) to \(k\)-forms on \(\reals^n\), with a \(k\)-form on \(\reals^p\) taking the form
\begin{equation}
  \omega: b \in \reals^p \mapsto \omega(b) \in \Omega^k(\reals^p_b)
\end{equation}
So we can write
\begin{equation}
  (f^*\omega)(a) = f^*_a(\omega(f(a)))
  \label{novs}
\end{equation}
which is the same as saying
\begin{equation}
  (f^*\omega)(a)(v_{1, a},...,v_{k, a}) = \omega(f(a))(f_{*a}(v_{1,a}),...,f_{*a}(v_{n,a}))
\end{equation}
that is, just repeating what the definition is about. So this is the formal definition. I understand that everyone would like to forget this as soon as possible, so I'm going to try to help.
\begin{proposition}
  Let \(f: \reals^n \to \reals^p\) be \(\mc{C}^r\). Then
  \begin{equation}
    f^*(dy_i) = \sum_{j = 1}^n\prt{f_i}{x_j}dx_j = df_i = d(y_i \circ f)
  \end{equation}
  where
  \begin{equation}
    dy_i = \sum\prt{y_i}{x_j}dx_j
  \end{equation}
  Recall also that
  \begin{equation}
    f_*\left(\prt{}{x_j}\right) = \sum\prt{f_j}{x_i}\prt{}{y_j}
  \end{equation}
\end{proposition}
Before we prove this, recall that equation \ref{novs} is true for every \(k\), including \(k = 0\). And what's a 0-form? It's just a function. So in this case, with a function (zero form) \(g\),
\begin{equation}
  f^*(g) = g \circ f
\end{equation}
Let's get to the proof:
\begin{proof}
  By definition,
  \begin{equation}
    f^*(dy_i)(a)(v_a) = dy_i(f(a))(f_{*a}v_a)
    \label{mprod}
  \end{equation}
  Now, what is the right hand side here? It's the matrix of partial derivatives of \(f(a)\) applied to \(v_a\). So if \(v_a = (v_1,...,v_n)\), then equation \ref{mprod} is equal to
  \begin{equation}
    dy_i(f(a))\left(\sum_{k = 1}^p\mb{e}_{k, f(a)}
      \sum_{j = 1}^n\prt{f_k}{x_j}(a)v_j\right)
      = \sum_{j = 1}^n\prt{f_i}{x_j}(a)v_j
      = \sum_{j = 1}^n\prt{f_i}{x_j}(a)dx_j(a)v_a
  \end{equation}
  Since this is true for any \(v_a\), it follows that
  \begin{equation}
    f^*(dy_i) = \sum_{j = 1}^n\prt{f_i}{x_j}dx_j
  \end{equation}
  as desired.
\end{proof}
Let's do an example: assume \(gx\) is a \(\mc{C}^r\) function on \(\reals^n\):
\begin{equation}
  f^*(dg) = f^*\left(\sum_{i = 1}\prt{g}{y_i}dy_i\right) = \sum_{i = 1}^p\prt{g}{y_i} \circ ff^*(dy_i) = \sum_{i = 1}^n\prt{g}{y_i} \circ f\sum_{j = 1}^n\prt{f}{x_j}dx_j
\end{equation}
Changing the order of the summation and using the chain rule, we obtain
\begin{equation}
  \sum_{j = 1}^n\left(\sum_{i = 1}^n\prt{g}{y_i}\circ f \circ \prt{f_i}{x_j}\right)dx_j = \sum_{j = 1}^n\prt{(g \circ f)}{x_j}dx_j = d(g \circ f)
\end{equation}
Let's extend the proposition with some ``trivial'' facts:
\begin{proposition}
  Let \(f: \reals^n \to \reals^p\) be \(\mc{C}^r\). Then
  \begin{enumerate}

    \item \(f^*(\omega_1 + \omega_2) = f^*(\omega_1) + f^*(\omega_2)\), i.e. \(f^*\) is linear

    \item \(f^*(g \cdot w) = (g \circ f)f^*\omega\)

    \item \(f^*(\omega \wedge \eta) = f^*(\omega) \wedge f^*(\eta)\)

  \end{enumerate}
\end{proposition}
\begin{proof}
  Immediate from the definitions
\end{proof}
\begin{proposition}
  Suppose \(f: \reals^n \to \reals^p\). Then
  \begin{equation}
    f^*(gdx_1 \wedge ... \wedge dx_n) = g \circ f\det g' \cdot dx_1 \wedge ... \wedge dx_n
  \end{equation}
\end{proposition}
This is why the natural object of integration is not functions but rather differential forms. Because if we understand that the object we're integrating is a differential form, not a function, then the formula for change of variable is built into the definition of a differential form.
\begin{equation}
  \int gdx_1 \wedge ... \wedge dx_n = \int gdx_1...dx_n
\end{equation}
Let's get to the proof:
\begin{proof}
  Consider
  \begin{equation}
    f^*(dx_1 \wedge ... \wedge dx_n)(a)(e_{1,a},...,e_{n,a})
  \end{equation}
\end{proof}

\section{Integration on Manifolds}

We haven't talked about manifolds for a while, but I hope you can remember how to imagine one because I'm too lazy to {\LaTeX} one. So what's the point of integrating on our imaginary manifold picture? We know how to integrate on a line, and how to integrate now in \(\reals^n\), and in both cases we're integrating with some notion of \(n\)-dimensional volume. But now, on our \(k\)-dimensional manifold, we're going to have to try to figure out how to integrate using a \(k\)-dimensional measure of volume on it, which is nontrivial.

Another reason is the following: if we go back to one dimensional calculus, and we want to compue the integral of some function \(f\) from \(a\) to \(b\), the very important and aptly named Fundamental Theorem of Calculus tells us that if \(f\) has a primitive \(g\) it's integral is very easy to compute:
\begin{equation}
  \int_a^bf = g(b) - g(a)
\end{equation}
We want to try to do the same thing with a manifold: generalize the fundamental theory of calculus to relate the integral of a function on the manifold to the integral of a function on the boundary. But what sense does that make? A function on a manifold, unlike that on a line, depends on many variables. So we don't have a notion of primitive like in single variable calculus, since an \(n\) dimensional function has many partial derivatives.

So those are the two things we're going to have to understand to get started:
\begin{itemize}
  \item What is this \(k\) dimensional measure of volume?
  \item What is the appropriate notion of a primitive?
\end{itemize}
But before that, we're going to do a bit of motivation, and consider the case of a one-dimensional manifold, or as you know it, a curve:

\subsection{Integration of Parametrized Curves}

We now consider the case of a parametrized Curve \(C\) in \(\reals^n\). Let's start from the beginning: what is a parametrized curve? It means \(C\) is the image of a mapping
\begin{equation}
  \gamma : [a, b] \to \reals^n
\end{equation}
We could write, for example, \(x = \gamma(t)\). We're going to assume that our curves define something almost like a manifold, except we're going to allow that the curves cross each other. So assume that \(\gamma\) is \underline{piecewise \(\mc{C}^1\)}. What this means is, except for some finite number of points (since we're working on a compact interval), it's \(\mc{C}^1\). Furthermore, we assume that except for isolated points, \(\gamma' \neq 0\) and \(\gamma\) is one to one.
We don't want to allow, for example,
\begin{equation}
  \gamma(t): [0, 3\pi] \to \reals^2 = t \mapsto (\cos t, \sin t)
\end{equation}
since we don't want a whole big region (in this case a semicircle) where \(\gamma\) overlaps itself.

So the first item of business is: what's the measure of volume on such a curve? Length, of course. Arc length, specifically. We have that
\begin{equation}
  \ell(C) = \int_a^b|\gamma'(t)|dt
\end{equation}
We're not assuming that people have seen this before, so the question is, why? Let's deal with length in a more concrete, geometric way. How can you do that? In the same way that you define the integral: you try to approximate by something that is easy to compute. So one good way to approximate length is to divide up the integral into a partition and look at the corresponding line segments: we define length as the supremum of the length of polygonal approximations to the curve.

Let's let \(P = \{t_0,t_1,...,t_k\}\) be a partition of \([a, b]\), and let \(\ell_P(C)\) be the length of the associated polygonal approximation:
\begin{equation}
  \ell_p(C) = \sum_{j = 1}^k|\gamma(t_j) - \gamma(t_{j - 1})|
\end{equation}
We'll say that the curve has length if the supremum of these things exists, and we'll define it as the length. Formally,
\begin{definition}
  The curve \(C\) is \underline{rectifiable} if
  \begin{equation}
    \sup_P\ell_P(C)
    \label{lensupremum}
  \end{equation}
  exists, in which case we define \ref{lensupremum} it to be the \underline{length} of \(C\), written \(\ell_P(C)\).
\end{definition}
Here's a brief homework exercise: prove that if \(\gamma\) is piecewise \(\mc{C}^1\), then \(C\) is rectifiable and \(\ell(C)\) is given by equation \ref{lensupremum}. This is a first year calculus exercise using Riemann sums (I think it's in Spivak), so it shouldn't be too hard.



\subsection{Integral of a \(k\)-form over a \(k\)-cube}

Talking about Stoke's theorem, what we're really interested in is not Stoke's theorem on \(k\)-cubes or on chains but on \textit{manifolds}, where we're going to be comparing the integral on a manifold to the integral on it's boundary. To do that, we're going to have to adapt all our machinery for differential forms to work with manifolds. But first, let's cover an important point I left off last time.

\subsubsection{Independence of Parametrization}

Let's say \(c\) is a \(\mc{C}^1\) singular \(k\)-cube in \(\reals^n\), and we want to look at a re-parametrization, i.e. a map
\begin{equation}
  p : [0, 1]^k \to [0, 1]^k
\end{equation}
which is \(\mc{C}^1\), one-to-one, onto, and \(\det p'(x) \geq 0\), i.e. the Jacobian does not reverse orientations.
If \(\omega\) is a \(k\)-form then
\begin{equation}
  \int_c\omega = \int_{c \circ p}\omega
\end{equation}
You should compare this with the situation with functions: what if we were integrating a function over \(p([0, 1]^k) = [0, 1]^k\). You'd have to say there's a change of variable formula
\begin{equation}
  \int_{[0, 1]^k}f \circ p|\det p'| = \int_{p([0, 1]^k)}f
\end{equation}
This is clearly a very different case, and indeed why differential forms are the ``natural'' thing to integrate over: the forumla for change of variables is ``built in'' to the definition. Let's see how:
\begin{proof}
  By definition,
  \begin{equation}
    \int_{c \circ p}\omega = \int_{[0, 1]^k}(c \circ p)^*\omega = \int_{[0, 1]^k}p^*(c^*\omega)
    \label{intdef}
  \end{equation}
  \(c^*\omega\) is a \(k\)-form on \([0, 1]^k\), so we know it looks like
  \begin{equation}
    c^*\omega = f(x)\bigwedge_{i = 1}^kdx_i \implies p^*(c^*\omega) = (f \circ p)\det p' \bigwedge_{i = 1}^k dx_i
  \end{equation}
  So we can rewrite equation \ref{intdef} as
  \begin{equation}
    \int_{[0, 1]^k}(f \circ p)\det p'dx_1 \wedge ... dx_k = \int_{[0, 1]^k}(f \circ p)\det p'
  \end{equation}
  Since \(\det p' \geq 0\), the above becomes
  \begin{equation}
    \int_{[0, 1]^k}(f \circ p)|\det p'| = \int_{p([0, 1]^k)}f = \int_{[0, 1]^k}fdx_1 \wedge ... \wedge dx_k = \int_{[0, 1]^k}c^*\omega = \int_c\omega
  \end{equation}

\end{proof}

\subsection{Integration of Differential Forms on Manifolds}

\subsubsection{Vector Fields and Differential Forms on a submanifold \(M\) of \(\reals^n\)}

We haven't talked about manifolds in a while, since differential calculus, and now we want to talk about them in terms of integral calculus. So let's start off with the definitions:
\begin{definition}
  A \underline{vector field \(F\) on \(M\)} is a function \(F\) defined on \(M\) such that
  \begin{equation}
    \forall x \in M, F(x) \in TM_x
  \end{equation}
\end{definition}
Recall the definition of a \(\mc{C}^r\) coordinate chart \(\varphi: W \subset \reals^k \to M \subset \reals^n\), where \(M\) has dimension \(k\). The derivative gives an isomorphism of vector spaces
\begin{equation}
  \varphi_{*a} : \reals_a^k \to TM_{\varphi(a)}
\end{equation}
If \(\varphi\) is a coordinate chart for \(M\) then there is a vector field \(G\) on \(W\) such that
\begin{equation}
  \varphi_{*a}(G(a)) = F(\varphi(a))
\end{equation}
With this in mind, how should we say what it means for \(F\) to be \(\mc{C}^r\)? We know what it means for \(G\) to be \(\mc{C}^r\), so we'll say that \(F\) is \(\mc{C}^r\) if \(G\) is \(\mc{C}^r\) for any coordinate chart \(\varphi: W \to M\). Note that this definition is independent of the choice of coordinate chart: suppose we have two \(\mc{C}^r\) coordinate charts \(\varphi: W \to M\), \(\psi : V \to M\) such that
\begin{equation}
  \varphi(a) = \psi(b)
\end{equation}
We can say that there are vector fields \(G\) on \(W\) and \(H\) on \(V\) such that
\begin{equation}
  \varphi_{*\xi}(G(\xi)) = F(\varphi(\xi)), \psi_{*\eta}(H(\eta)) = F(\psi(\eta))
\end{equation}
If \(\varphi(a) = \psi(b)\) then we have that
\begin{equation}
  \varphi_{*a}(G(a) = F(\varphi(a))) = F(\psi(b)) = \psi_{*b}(H(b))
\end{equation}
So
\begin{equation}
  H(b) = \psi_{*b}^{-1}(F(\psi(b)))
\end{equation}
But \(\psi = \varphi \circ \varphi^{-1} \circ \psi\), giving
\begin{equation}
  \psi_{*b} = \varphi_{*a} \circ (\varphi^{-1} \circ \psi)_{*b}
\end{equation}
Hence,
\begin{equation}
  H(b) = \psi_{*b}^{-1}(F(\psi(b))) = ((\varphi^{-1} \circ \psi)_{*b})^{-1} \circ \varphi^{-1}_{*a}(F(\psi(b))) = (\psi^{-1} \circ \varphi)_{*a}(G(a))
\end{equation}
It's better not to try to write this down in complete detail (this was about the third attempt)...


Let's take off from last time: assume \(\omega\) is a \(p\)-form on a \(\mc{C}^{r+1}\) submanifold \(M\) of \(\reals^n\). That means that \(\omega\) maps each point \(a \in M\) to an alternating \(p\)-tensor \(\omega(a)\) on the tangent space to the manifold at \(a\), i.e.
\begin{equation}\omega : a \mapsto \omega(a) \in \Omega^p(M_a)\end{equation}
Consider two coordinate charts
\begin{equation}\varphi: W \to M, \psi: V \to M\end{equation}
We have, on the intersection \(\varphi(W) \cap \psi(V)\),
\begin{equation}\psi = \varphi \circ (\varphi^{-1} \circ \psi) \implies \psi^*\omega = (\varphi^{-1} \circ \psi)^*(\varphi^*\omega)\end{equation}
We used this to conclude that \(\varphi^*\) is \(\mc{C}^r\) if and only if \(\psi^*\) is \(\mc{C}^r\). So we say \(\omega\) is \(\mc{C}^r\) if \(\varphi^*\omega\) is \(\mc{C}^r\) for every coordinate chart \(\varphi\).
\begin{proposition}
  If \(\omega\) is a \(\mc{C}^r\) \(p\)-form on \(M\) then there is a unique \(\mc{C}^{r - 1}\) \((p + 1)\) form \(d\omega\) on \(M\) such that
  \begin{equation}
    \varphi^*(d\omega) = d(\varphi*\omega)
  \end{equation}
  for every coordinate chart \(\varphi\)
\end{proposition}
\begin{proof}
  Given \(\varphi\), we define \(d\omega\) on \(\varphi(W)\) by the formula
  \begin{equation}
    d\omega(a)(v_1,...,v_{p + 1}) = d(\varphi^*\omega)(x)(w_1,...,w_{p + 1})
    \label{dphi}
  \end{equation}
  where \(a = \varphi(x)\) and each \(w_i\) is the thing that the tangent mapping takes to \(v_i\), i.e.
  \begin{equation}
    v_i = \varphi_{*x}(w_i)
  \end{equation}
  For the skeptics among you, let's call equation \ref{dphi} \(d^\varphi\omega\), since it might depend on \(\varphi\). Last time, we observed that \(d^\varphi\omega\) is the unique \(p + 1\) form such that
  \begin{equation}
    \varphi^*(d^\varphi\omega) = d(\varphi^*\omega) \label{pullback}
  \end{equation}
  In the overlap of two charts \(\varphi: W \to \reals^n\), \(\psi: V \to \reals^n\), we have
  \begin{equation}
    \psi^*(d^\varphi\omega) = ((\varphi^{-1} \circ \psi)^* \circ \varphi^*)(d^\varphi\omega) = (\varphi^{-1} \circ \psi^{-1})^*d(\varphi^*\omega) = d((\varphi^{-1} \circ \psi)^* \circ \varphi^*\omega) = d(\psi^*\omega)
  \end{equation}
  But remember, \(d^\psi\omega\) was the unique thing satisfying equation \ref{pullback} substituting \(\psi\) for \(\varphi\). And hence we have that
  \begin{equation}
    d^\varphi\omega = d^\psi\omega
  \end{equation}
  So we can just call this \(d\omega\).
\end{proof}

\subsubsection{Orientiation of a sumbanifold \(M\) of \(\reals^n\)}
means there is a \underline{consistent} orientiation \(\mu_x\) on every tangent space \(M_x \subset \reals^k\). This means for every coordinate system \(\varphi: W \to \reals^n\), any two points \(a, b \in W\),
\begin{equation}
  [\varphi_{*a}(e_{1,a}),...,\varphi_{*a}(e_{k,a})] = \mu_{\varphi(a)} \iff [\varphi_{*b}(e_{1,b}),...,\varphi_{*b}(e_{k,b})] = \mu_{\varphi(b)}]
  \label{forevery}
\end{equation}
\begin{definition}
  An \underline{orientable manifold} is a manifold \(M\) for which it's possible to choose consistent orientations \(\mu_x\) at every point.
\end{definition}
Suppose the orientations \(\mu_x\) can be chosen consistently. Given any coordinate chart, we're going to call it either \underline{orientation preserving} or \underline{orientation reversing}. Specifically,
\begin{definition}
  A coordinate chart \(\varphi: W \to \reals^k\) is \underline{orientation preserving} if
  \begin{equation}
    [\varphi_{*a}(e_{1,a}),...,\varphi_{*a}(e_{k, a})] = \mu_{\varphi(a)}
  \end{equation}
  for one point \(a\) or every point \(a\) (these are equivalent by equation \ref{forevery}).
  If \(\varphi: W \to \reals^k\) is \underline{orientation reversing} (i.e. not orientation preserving), then if \(T: \reals^k \to \reals^k\) is a linear transformation with \(\det T < 0\), then \(\varphi \circ T\) is orientation preserving.
\end{definition}
So, every orientable manifold can be covered by orientation preserving coordinate charts (i.e. orientation preserving with respect to a consistent choice of orientations). Suppose now \(\varphi, \psi\) are two coordinate systems such that \(\varphi(a) = \psi(b)\) and
\begin{equation}
  [\varphi_{*a}(e_{1,a}),...,\varphi_{*b}(e_{k, a})] = \mu_x = [\psi_{*b}(e_{1,b}),...,\psi_{*b}(e_{k,b})]
\end{equation}
then \(\det(\varphi^{-1}, \psi)'(b) > 0\) (at every point in the overlap \(\varphi(W) \cap \psi(V)\)). We now obtain the following definitions:
\begin{definition}
If \(M\) is orientable, then a particular choice of consistent orientations \(\mu_x\) for \(x \in M\) is called an \underline{orientation of \(M\)}. An \underline{oriented manifold} \(M\) is a manifold together with an orientation.
\end{definition}
Let's now look at an example: the Möbius strip is \underline{not} orientable:s
\begin{proof} (Sketch): consider an orientation at one point along the strip. Move it along the strip until it returns back to where it started. Now it's reversed, so we have an inconsistency.
\end{proof}
Obviously, this is not very rigorous, but we'll look at another approach next time.


\subsection{Manifolds with Boundary}

\TODO{this}

\subsection{Stoke's Theorem on Manifolds}


We now want to try to prove the general form of Stokes' theorem:
\begin{theorem}[Stokes' Theorem]

  Let \(M\) be a compact oriented \(k\)-dimensional manifold with boundary (which is at least
  \(\mc{C}^2\)) and \(\omega\) be a \((k - 1)\)-form on \(M\) (which is at least \(\mc{C}^1\)). Then
  \begin{equation}
    \int_Md\omega = \int_{\partial M}\omega
    \label{stokes}
  \end{equation}
  where \(\partial M\) has induced orientation
\end{theorem}
The problem is, the integrals in equation \ref{stokes} are yet to be defined. So that's what we're going to be working up to today.
Consider a singular \(p\)-cube \(c: [0, 1]^k \to M\) in \(M\). If \(\omega\) is a \(p\)-form on \(M\), then we define
\begin{equation}
  \int_c\omega = \int_{[0, 1]^p}c^*\omega
\end{equation}
Integrals over \(p\)-chains are defined as before. In the case that \(p = k\) (e.g. in theorem) we'll assume that our \(k\)-cubes \(c:[0, 1]^k \to M\) satisfy the following condition: there is a coordinate chart \(\xi: W \to M\) such that \([0, 1]^k \subset W\) and
\begin{equation}
  c = \xi|_{[0, 1]^k}
\end{equation}
As a mapping, \(c\) is orientation preserving if and only if \(\xi\) is orientation preserving.
\begin{lemma}
  Let \(M\) be an oriented \(k\)-dimensional manifold (with or without boundary), and let \(c_1, c_2: [0, 1]^k \to M\) be orientation-preserving singular \(k\)-cubes, with the above assumption holding. If \(\omega\) is a \(k\)-form on \(M\) such that \(\omega = 0\) outside \(c_1([0, 1]^k) \cap c_2([0, 1]^k)\) then
  \begin{equation}
    \int_{c_1}\omega = \int_{c_2}\omega
  \end{equation}
  \label{cind}
\end{lemma}
\begin{proof}
  By definition, we have that
  \begin{equation}
    \int_{c_1}\omega = \int_{[0, 1]^k}c_1^*\omega
    \label{defncube}
  \end{equation}
  Using the fact that
  \begin{equation}
    c_1 = c_2 \circ (c_2^{-1} \circ c_1)
    \label{composedinv}
  \end{equation}
  Of course, ordinarily we can't talk about inverses when we're talking about cubes, but \(c_1, c_2\) have inverses since they are assumed to be the restrictions of a diffeomorphism. If you want, you can even write that
  \begin{equation}
    c_1^*\omega = \xi^*\omega
  \end{equation}
  Plugging equation \ref{composedinv} into equation \ref{defncube}, we obtain
  \begin{equation}
    \int_{[0, 1]^k}(c_2^{-1} \circ c_1)^*(c_2^*\omega)
  \end{equation}
  We have that
  \begin{equation}
    c_2^*\omega = f(\omega)dx_1 \wedge ... \wedge dx_k
  \end{equation}
  Let \(g = c_2^{-1} \circ c_1\). We then have that
  \begin{equation}
    (c_1^{-1} \circ c_2)^*(c_2^*\omega) = g^*(f dx_1 \wedge ... \wedge dx_k) = (f \circ g)\det g'dx_1 \wedge ... \wedge dx_k
  \end{equation}
  Suince \(g\) is orientation preserving, we get that \(\deg g' = |\det g'|\), implying the above is equal to
  \begin{equation}
    (f \circ g)|\det g'|dx_1 \wedge ... \wedge dx_k = \int_{c_1}\omega
  \end{equation}
  as desired. TODO: check

\end{proof}
We now want to define the integral of a form over a manifold:
\begin{definition}
  Let \(M\) be an oriented \(k\)-dimensional manifold and \(\omega\) be a \(k\)-form on \(M\).
  \begin{enumerate}

    \item If there is an orientation preserving \(k\)-cube \(c\) on \(M\) such that \(\omega = 0\) outside \(c([0, 1]^j)\) we define
    \begin{equation}
      \int_M\omega \int_c\omega
    \end{equation}
    which is independent of the choice of \(c\) by Lemma \ref{cind} as long as \(\omega\) vanishes outside it.

    \item In the general case, there exists an open cover \(\mc{O}\) of \(M \subset \reals^n\) such that for all \(U \in \mc{O}\), thee is an orientation preserving \(k\)-cube \(c_U\) in \(M\) such that
    \begin{equation}
      U \cap M \subset c_U([0, 1]^k)
    \end{equation}
    Now, let's let \(\Phi\) be a \(\mc{C}^2\) partition of unity subordinate to \(\mc{O}\). We define
    \begin{equation}
      \int_M\omega = \sum_{\varphi \in \Phi}\int_{M}\varphi \circ \omega
    \end{equation}
    If \(M\) is compact, this sum is finite, but in general, the definition still works out.

  \end{enumerate}
\end{definition}


\section{The Volume Element}

We already talked about the volume element in the context of alternating tensors. Of course, we're going to apply that to the tangent space at every point of an oriented \(k\)-dimensional manifold \(M \subseteq \reals^n\) (with or without bound). Consider a particular orientation \(\mu\). If we look at the tangent space \(M_x\) at a point \(x \in M\), it has an orientation \(\mu_x\) and also has an inner product \(T_x\) given by the standard inner product in \(\reals^n\).

These two things, as we saw before, are what we need to talk about a volume element (at the point \(x\)), which is an alternating \(k\)-tensor on the tangent space
\begin{equation}
  \omega(x) \in \Omega^k(M_x)
\end{equation}
which is the unique element in the space of alternating \(k\)-tensors such that if \(v_1,...,v_k\) is a positively oriented orthonormal basis of \(M_x\),
\begin{equation}
  \omega(x)(v_1,...,v_k) = 1
\end{equation}
In particular, for any \(w_1,...,w_k \in M_x\),
\begin{equation}
  \omega(x)(w_1,...,w_k)
\end{equation}
is the \textit{oriented} \(k\)-dimensional volume of the parallelopiped determined by \(w_1,...,w_k\), implying it's absolute value is the \(k\)-dimensional volume of the parallelopiped determined by \(w_1,...,w_k\). I didn't say these \(w_1,...,w_k\) were linearly independent, but if they weren't the \(k\)-dimensional volume would be zero. Also notice that if \(w_1,...,w_k\) are linearly independent, then
\begin{equation}
  [w_1,...,w_k] = [v_1,...,v_k] \implies \omega(x)(w_1,...,w_k) > 0
\end{equation}
For an example, note that \(\det\) is the volume element of \(\reals^n\) (at every point) with the standard inner product and orientation.
Note that, taken over the whole manifold, \(\omega\) is by definition a (nonzero, and \(\mc{C}^r\) if \(M\) is \(\mc{C}^r\)) \(k\)-form. We often call this form \(dV\), but be careful: it's not necessarily the differential of some \((k - 1)-\) form ``\(V\)''. In low dimensions,
\begin{itemize}
  \item Where \(k = 1\), \(dV\) is often written \(ds\) for \underline{length}
  \item Where \(k = 2\), \(dV\) is often written \(dA\) or \(dS\) for \underline{surface area}
\end{itemize}
We're going to use these for specific versions of Stokes' Theorem.

\subsubsection{The Volume of \(M\)}
\begin{definition}
  Provided it exists (e.g., if \(M\) is compact), we define the \underline{volume} of \(M\) to be
  \begin{equation}
    \int_MdV
  \end{equation}
\end{definition}
Let's look at some examples:
\begin{enumerate}

  \item Let \(M\) be some \(n\)-dimensional submanifold of \(\reals^n\) with the standard orientation. Then
  \begin{equation}
    dV = dx_1 \wedge ... \wedge dx^n
  \end{equation}
  Hence we have that
  \begin{equation}
    \int_MdV = \int_M1
  \end{equation}
  giving us that the volume of \(M\) agrees with our old definition.

  \item Another situation that we've looked at before is the case where \(M\) is a \(1\)-dimensional oriented submanifold of \(\reals^n\) (a curve). Note that all \(1\)-dimensional submanifolds are orientable.

  It doesn't make sense in general to say ``let's compute the length of this'', because the length may not even be finite. But let's compute the length of a finite piece. This means in general that we'll be looking at an oriented 1-cube in \(M\)
  \begin{equation}
    c: [0, 1] \to M
  \end{equation}
  Let's try to compute the length of \(c([0, 1])\), i.e. just that piece. That means we're going to integrate
  \begin{equation}
    \int_{c([0, 1])}ds = \int_{c}ds = \int_{[0, 1]}c^*(ds) = \int_{[0, 1]}\sqrt{\dot{c_1}^2 + ... + \dot{c_n}^2}
  \end{equation}
  Why? Say \(c^*(ds) = f(t)dt\). That means that
  \begin{equation}
    f(t) = c^*(ds)(t)(e_{1, t}) = ds(c(t))(c_{*,t}(e_{1, t})) = ds(c(t))(c'(t)_{c(t)}) = |c'(t)|
  \end{equation}

  \item Let's see how to compute \(k\)-dimensional volume (in fact, the integral of a \(k\)-form in general) on a \(k\)-dimensional manifold: let \(M\) be a \(k\)-dimensional oriented submanifold of \(\reals^n\) with orientation \(\mu\), and let \(\omega\) be a \(k\)-form on \(M\). We can write
  \begin{equation}
    \omega = \lambda dV
  \end{equation}
  where \(\lambda\) is a function on \(M\). Let \(\varphi: W \to M\) be a \(\mc{C}^{r + 1}\) one-to-one function of constant rank \(k\) (e.g. a coordinate chart). Provided the integral exists, then, we have
  \begin{equation}
    \int_{\varphi(W)}\omega = \int\omega\varphi^*(\omega) = \int_W\lambda \circ \varphi\varphi^*dV
  \end{equation}
  Say, for \(x \in W\),
  \begin{equation}
    \varphi^*dV = h(x)dx_1 \wedge ... \wedge dx_k
  \end{equation}
  How do we compute \(h(x)\). It's:
  \begin{equation}
    (\varphi^*dV)(x)(e_{1,x},...,e_{k,x})
  \end{equation}

\end{enumerate}



\end{document}
