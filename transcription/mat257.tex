\documentclass{article}
\usepackage[utf8]{inputenc}

\title{MAT257 Notes}
\author{Jad Elkhaleq Ghalayini}
\date{April 2019}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{xcolor}

\usepackage[margin=1in]{geometry}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem*{corollary}{Corollary}
\newtheorem{exercise}{Exercise}
\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}

\DeclareMathOperator{\Int}{Int}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\Div}{div}
\DeclareMathOperator{\curl}{curl}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Vol}{vol}
\DeclareMathOperator{\D}{D}

\newcommand{\reals}[0]{\mathbb{R}}
\newcommand{\nats}[0]{\mathbb{N}}
\newcommand{\ints}[0]{\mathbb{Z}}
\newcommand{\rationals}[0]{\mathbb{Q}}
\newcommand{\brac}[1]{\left(#1\right)}
\newcommand{\sbrac}[1]{\left[#1\right]}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\eval}[3]{\left.#3\right|_{#1}^{#2}}
\newcommand{\ip}[2]{\left\langle#1,#2\right\rangle}
\newcommand{\prt}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\mb}[1]{\mathbf{#1}}
\newcommand{\hlfspc}[0]{\mathbb{H}}
\newcommand{\loint}[0]{\operatorname{L}\int}
\newcommand{\hiint}[0]{\operatorname{U}\int}
\newcommand{\indic}[1]{\chi_{#1}}

\newcommand{\TODO}[1]{\textcolor{red}{\textbf{TODO:} #1}}


\begin{document}

\maketitle

This document is a collection of notes for the course MAT257: Analysis II, as taught by Professor Edward Bierstone in 2018 at the University of Toronto. The notes are a combination of notes I made in class (which can be found in their original form in the \verb|notes| folder in this repository) and scans of handwritten notes which Professor Bierstone has generously given me the permission to use.

\section{Introduction}

\TODO{this}

\section{Differentiation}

\TODO{this}

\section{Integration}

\subsection{The (Riemann) Integral Over a Rectangle}

\TODO{this}

\subsection{Integrals Over More General Bounded Sets}

\TODO{this}

\subsection{Fubini's Theorem}

\TODO{rewrite}

We are now going to talk about Fubini's theorem, which is about how to integrate over a high-dimensional rectangle by repeatedly performing individual integrals. Let's start with an example. Suppose we want to integrate over a rectangle
\begin{equation}
  A = [a, b] \times [c, d] \subseteq \reals^2
\end{equation}
and let's suppose, to make it easy, we have a continuous, non-negative function \(f\), defined on this rectangle, say
\begin{equation}
  z = f(x, y) \geq 0
\end{equation}
The idea is that if we fix a point on the \(x\)-axis, say \(x\), we can consider the ``slice'' in the \(y\)-direction determined by this point. We could find, for example, the area of that slice, and it's reasonable to expect that the integral of \(f\) on the rectangle, we could obtain by integrating the area of that slice along the length of the rectangle.

The idea then is to try to find the area of such a slice, which would be in this case, for some \(x\),
\begin{equation}
  h(x) = \int_c^dg_x(y)dy = \int_c^df(x, y)dy
\end{equation}
where \(g_x(y) = f(x, y)\) fixing \(x\). It's reasonable to expect that
\begin{equation}
  \int_Af = \int_a^bh = \int_a^b\left(\int_c^df(x, y)dy\right)dx
\end{equation}
So that's the idea of Fubini's theorem: that we should be able to integrate a function over a rectangle by repeated one-dimensional integrals. Of course, we're not interested in integrating only continuous functions, we want to look at more general, integrable functions, but if you think about that, supposing \(f\) is integrable, this could run into a problem: one of the functions \(g_x\) might not be integrable on \([c, d]\). After all, it's set of discontinuities could be \(x_0 \times [c, d]\) for some \(x_0\), so \(\int_c^dg_{x_0}\) makes no sense!

So we'll have to formulate something maybe a little bit more technical, but it's to capture that problem. Suppose we just have \(f: A \to \reals\) bounded. The function may or may not be integrable, meaning the supremum of lower sums is equal to the infimum of lower sums. Whether the function is integrable or not, we can still look at the supermum of lower sums and the infimum of lower sums, which is what we'll do.

We'll define the \underline{lower} and \underline{upper integrals} of \(f\) on \(A\), \(\loint_Af\) and \(\hiint_Af\) respectively, to be the supremum of all the lower sums \(L(f, \mc{P})\) and infimum of all the upper sums \(U(f, \mc{P})\) respectively. We can now write down our theorem as the previous formula, but taking into account that we don't know that the function \(g_x\) mentioned before, we just replace it by the lower or upper integral:
\begin{theorem}
  Suppose \(A \subset \reals^m\) and \(B \subset \reals^n\) are closed rectangles and \(f: A \times B \to \reals\) is integrable. For all \(x \in A\), define
  \begin{equation}
    g_x: B \to \reals, g_x(y) = f(x, y)
  \end{equation}
  Set
  \begin{equation}
    \mc{L}(x) = \loint_Bg_x = \loint_Bf(x, y)dy, \mc{U}(x) = \hiint_Bg_x = \hiint_Bf(x, y)dy
  \end{equation}
  Then \(\mc{L}(x), \mc{U}(x)\) are integrable on \(A\) and
  \begin{equation}
    \int_{A \times B}f = \int_A\mc{L} = \int_A\mc{U} = \int_A\left(\loint_Bf(x, y)dy\right)dx = \int_A\left(\hiint_Bf(x, y)dy\right)dx
  \end{equation}
\end{theorem}
Before we prove this, some remarks:
\begin{enumerate}

  \item We also have
  \begin{equation}
    \int_{A \times B}f = \int_B\left(\loint_A(f(x, y)dx)\right)dy = \int_B\left(\hiint_A(f(x, y)dx)\right)dy
  \end{equation}

  \item If \(g_x\) is integrable on \(B\) for every \(x \in A\), then
  \begin{equation}
    \int_{A \times B}f = \int_A\left(\int_Bf(x, y)dy\right)dx
  \end{equation}

  \item If
  \begin{equation}
    A = [a_1, b_1] \times [a_2, b_2] \times ... \times [a_n, b_n]
  \end{equation}
  we can apply Fubini's theorem repeatedly to obtain
  \begin{equation}
    \int_Af = \int_{a_n}^{b_n} \left(... \int_{a_2}^{b_2}\left(\int_{a_1}^{b_1}f(x_1,...,x_n)dx_1\right)dx_2...\right) dx_n
  \end{equation}
  if \(f\) is ``sufficiently nice'' (otherwise we'll have to sprinkle in some L's or U's).

\end{enumerate}

Now here's an application to \(\int_Cf\) when \(f\) is integrable on a Jordan-measurable set \(C \subset \reals^n\). What we're going to use is Fubini's theorem in a rectangle with the formula
\begin{equation}
  \int_Cf = \int_Af\indic{C}
\end{equation}
where \(A \supseteq C\) is a closed rectangle.
Let's do some quick examples:
\begin{enumerate}

\item Let's say we were integrating on the region
\begin{equation}
  C = [-1, 1] \times [-1, 1] \setminus \{x \in \reals^2, x_1^2 + x_2^2 < 1\}
\end{equation}
i.e. the unit square with the unit circle removed from it. We have
\begin{equation}
  \int_Cf = \int_{[-1, 1]^2}f\indic{C}
\end{equation}
We can write
\begin{equation}
  \indic{C} = \left\{\begin{array}{cc}
    1 & \text{if } -1 \leq y \leq -\sqrt{1 - x^2} \text{ or } \sqrt{1 - x^2} \leq y \leq 1 \\
    0 & \text{otherwise}
  \end{array}\right.
\end{equation}
We can write
\begin{equation}
  \int_{-1}^1f(x, y)\indic{C}(x, y)dy = \int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy
\end{equation}
i.e. the limits of integration correspond to the boundary of \(C\). So we get
\begin{equation}
  \int_Cf = \int_{-1}^1\left(\int_{-1}^{-\sqrt{1 - x^2}}f(x, y)dy + \int_{\sqrt{1 - x^2}}^1f(x, y)dy\right)dx
\end{equation}

\item Say we want to integrate over a triangle \(C\), under the line from \((0, 0)\) to \((a, a)\). We have
\begin{equation}
  \int_Cf = \int_0^a\left(\int_y^af(x, y)dx\right)dy
\end{equation}
We could have done it the other way, integrating first with respect to \(y\) and then with respect to \(x\), and get
\begin{equation}
  \int_Cf = \int_0^a\left(\int_0^xf(x, y)dy\right)dx
\end{equation}
This shows the form might not be the same in different directions. And it could be important to exploit the difference. For example, what if our function \(f(x, y)\) only depends on one of the variables, say \(y\) (i.e. \(f\) is \textit{independent} of \(x\)). Then
\begin{equation}
  \int_0^a\left(\int_0^xf(y)dy\right)dx
\end{equation}
doesn't simplify very much, but
\begin{equation}
  \int_0^a\left(\int_y^af(y)dx\right)dy = \int_0^a(a - y)f(y)dy
\end{equation}
So this double integral reduces to a single integral with respect to \(y\).

\TODO{proof}

\TODO{polish}
Before we move on, we will give some more examples of computing integrals, using Fubini's theorem and the change of variables theorems as tools.

Let us begin by recalling some facts about the regions on whuch we can integrate. What are the kinds of region on which we integrate? One of the kinds of regions would be the region between the graphs of two functions that are defined on a Jordan-measurable set. Specifically, if we have a subset \(C \subset \reals^n\) that is Jordan-measurable, and two integrable functions \(\varphi(x) \leq \psi(x)\) defined on \(C\), we're interested in integrating over the region bounded by the graphs of the two functions Let's call this region \(S\), that is, define
\begin{equation}
  S = \{(x, y) \in \reals^n \times \reals : x \in C, y \in [\varphi(x), \psi(x)]\}
\end{equation}

We have that \(S\) is a Jordan-measurable set. We want to be able to integrate a continuous function on \(S\). If \(f(x, y)\) is a bounded continuous on \(S\), then we can compute using Fubini's theorem that
\begin{equation}
  \int_Sf = \int_C\left(\int_{\varphi(x)}^{\psi(x)}f(x, y)dy\right)dx
\end{equation}
We went through the proof of this before, but I want to use it to compute some examples.
\begin{enumerate}

  \item Consider
  \begin{equation}
    \int_0^2\int_{y/2}^1ye^{-x^3}dxdy
  \end{equation}
  There's no hope in proceeding naively, since \(e^{-x^3}\) doesn't even have an elementary primitive, i.e. you cannot write down it's integral with only elementary functions. But let's consider what region we're integrating along: the area under the line between \((0, 0)\) and \((2, 1)\). So using the above observation, we can rewrite the integral to be
  \begin{equation}
    \int_0^1\left(\int_0^{2x}ye^{-x^3}dy\right)dx = \int_0^1\left.\frac{y^2}{2}e^{-x^3}\right|_0^{2x}dx = 2\int_0^1x^2e^{-x^3}dx = \left.-\frac{2}{3}e^{-x^3}\right|_{0}^{1} = \frac{2}{3}(1 - e^{-1})
  \end{equation}
  Here, the change of variable was a useful thing to do, because it enabled us to write down an original integral, which we couldn't do in the original form.

  \item Consider
  \begin{equation}
    \int_2^4\int_{4/x}^{\frac{20 - 4x}{8 - x}}(y - 4)dydx
  \end{equation}
  So here, again, should we just go ahead and do it as it's written? Well then we'll get \(\frac{y^2}{2} - 4y\) and we'll substitute those things in and we'll just get a terrible mess. But what's the region we're integrating over? \(\frac{4}{x}\) is like a hyperbola, which is 2 when \(x\) is 2 and 1 when \(x\) is 4. The other function on top, what does it look like? It's also a hyperbola: it's a constant plus something over \(8 - x\), so it'll open down. And this is the region we're integrating on: the area between a hyperbola open up and another opening down.

  If we change the order of integration, we'll be integrating on the outside with respect to \(y\), which goes from 1 to 2, and we'll be integrating on the inside with respect to \(x\) with \(x\) going from \(4/y\) to... let's solve:
  \begin{equation}
    y = \frac{20 - 4x}{8 - x} = 4 + \frac{12}{x - 8} \implies x - 8 = \frac{12}{y - 4} \iff x = 8 + \frac{12}{y - 4}
  \end{equation}
  Hence we can write the above as
  \begin{equation}
    \int_1^2\left(\int_{4/y}^{8 + 12/(y - 4)}(y - 4)dx\right)dy = \left.\int_1^2(y - 4)x\right|_{4/y}^{8 + 12/(y - 4)}dy = \int_1^2(y - 4)\left(8 + \frac{12}{y - 4} - \frac{4}{y}\right)dy
  \end{equation}
  which is all stuff that's very easy to integrate

  \item Let's see how to integrate a simple function \(z\) over a region of \(3\)-space
  \begin{equation}
    S = \{(x, y, z) \in \reals^3 : x^2 + y^2 \leq z^2 \land x^2 + y^2 + z^2 \leq 1\}
  \end{equation}
  So, what is this region? The second equation says we're inside the closed ball of radius 1 centered at the origin. The first equation is the region inside two  cones, opening up and down from the origin (kind of like an hour glass). So we want the intersection of these two regions. In terms of the theorem that we wrote down at the beginning, the set \(C\) is like the discs inside the circles formed by the intersections of the top and bottom of the cones and circles, and we're integrating on the region between the semicircles above and below the plane, and the cones above and below the plane. So what's this going to be? Well, by symmetry, it's going to be \textit{zero}.

  Let's make it a little harder. Let's consider only the top, i.e.
  \[S^+ = \{(x, y, z) \in S : z \geq 0\}\]
  So, rewriting our integral to be over the disc \(C\), we obtain
  \begin{equation}
  \begin{split}
    \int\int_{x^2 + y^2 \leq \frac{1}{2}}\left(\int_{\sqrt{x^2 + y^2}}^{\sqrt{1 - (x^2 + y^2)}}zdz\right)dxdy \\
    = \int\int_{x^2 + y^2 \leq \frac{1}{2}}\frac{1}{2}(1 - (x^2 + y^2) - (x^2 + y^2))dxdy \\
    = \frac{1}{2}\int\int_{x^2 + y^2 \leq \frac{1}{2}}dxdy - \int\int_{x^2 + y^2 - \frac{1}{2}}(x^2 + y^2)dxdy = \frac{1}{2}\pi\frac{1}{2} = \frac{\pi}{4}
  \end{split}
  \end{equation}
  How do we justify that last, ``magical'' step? Well, the prettiest way to do so is to change to polar coordinates, but we haven't justified this quite yet, which is the point of this example. You'll remember from first year caclulus that we can write
  \begin{equation}
    \int_a^bf(g(x))g'(x)dx = \int_{g(a)}^{g(b)}f(u)du
    \label{firstyearsub}
  \end{equation}
  This is what we're going to be doing in several variables. But first: polar coordinates? That means writing
  \begin{equation}
    x = r\cos\theta, y = r\cos\theta, \theta \in [0, 2\pi], r \in [0, \infty)
  \end{equation}
  This gives that
  \begin{equation}
    \prt{(x, y)}{(r, \theta)} = \begin{pmatrix}\cos\theta & -r\sin\theta \\ \sin\theta & r\cos\theta\end{pmatrix} \implies \det\prt{(x, y)}{(r, \theta)} = r \implies dxdy = drd\theta
  \end{equation}
  ``generalizing'' equation \ref{firstyearsub}. On the other hand, we have that
  \begin{equation}
    x^2 + y^2 = r^2
  \end{equation}
  Since we have that, for this disc, \(\theta\) ranges over the whole interval \([0, 2\pi]\) whereas \(r\) ranges over \([0, 1/\sqrt{2}]\), we can hence rewrite the above integral as
  \begin{equation}
    \int_0^{2\pi}\int_0^{\frac{1}{\sqrt{2}}}r^2 \cdot rdrd\theta =
    \int_0^{2\pi}d\theta\int_0^{\frac{1}{\sqrt{2}}}r^3dr =
    \left.2\pi\frac{r^4}{4}\right|_{0}^{\frac{1}{\sqrt{2}}} = \frac{\pi}{8}
  \end{equation}

\end{enumerate}


\subsection{Partitions of Unity}


\subsubsection{Bump Functions}

A bump function, in the one dimensional case, is a function which goes up to 1 from 0, stays there for a while, and then returns to zero. But we're interested in doing this in \(n\)-dimensions. How? Given an open \(U \subset \reals^n\), and compact \(C \subset U\), we want to construct a \(\mc{C}^\infty\) function \(f: U \to \reals\) with \(0 \leq f \leq 1\) such that \(\forall x \in C, f(x) = 1\) and \(f(x) = 0\) outside some compact subset of \(U\). So it looks like the 1-dimensional bump function, except over an \(n\)-dimensional area \(C\).

How can we construct such a function? We'll do it as a kind of series of exercises, based on one particular function, which you can imagine ought to be the kind of thing that you have to build this function, that is,
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    e^{1/x^2} & \text{if} & x \neq 0 \\
    0 & \text{if} & x = 0
  \end{array}\right.
  \label{bumpbuild}
\end{equation}
We proceed as follows, building up by steps:
\begin{enumerate}

  \item There is a \(\mc{C}^\infty\) function \(g: \reals \to \reals\) such that \(g > 0\) on \((-1, 1)\), 0 elsewhere. It's easy to write down a formula for such a function using the function given in \ref{bumpbuild}, as follows:
  \begin{equation}
    g(x) = \left\{\begin{array}{ccc}
      e^{-\frac{1}{(x - 1)^2}}e^{-\frac{1}{(x + 1)^2}} & \text{if} & x \in (-1, 1) \\
      0 & \text{if} & x \notin (-1, 1)
    \end{array}\right.
    \label{g}
  \end{equation}
  This works because \(e^{-\frac{1}{(x - 1)^2}}\) becomes completely flat at \(x = 1\), whereas \(e^{-\frac{1}{(x + 1)^2}}\) becomes completely flat at \(x = -1\).

  \item Given \(a \in \reals^n, \delta > 0\), there is a \(C^\infty\) function \(g_{a, \delta}: \reals^n \to \reals\) such that
  \begin{equation}
    g_{a, \delta} > 0 \text{ on } |x - a| < \delta, g_{a, \delta} = 0 \text{ outside}
  \end{equation}
  So how could achieve this? We could use the function defined in \ref{g} to write
  \begin{equation}
    g_{a, \delta} = g\left(\frac{|x - a|^2}{\delta^2}\right)
  \end{equation}

  \item There is a \(\mc{C}^\infty\) function \(h: U \to \reals\) such that \(h\) is greater than 0 on \(C\) and \(h = 0\) outside a compact subset of \(U\). So how do we do this?

  We could cover \(C\) with a finite set of open balls \(U_1,...,U_k\) with centers \(a_1,...,a_k\) and radii \(\delta_1,...,\delta_k\) whose closures lie in \(U\). We can do this since \(C\) is compact. Once we do this, we can define quite simply
  \begin{equation}
    h(x) = \sum_{i = 1}^kg_{a_k, \delta_k}(x)
    \label{compactsum}
  \end{equation}

  \item As a fourth thing, going back to the real line, let's show that if we're given any \(\epsilon \in \reals^+\), we can find a \(\mc{C}^\infty\) function \(\psi_\epsilon\) which is zero up to 0, becomes 1 at \(\epsilon\), and then remains 1 forever after. How can we do this from what we've already done? We're going to do it starting with a \(\mc{C}^\infty\) function like that in equation \ref{g}, but instead of on \((-1, 1)\), on \((0, \epsilon)\). More generally, actually, we can use any function \(g\) such that \(g > 0\) on \((0, \epsilon)\) and is zero outside. We can then define \(\psi_\epsilon\) by
  \begin{equation}
    \psi_\epsilon(x) = \frac{\int_0^xg}{\int_0^\epsilon g}
    \label{riseup}
  \end{equation}

  \item Now, putting everything together, how can we define the bump function that we wanted to begin with? We begin with the function \(h\) as defined in equation \ref{compactsum}, and we're going to modify it using equation \ref{riseup} as follows: letting \(\epsilon = \min_Ch > 0\), which exists since \(C\) is compact, let
  \begin{equation}
    f(x) = \psi_\epsilon \circ h
  \end{equation}

\end{enumerate}

Now there are other ways of doing it. One of them is to write down one integral formula from the beginning, but it would be harder to see just where this formula comes from in the first place.

\subsubsection{Definition}

We can now get to defining partitions of unity:
\begin{theorem}
  Given \(A \subset \reals^n\) and an open cover \(\mc{O}\) of \(A\), there is a countable collection \(\Phi\) of \(\mc{C}^\infty\) functions \(\varphi\) on \(\reals^n\) with the following properties:
  \begin{enumerate}

    \item \(0 \leq \varphi \leq 1\) \label{cond:partition_bounded}

    \item For all \(x \in A\), there is an open neighborhood \(V_x\) of \(x\) in \(\reals^n\) such that all but finitely many \(\varphi\) vanish on \(V_x\) \label{cond:finite_vanish}

    \item For all \(x \in A\), \(\sum_{\varphi \in \Phi}\varphi(x) = 1\) (sum is finite in a neighborhood of \(x\)) \label{cond:partition_sum_finite}

    \item For all \(\varphi \in \Phi\), there is some open \(U \in \mc{O}\) such that \(\varphi = 0\) outside a compact subset of \(U\). \label{cond:subordinate}

  \end{enumerate}

  \label{cheapway}
\end{theorem}
\begin{definition}
  A collection \(\varphi\) satisfying conditions
  \ref{cond:partition_bounded},
  \ref{cond:finite_vanish},
  \ref{cond:partition_sum_finite}
  above is called a \underline{\(\mc{C}^\infty\) partition for \(A\)}.
  If condition \ref{cond:subordinate} holds, this partition is called \underline{subordinate to \(\mc{O}\)}
\end{definition}
This is all \(\mc{C}^\infty\) stuff, but we could have made something weaker. We could have asked for a \(\mc{C}^r\) partition, or even a partition that was merely continous. So suppose we did that, and only cared about a \(\mc{C}^0\) partition. Then, we could also, though the proof will have to come next time, we could use functions that look like trapezoids, using \(|x|\) instead of \(e^{-\frac{1}{x^2}}\). If we wanted a \(\mc{C}^r\) partition of unity, then of course, we could do the same thing, but only caring about \(\mc{C}^r\) instead of \(\mc{C}^\infty\), and hence using the function given by
\begin{equation}
  f(x) = \left\{\begin{array}{ccc}
    |x^{r + 1}| & \text{on} & [0, \infty) \\
    -|x^{r + 1}| & \text{on} & (-\infty, 0]
  \end{array}\right.
\end{equation}

Before we try to construct such a (\(\mc{C}^\infty\)) partition, let's discuss why this is an interesting idea. We're going to use this for several things in this course:
\begin{itemize}
  \item To extend the definition of the integral to more general regions
  \item To prove the change of variables theorem
  \item To extend the definition of the integral to manifolds
  \item To prove Stoke's theorem
\end{itemize}
So why? Suppose we wanted to extend the definition of the integral to manifolds. Take some shape in 3-space, with a boundary, which is a manifold. Right now, we can integrate over a rectangle. Once we prove the change of variables theorem, we'll be able to integrate over a space diffeomorphic to a rectangle, a sort of deformed rectangle. Now, if we have a coordinate chart, a diffeomorphism from \(\reals^n\) to some part of the manifold, we could use this to extend the definition of the integral from the rectangle to at least some kind of ``rectangular'' piece of the manifold. The question remains: how can we extend the definition to the \textit{entire} manifold? A reasonable way to try to do this is to try to cover the entire manifold by a grid of rectangles. If we add up the integrals on each of those pieces...

That was the way this was doen classically: you'd take a manifold and divide it up by some kind of rectangular grid, and just add up the contributions of every piece. This classical approach, however, ran into a big obstacle, which was: how do you know that you can really cover every manfiold with such a grid? How can you really divide up a really complex surface into rectangles? That is an extremely hard problem: it's true you can do it, but it's an extremely hard problem, and was solved way after the extension of integrals to manifolds.

So how can we avoid this issue? The way this was solved was, you do something a million times simpler. When you cover the manifold with rectangles, you don't care that they all fit together in a nice grid. Just, for every point, you take a rectangle containing that point, and we don't care whether they overlap or not. How do we exploit that? We use a partition of unity! Because what we'll do is, we'll pick a partition of unity such that every one of these bump functions sits in teh interior of one of these rectangles. Now, if we want to integrate our fucntion \(f\) over the manifold, and we have our partition of unity \(\Phi\), and we look at \(\varphi f\), it vanishes outside one of these rectangles, so we can just integrate over the rectangle and get the answer for the manifold. We can then write
\begin{equation}
  \int f = \sum\int\varphi f
\end{equation}
So the key is to take a function, and make it's support very small, and then work with those small pieces. And we're going to use this in the same way to prove Stoke's theorem. Stoke's theorem is like the FTC. What does the FTC say? It says that
\begin{equation}
  \int_a^bf' = f(a) - f(b)
\end{equation}
So the integral of \(f\) over the \underline{boundary} of \([a, b]\) is the same as the integral of the derivative of \(f\) over the whole interval. So Stoke's theorem is that the integral of a function over the boundary of a manifold is the same as the integral of the derivative over the whole manifold.

We can prove this for rectangles. Then, if we want to prove it for manifolds, again we need to try to cover the manfolds with a grid like this. If we can do that, we can prove Stoke's theorem for surfaces that we know can be covered by a grid. But we can do it for general manifolds in a really cheap way, again by multiplying with partitions of unity. So that's whay we're going to do in the rest of the course. But first, we have to prove Theorem \ref{cheapway}, because that's what we're going to use to glue together little pieces in a very nice but cheap way.

\begin{proof}

  The way we're going to do this is by considering a number of cases building up to the general case.

  \begin{enumerate}

    \item Assume \(A\) is compact. That means finitely many \(U \in \mc{O}\) cover \(A\). Call them \(U_1,...,U_q\). In this case we're going to construct a \textit{finite} (not countable) partition of unity subordinate to \(\{U_1,...,U_q\}\).

    For each \(i \in \{1,...,q\}\), the first thing that we're going to do is show that we can find a compact set \(D_i \subset U_i\) such that the interiors of the \(D_i\) cover \(A\). How?

    For all \(x \in A\), let \(D_x\) be a closed ball with centre \(x\) lying inside some \(U_i\). Since \(A\) is compact, finitely many of the interiors of the \(D_x\) cover \(A\). Number them \(D_{x_1},...,D_{x_p}\). For each \(i \in \{1,...,q\}\), let
    \begin{equation}
      D_i = \bigcup_{D_{x_j} \subset U_i}D_{x_j}
    \end{equation}

    Let \(\psi_i\) be a \(\mc{C}^\infty\) function \(0 \leq \psi_i \leq 1\) such that \(\psi_i > 0\) on \(D_i\), and is 0 outside a compact subset of \(U_i\). Notice if we do this, then, since the interior of the \(D_i\)'s cover's \(A\),
    \begin{equation}
      \forall x \in U \supseteq A, \sum_{i = 1}^q\psi_i(x) > 0
    \end{equation}
    where \(U\) is an open neighborhood of \(A\). SO we define our partition of unity as
    \begin{equation}
      \Phi = \{\varphi_1,...,\varphi_q\}, \varphi_i = \frac{\psi_i}{\sum_{i = 1}^q\psi_i}
    \end{equation}
    \label{compactcase}

    \item Now assume \(A = \bigcup_{i \in \nats}A_i\) where each \(A_i\) is compact and a subset of the interior of \(A_{i + 1}\). Let \(B_i = A_i \setminus \Int A_{i - 1}\). Let \(\mc{O}_i\) be covers of \(B_i\) by the open sets
    \begin{equation}
      U \cap (\Int A_{i + 1} \setminus A_{i - 2})
    \end{equation}
    for each \(U \in \mc{O}\). By Case \ref{compactcase} there is a finite partition of unity \(\Phi_i\) for \(B_i\) subordinate to \(\mc{O}_i\). Consider
    \begin{equation}
      \sigma = \sum_i\sum_{\psi_i \in \Phi_i}\psi_i
    \end{equation}
    This is a finite sum in some neighborhood of every \(x \in A\). Take for a partition
    \begin{equation}
      \varphi = \frac{\psi}{\sigma}
    \end{equation}
    for each \(\psi \in \Phi_i\), for every \(i\).

    \label{compacttowercase}

    \item Assume \(A\) is open. Then \(A = \bigcup_{i \in \nats}A_i\) as in Case \ref{compacttowercase}. How can we get such a representation? Try
    \begin{equation}
      A_i = \{x : |x| \leq i, d(x_, \reals^n \setminus A) > i^{-1}\}
    \end{equation}

    \label{opencase}

    \item Let \(A\) be arbitrary. Apply Case \ref{opencase} to \(\bigcup_{U \in \mc{O}}U\). A partition of unity subordinate to \(\mc{O}\) is also a partition of unity for \(A\) subordinate to \(\mc{O}\)

  \end{enumerate}

\end{proof}

\TODO{rest}

\subsection{Change of Variables}

\TODO{this}

\subsection{Parametrically Defined Curves}

\TODO{this}

\section{Manifolds}

\subsection{What is a manifold?}

\TODO{this}

\subsection{Functions Between Manifolds}

\TODO{this}

\subsection{Manifolds with Boundary}

\TODO{this}

\subsection{Multilinear Algebra}

\TODO{this}

\subsection{Vector Fields and Differential Forms}

\TODO{this}

\subsection{The Differential Operator}

\TODO{this}

\section{Integration on Manifolds}

\TODO{this}

\subsection{Integration of Parametrized Curves}

\TODO{this}

\subsection{Integral of a \(k\)-form over a \(k\)-cube}

\TODO{this}

\subsection{Integration of Differential Forms on Manifolds}

\TODO{this}

\subsection{Manifolds with Boundary}

\TODO{this}

\subsection{Stoke's Theorem on Manifolds}

\TODO{this}

\end{document}
